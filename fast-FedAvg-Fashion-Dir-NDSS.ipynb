{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for Fashion MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821b1014",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu065/6672123/ipykernel_361451/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a52c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "            \n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_train_data(trainset, num_workers=100, bias=0.5):\n",
    "    np.random.seed(42)\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        \n",
    "        if not len(each_worker_data[selected_worker]):\n",
    "            each_worker_data[selected_worker] = x[None, :]\n",
    "        else:\n",
    "            each_worker_data[selected_worker]= torch.concat((each_worker_data[selected_worker], x[None, :]))\n",
    "        \n",
    "        each_worker_label[selected_worker].append(y)\n",
    "    \n",
    "    each_worker_tr_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_tr_label = [[] for _ in range(num_workers)]\n",
    "    each_worker_val_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_val_label = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "\n",
    "    for i in range(num_workers):\n",
    "        w_len = len(each_worker_data[i])\n",
    "        len_tr = int(5 * w_len / 7)\n",
    "        len_val = int(1 * w_len / 7)\n",
    "        len_te = w_len - (len_tr + len_val)\n",
    "        # tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        # te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        w = np.arange(w_len)\n",
    "        np.random.shuffle(w)\n",
    "        tr_idx, val_idx, te_idx = w[:len_tr], w[len_tr : (len_tr + len_val)], w[-len_te:]\n",
    "        each_worker_tr_data[i] = each_worker_data[i][tr_idx]\n",
    "        each_worker_tr_label[i] = torch.Tensor(each_worker_label[i])[tr_idx]\n",
    "        each_worker_val_data[i] = each_worker_data[i][val_idx]\n",
    "        each_worker_val_label[i] = torch.Tensor(each_worker_label[i])[val_idx]\n",
    "        each_worker_te_data[i] = each_worker_data[i][te_idx]\n",
    "        each_worker_te_label[i] = torch.Tensor(each_worker_label[i])[te_idx]\n",
    "        \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    del each_worker_data, each_worker_label\n",
    "    return each_worker_tr_data, each_worker_tr_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f36f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_data_dirichlet(trainset, num_workers, alpha=1, force=False):\n",
    "    per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=alpha, force=force)\n",
    "    \n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(6*w_len/7)\n",
    "        len_te = w_len - len_tr\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        \n",
    "        for idx in w_indices[tr_idx]:\n",
    "            each_worker_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_data[worker_idx] = torch.stack(each_worker_data[worker_idx])\n",
    "        each_worker_label[worker_idx] = torch.Tensor(each_worker_label[worker_idx]).long()\n",
    "        \n",
    "        for idx in w_indices[te_idx]:\n",
    "            each_worker_te_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_te_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_te_data[worker_idx] = torch.stack(each_worker_te_data[worker_idx])\n",
    "        each_worker_te_label[worker_idx] = torch.Tensor(each_worker_te_label[worker_idx]).long()\n",
    "    \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    \n",
    "    return each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca6b5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 30, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(30, 50, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(1250, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "#         print(x.shape)\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb5ef3",
   "metadata": {},
   "source": [
    "# Good FedAvg baseline Fashion MNIST + Fang distribution + 80 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24164616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(train_data, labels, model, optimizer, batch_size=20):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    len_t = (len(train_data) // batch_size)\n",
    "    if len(train_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    r=np.arange(len(train_data))\n",
    "    np.random.shuffle(r)\n",
    "    \n",
    "    train_data = train_data[r]\n",
    "    labels = labels[r]\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "\n",
    "        inputs = train_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "        targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "\n",
    "def test(test_data, labels, model, criterion, use_cuda, debug_='MEDIUM', batch_size=64):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    len_t = (len(test_data) // batch_size)\n",
    "    if len(test_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ind in range(len_t):\n",
    "            # measure data loading time\n",
    "            inputs = test_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "            targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96353cd6",
   "metadata": {},
   "source": [
    "# NDSS attack on Dir + TrMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1f118ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndss21_attack_trmean(all_updates, n_attackers, dev_type='sign', threshold=5.0, threshold_diff=1e-5, fast_ndss=False):\n",
    "    \n",
    "    model_re = torch.mean(all_updates, 0)\n",
    "    \n",
    "    if dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    if fast_ndss:\n",
    "        return (model_re - threshold * deviation)\n",
    "\n",
    "    lamda = torch.Tensor([threshold]).to(device)  # compute_lambda_our(all_updates, model_re, n_attackers)\n",
    "\n",
    "    threshold_diff = threshold_diff\n",
    "    prev_loss = -1\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        agg_grads = tr_mean(mal_updates, n_attackers)\n",
    "\n",
    "        loss = torch.norm(agg_grads - model_re)\n",
    "\n",
    "        if prev_loss < loss:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "        prev_loss = loss\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "357b9e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Processing NDSS on Dir 10.0 run 0\n",
      "generating participant indices for alpha 10.0\n",
      "NDSS on Dir 10.0 + TrMean | r 0 e 0 val loss 1.043 val acc 69.790 best val_acc 69.790\n",
      "NDSS on Dir 10.0 + TrMean | r 0 e 10 val loss 0.537 val acc 80.016 best val_acc 80.016\n",
      "NDSS on Dir 10.0 + TrMean | r 0 e 20 val loss 0.489 val acc 82.346 best val_acc 82.346\n",
      "NDSS on Dir 10.0 + TrMean | r 0 e 30 val loss 0.457 val acc 83.919 best val_acc 83.999\n",
      "NDSS on Dir 10.0 + TrMean | r 0 e 40 val loss 0.463 val acc 84.357 best val_acc 84.815\n",
      "NDSS on Dir 10.0 + TrMean | r 0 e 49 val loss 0.449 val acc 85.124 best val_acc 85.124\n",
      "NDSS on Dir 10.0 + TrMean | r 0 e 50 val loss 0.446 val acc 85.253 best val_acc 85.253\n",
      "===> Processing NDSS on Dir 10.0 run 1\n",
      "generating participant indices for alpha 10.0\n",
      "NDSS on Dir 10.0 + TrMean | r 1 e 0 val loss 1.138 val acc 67.629 best val_acc 67.629\n",
      "NDSS on Dir 10.0 + TrMean | r 1 e 10 val loss 0.535 val acc 80.116 best val_acc 80.116\n",
      "NDSS on Dir 10.0 + TrMean | r 1 e 20 val loss 0.486 val acc 82.495 best val_acc 82.495\n",
      "NDSS on Dir 10.0 + TrMean | r 1 e 30 val loss 0.453 val acc 83.979 best val_acc 84.108\n",
      "NDSS on Dir 10.0 + TrMean | r 1 e 40 val loss 0.438 val acc 84.875 best val_acc 84.905\n",
      "NDSS on Dir 10.0 + TrMean | r 1 e 49 val loss 0.437 val acc 85.303 best val_acc 85.303\n",
      "NDSS on Dir 10.0 + TrMean | r 1 e 50 val loss 0.432 val acc 85.313 best val_acc 85.313\n",
      "===> Processing NDSS on Dir 10.0 run 2\n",
      "generating participant indices for alpha 10.0\n",
      "NDSS on Dir 10.0 + TrMean | r 2 e 0 val loss 1.216 val acc 68.346 best val_acc 68.346\n",
      "NDSS on Dir 10.0 + TrMean | r 2 e 10 val loss 0.525 val acc 80.454 best val_acc 80.454\n",
      "NDSS on Dir 10.0 + TrMean | r 2 e 20 val loss 0.475 val acc 82.983 best val_acc 83.152\n",
      "NDSS on Dir 10.0 + TrMean | r 2 e 30 val loss 0.449 val acc 84.138 best val_acc 84.288\n",
      "NDSS on Dir 10.0 + TrMean | r 2 e 40 val loss 0.441 val acc 84.706 best val_acc 84.845\n",
      "NDSS on Dir 10.0 + TrMean | r 2 e 49 val loss 0.449 val acc 84.905 best val_acc 84.925\n",
      "NDSS on Dir 10.0 + TrMean | r 2 e 50 val loss 0.443 val acc 84.865 best val_acc 84.925\n",
      "===> Processing NDSS on Dir 0.5 run 0\n",
      "generating participant indices for alpha 0.5\n",
      "NDSS on Dir 0.5 + TrMean | r 0 e 0 val loss 1.900 val acc 48.422 best val_acc 48.422\n",
      "NDSS on Dir 0.5 + TrMean | r 0 e 10 val loss 0.638 val acc 77.123 best val_acc 77.123\n",
      "NDSS on Dir 0.5 + TrMean | r 0 e 20 val loss 0.591 val acc 78.556 best val_acc 78.556\n",
      "NDSS on Dir 0.5 + TrMean | r 0 e 30 val loss 0.589 val acc 78.925 best val_acc 79.383\n",
      "NDSS on Dir 0.5 + TrMean | r 0 e 40 val loss 0.564 val acc 80.329 best val_acc 80.677\n",
      "NDSS on Dir 0.5 + TrMean | r 0 e 49 val loss 0.575 val acc 80.846 best val_acc 80.946\n",
      "NDSS on Dir 0.5 + TrMean | r 0 e 50 val loss 0.575 val acc 80.647 best val_acc 80.946\n",
      "===> Processing NDSS on Dir 0.5 run 1\n",
      "generating participant indices for alpha 0.5\n",
      "NDSS on Dir 0.5 + TrMean | r 1 e 0 val loss 1.933 val acc 49.199 best val_acc 49.199\n",
      "NDSS on Dir 0.5 + TrMean | r 1 e 20 val loss 0.563 val acc 79.343 best val_acc 79.343\n",
      "NDSS on Dir 0.5 + TrMean | r 1 e 30 val loss 0.550 val acc 80.209 best val_acc 80.209\n",
      "NDSS on Dir 0.5 + TrMean | r 1 e 40 val loss 0.537 val acc 80.757 best val_acc 80.846\n",
      "NDSS on Dir 0.5 + TrMean | r 1 e 49 val loss 0.526 val acc 81.483 best val_acc 81.483\n",
      "NDSS on Dir 0.5 + TrMean | r 1 e 50 val loss 0.519 val acc 81.593 best val_acc 81.593\n",
      "===> Processing NDSS on Dir 0.5 run 2\n",
      "generating participant indices for alpha 0.5\n",
      "NDSS on Dir 0.5 + TrMean | r 2 e 0 val loss 1.936 val acc 35.819 best val_acc 35.819\n",
      "NDSS on Dir 0.5 + TrMean | r 2 e 10 val loss 0.649 val acc 76.645 best val_acc 76.645\n",
      "NDSS on Dir 0.5 + TrMean | r 2 e 20 val loss 0.617 val acc 77.770 best val_acc 77.810\n",
      "NDSS on Dir 0.5 + TrMean | r 2 e 30 val loss 0.576 val acc 79.433 best val_acc 79.433\n",
      "NDSS on Dir 0.5 + TrMean | r 2 e 40 val loss 0.566 val acc 80.119 best val_acc 80.119\n",
      "NDSS on Dir 0.5 + TrMean | r 2 e 49 val loss 0.554 val acc 80.976 best val_acc 80.976\n",
      "NDSS on Dir 0.5 + TrMean | r 2 e 50 val loss 0.556 val acc 81.055 best val_acc 81.055\n",
      "===> Processing NDSS on Dir 0.4 run 0\n",
      "generating participant indices for alpha 0.4\n",
      "NDSS on Dir 0.4 + TrMean | r 0 e 0 val loss 2.168 val acc 25.137 best val_acc 25.137\n",
      "NDSS on Dir 0.4 + TrMean | r 0 e 10 val loss 0.763 val acc 67.483 best val_acc 67.663\n",
      "NDSS on Dir 0.4 + TrMean | r 0 e 20 val loss 0.674 val acc 70.501 best val_acc 70.501\n",
      "NDSS on Dir 0.4 + TrMean | r 0 e 30 val loss 0.632 val acc 73.230 best val_acc 73.449\n",
      "NDSS on Dir 0.4 + TrMean | r 0 e 40 val loss 0.618 val acc 74.544 best val_acc 74.813\n",
      "NDSS on Dir 0.4 + TrMean | r 0 e 49 val loss 0.603 val acc 75.620 best val_acc 75.710\n",
      "NDSS on Dir 0.4 + TrMean | r 0 e 50 val loss 0.592 val acc 76.188 best val_acc 76.188\n",
      "===> Processing NDSS on Dir 0.4 run 1\n",
      "generating participant indices for alpha 0.4\n",
      "NDSS on Dir 0.4 + TrMean | r 1 e 0 val loss 2.071 val acc 25.386 best val_acc 25.386\n",
      "NDSS on Dir 0.4 + TrMean | r 1 e 10 val loss 0.737 val acc 68.858 best val_acc 68.858\n",
      "NDSS on Dir 0.4 + TrMean | r 1 e 20 val loss 0.688 val acc 71.716 best val_acc 72.343\n",
      "NDSS on Dir 0.4 + TrMean | r 1 e 30 val loss 0.655 val acc 73.210 best val_acc 73.957\n",
      "NDSS on Dir 0.4 + TrMean | r 1 e 40 val loss 0.623 val acc 75.710 best val_acc 75.909\n",
      "NDSS on Dir 0.4 + TrMean | r 1 e 49 val loss 0.622 val acc 76.367 best val_acc 76.696\n",
      "NDSS on Dir 0.4 + TrMean | r 1 e 50 val loss 0.619 val acc 76.656 best val_acc 76.696\n",
      "===> Processing NDSS on Dir 0.4 run 2\n",
      "generating participant indices for alpha 0.4\n",
      "NDSS on Dir 0.4 + TrMean | r 2 e 0 val loss 2.015 val acc 32.457 best val_acc 32.457\n",
      "NDSS on Dir 0.4 + TrMean | r 2 e 10 val loss 0.788 val acc 66.926 best val_acc 66.965\n",
      "NDSS on Dir 0.4 + TrMean | r 2 e 20 val loss 0.688 val acc 71.019 best val_acc 71.019\n",
      "NDSS on Dir 0.4 + TrMean | r 2 e 30 val loss 0.653 val acc 72.632 best val_acc 72.782\n",
      "NDSS on Dir 0.4 + TrMean | r 2 e 40 val loss 0.636 val acc 74.226 best val_acc 74.903\n",
      "NDSS on Dir 0.4 + TrMean | r 2 e 49 val loss 0.627 val acc 75.291 best val_acc 76.337\n",
      "NDSS on Dir 0.4 + TrMean | r 2 e 50 val loss 0.622 val acc 75.391 best val_acc 76.337\n",
      "===> Processing NDSS on Dir 0.3 run 0\n",
      "generating participant indices for alpha 0.3\n",
      "NDSS on Dir 0.3 + TrMean | r 0 e 0 val loss 2.154 val acc 19.570 best val_acc 19.570\n",
      "NDSS on Dir 0.3 + TrMean | r 0 e 10 val loss 0.821 val acc 64.854 best val_acc 64.854\n",
      "NDSS on Dir 0.3 + TrMean | r 0 e 20 val loss 0.724 val acc 68.748 best val_acc 68.748\n",
      "NDSS on Dir 0.3 + TrMean | r 0 e 30 val loss 0.716 val acc 70.790 best val_acc 70.790\n",
      "NDSS on Dir 0.3 + TrMean | r 0 e 40 val loss 0.691 val acc 72.881 best val_acc 72.881\n",
      "NDSS on Dir 0.3 + TrMean | r 0 e 49 val loss 0.696 val acc 73.349 best val_acc 74.485\n",
      "NDSS on Dir 0.3 + TrMean | r 0 e 50 val loss 0.699 val acc 73.359 best val_acc 74.485\n",
      "===> Processing NDSS on Dir 0.3 run 1\n",
      "generating participant indices for alpha 0.3\n",
      "NDSS on Dir 0.3 + TrMean | r 1 e 0 val loss 2.116 val acc 20.357 best val_acc 20.357\n",
      "NDSS on Dir 0.3 + TrMean | r 1 e 10 val loss 0.854 val acc 65.063 best val_acc 65.063\n",
      "NDSS on Dir 0.3 + TrMean | r 1 e 20 val loss 0.773 val acc 68.310 best val_acc 68.310\n",
      "NDSS on Dir 0.3 + TrMean | r 1 e 30 val loss 0.748 val acc 70.630 best val_acc 71.089\n",
      "NDSS on Dir 0.3 + TrMean | r 1 e 40 val loss 0.684 val acc 74.783 best val_acc 74.783\n",
      "NDSS on Dir 0.3 + TrMean | r 1 e 49 val loss 0.691 val acc 74.714 best val_acc 74.783\n",
      "NDSS on Dir 0.3 + TrMean | r 1 e 50 val loss 0.680 val acc 75.570 best val_acc 75.570\n",
      "===> Processing NDSS on Dir 0.3 run 2\n",
      "generating participant indices for alpha 0.3\n",
      "NDSS on Dir 0.3 + TrMean | r 2 e 0 val loss 2.233 val acc 19.958 best val_acc 19.958\n",
      "NDSS on Dir 0.3 + TrMean | r 2 e 10 val loss 0.813 val acc 66.906 best val_acc 67.334\n",
      "NDSS on Dir 0.3 + TrMean | r 2 e 20 val loss 0.757 val acc 69.216 best val_acc 69.216\n",
      "NDSS on Dir 0.3 + TrMean | r 2 e 30 val loss 0.725 val acc 71.517 best val_acc 71.606\n",
      "NDSS on Dir 0.3 + TrMean | r 2 e 40 val loss 0.706 val acc 73.439 best val_acc 73.698\n",
      "NDSS on Dir 0.3 + TrMean | r 2 e 49 val loss 0.698 val acc 74.345 best val_acc 74.505\n",
      "NDSS on Dir 0.3 + TrMean | r 2 e 50 val loss 0.695 val acc 74.773 best val_acc 74.773\n",
      "===> Processing NDSS on Dir 0.2 run 0\n",
      "generating participant indices for alpha 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDSS on Dir 0.2 + TrMean | r 0 e 0 val loss 2.240 val acc 19.283 best val_acc 19.283\n",
      "NDSS on Dir 0.2 + TrMean | r 0 e 10 val loss 0.974 val acc 62.769 best val_acc 62.769\n",
      "NDSS on Dir 0.2 + TrMean | r 0 e 20 val loss 0.849 val acc 67.261 best val_acc 67.261\n",
      "NDSS on Dir 0.2 + TrMean | r 0 e 30 val loss 0.833 val acc 68.516 best val_acc 68.516\n",
      "NDSS on Dir 0.2 + TrMean | r 0 e 40 val loss 0.811 val acc 71.325 best val_acc 71.325\n",
      "NDSS on Dir 0.2 + TrMean | r 0 e 49 val loss 0.796 val acc 72.490 best val_acc 72.490\n",
      "NDSS on Dir 0.2 + TrMean | r 0 e 50 val loss 0.796 val acc 72.659 best val_acc 72.659\n",
      "===> Processing NDSS on Dir 0.2 run 1\n",
      "generating participant indices for alpha 0.2\n",
      "NDSS on Dir 0.2 + TrMean | r 1 e 0 val loss 2.322 val acc 27.102 best val_acc 27.102\n",
      "NDSS on Dir 0.2 + TrMean | r 1 e 10 val loss 1.015 val acc 62.888 best val_acc 62.888\n",
      "NDSS on Dir 0.2 + TrMean | r 1 e 20 val loss 0.856 val acc 68.934 best val_acc 68.934\n",
      "NDSS on Dir 0.2 + TrMean | r 1 e 30 val loss 0.820 val acc 69.930 best val_acc 69.930\n",
      "NDSS on Dir 0.2 + TrMean | r 1 e 40 val loss 0.815 val acc 69.323 best val_acc 70.498\n",
      "NDSS on Dir 0.2 + TrMean | r 1 e 49 val loss 0.809 val acc 71.305 best val_acc 71.355\n",
      "NDSS on Dir 0.2 + TrMean | r 1 e 50 val loss 0.789 val acc 71.484 best val_acc 71.484\n",
      "===> Processing NDSS on Dir 0.2 run 2\n",
      "generating participant indices for alpha 0.2\n",
      "NDSS on Dir 0.2 + TrMean | r 2 e 0 val loss 2.214 val acc 25.448 best val_acc 25.448\n",
      "NDSS on Dir 0.2 + TrMean | r 2 e 10 val loss 1.016 val acc 62.948 best val_acc 62.948\n",
      "NDSS on Dir 0.2 + TrMean | r 2 e 20 val loss 0.883 val acc 67.201 best val_acc 67.201\n",
      "NDSS on Dir 0.2 + TrMean | r 2 e 30 val loss 0.867 val acc 68.257 best val_acc 68.765\n",
      "NDSS on Dir 0.2 + TrMean | r 2 e 40 val loss 0.804 val acc 69.890 best val_acc 70.179\n",
      "NDSS on Dir 0.2 + TrMean | r 2 e 49 val loss 0.795 val acc 71.424 best val_acc 71.554\n",
      "NDSS on Dir 0.2 + TrMean | r 2 e 50 val loss 0.812 val acc 70.946 best val_acc 71.554\n",
      "===> Processing NDSS on Dir 0.1 run 0\n",
      "generating participant indices for alpha 0.1\n",
      "NDSS on Dir 0.1 + TrMean | r 0 e 0 val loss 2.422 val acc 15.197 best val_acc 15.197\n",
      "NDSS on Dir 0.1 + TrMean | r 0 e 10 val loss 1.476 val acc 43.402 best val_acc 43.710\n",
      "NDSS on Dir 0.1 + TrMean | r 0 e 20 val loss 1.262 val acc 55.772 best val_acc 55.772\n",
      "NDSS on Dir 0.1 + TrMean | r 0 e 30 val loss 1.177 val acc 58.668 best val_acc 58.668\n",
      "NDSS on Dir 0.1 + TrMean | r 0 e 40 val loss 1.125 val acc 59.514 best val_acc 59.514\n",
      "NDSS on Dir 0.1 + TrMean | r 0 e 49 val loss 1.104 val acc 60.042 best val_acc 60.480\n",
      "NDSS on Dir 0.1 + TrMean | r 0 e 50 val loss 1.091 val acc 60.231 best val_acc 60.480\n",
      "===> Processing NDSS on Dir 0.1 run 1\n",
      "generating participant indices for alpha 0.1\n",
      "NDSS on Dir 0.1 + TrMean | r 1 e 0 val loss 2.496 val acc 13.236 best val_acc 13.236\n",
      "NDSS on Dir 0.1 + TrMean | r 1 e 10 val loss 1.295 val acc 53.593 best val_acc 53.593\n",
      "NDSS on Dir 0.1 + TrMean | r 1 e 20 val loss 1.172 val acc 59.733 best val_acc 59.733\n",
      "NDSS on Dir 0.1 + TrMean | r 1 e 30 val loss 1.088 val acc 62.928 best val_acc 62.928\n",
      "NDSS on Dir 0.1 + TrMean | r 1 e 40 val loss 1.104 val acc 63.027 best val_acc 63.296\n",
      "NDSS on Dir 0.1 + TrMean | r 1 e 49 val loss 1.134 val acc 61.913 best val_acc 63.794\n",
      "NDSS on Dir 0.1 + TrMean | r 1 e 50 val loss 1.063 val acc 63.595 best val_acc 63.794\n",
      "===> Processing NDSS on Dir 0.1 run 2\n",
      "generating participant indices for alpha 0.1\n",
      "NDSS on Dir 0.1 + TrMean | r 2 e 0 val loss 2.357 val acc 17.805 best val_acc 17.805\n",
      "NDSS on Dir 0.1 + TrMean | r 2 e 10 val loss 1.382 val acc 44.238 best val_acc 44.238\n",
      "NDSS on Dir 0.1 + TrMean | r 2 e 20 val loss 1.210 val acc 56.479 best val_acc 56.479\n",
      "NDSS on Dir 0.1 + TrMean | r 2 e 30 val loss 1.098 val acc 61.127 best val_acc 61.127\n",
      "NDSS on Dir 0.1 + TrMean | r 2 e 40 val loss 1.062 val acc 63.346 best val_acc 63.854\n",
      "NDSS on Dir 0.1 + TrMean | r 2 e 49 val loss 1.159 val acc 60.141 best val_acc 63.854\n",
      "NDSS on Dir 0.1 + TrMean | r 2 e 50 val loss 1.093 val acc 61.465 best val_acc 63.854\n",
      "======= Results for NDSS attack on Dir -0.3  =====\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fcj_p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dir_p \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m======= Results for NDSS attack on Dir \u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[38;5;124m  =====\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dir_p)\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(results[\u001b[43mfcj_p\u001b[49m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fcj_p' is not defined"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "fast_ndss = True\n",
    "force = True\n",
    "params = [.1, .2, .3, .4, .5, 10]\n",
    "\n",
    "results = {}\n",
    "for dir_p in np.array(params)[::-1]:\n",
    "    results[dir_p] = []\n",
    "    for run in range(3):\n",
    "        print('===> Processing NDSS on Dir %.1f run %d' % (dir_p, run))\n",
    "        each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_data_dirichlet(all_data, num_workers, alpha=dir_p, force=force)\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = cnn().to(device)\n",
    "        fed_model.apply(init_weights)\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_updates=[]\n",
    "            benign_norm = 0\n",
    "\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(\n",
    "                        each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                        (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "            mal_update = ndss21_attack_trmean(user_updates[:nbyz], 5, dev_type='sign', threshold=20.0, threshold_diff=1e-5, fast_ndss=fast_ndss)\n",
    "            user_updates[:nbyz] = torch.stack([mal_update]*nbyz)\n",
    "            agg_update = tr_mean(user_updates, nbyz)\n",
    "\n",
    "            del user_updates\n",
    "            model_received = model_received + global_lr * agg_update\n",
    "            fed_model = cnn().to(device)\n",
    "            fed_model.apply(init_weights)\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "            if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "                print('NDSS on Dir %.1f + TrMean | r %d e %d val loss %.3f val acc %.3f best val_acc %.3f'% (dir_p, run, epoch_num, val_loss, val_acc, best_global_acc))\n",
    "            epoch_num+=1\n",
    "        results[dir_p].append(best_global_acc)\n",
    "\n",
    "for dir_p in params:\n",
    "    print('======= Results for NDSS attack on Dir %.1f  =====' % dir_p)\n",
    "    print(results[fcj_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4143f778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10.0: [85.25341034087481, 85.31315346679565, 84.92482327745455],\n",
       " 0.5: [80.94574411334298, 81.59283222902943, 81.05525133694032],\n",
       " 0.4: [76.18763070875353, 76.69554823089105, 76.3370182260362],\n",
       " 0.3: [74.48461307190932, 75.57016234126726, 74.77342892158508],\n",
       " 0.2: [72.6593625619592, 71.48406374501992, 71.55378486055777],\n",
       " 0.1: [60.4796974522293, 63.7937898089172, 63.853503184713375]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec12f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
