{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for CIFAR10with Fang distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e300ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu108/4303768/ipykernel_1097322/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "import copy\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77aa30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_models import *\n",
    "from cifar10_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0d17e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A', batch_norm=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        if batch_norm:\n",
    "            self.bn1 = nn.BatchNorm2d(planes)\n",
    "        else:\n",
    "            self.bn1 = nn.GroupNorm(min(32, planes), planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        if batch_norm:\n",
    "            self.bn2 = nn.BatchNorm2d(planes)\n",
    "        else:\n",
    "            self.bn2 = nn.GroupNorm(min(32, planes), planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        #print(out.shape)\n",
    "        #print(self.shortcut(x).shape)\n",
    "        #out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, batch_norm=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        if batch_norm:\n",
    "            self.bn1 = nn.BatchNorm2d(16)\n",
    "        else:\n",
    "            self.bn1 = nn.GroupNorm(16, 16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1, batch_norm=batch_norm)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2, batch_norm=batch_norm)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2, batch_norm=batch_norm)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride, batch_norm):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, batch_norm))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20(batch_norm):\n",
    "    return ResNet(BasicBlock, [3, 3, 3], batch_norm=batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "332a5386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "__all__ = ['densenet']\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, inplanes, expansion=4, growthRate=12, dropRate=0):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        planes = expansion * growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, growthRate, kernel_size=3, \n",
    "                               padding=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropRate = dropRate\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        if self.dropRate > 0:\n",
    "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
    "\n",
    "        out = torch.cat((x, out), 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, inplanes, expansion=1, growthRate=12, dropRate=0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        planes = expansion * growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = nn.Conv2d(inplanes, growthRate, kernel_size=3, \n",
    "                               padding=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropRate = dropRate\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        if self.dropRate > 0:\n",
    "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
    "\n",
    "        out = torch.cat((x, out), 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, inplanes, outplanes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = nn.Conv2d(inplanes, outplanes, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "\n",
    "    def __init__(self, depth=22, block=Bottleneck, \n",
    "        dropRate=0, num_classes=10, growthRate=12, compressionRate=2):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        assert (depth - 4) % 3 == 0, 'depth should be 3n+4'\n",
    "        n = (depth - 4) / 3 if block == BasicBlock else (depth - 4) // 6\n",
    "\n",
    "        self.growthRate = growthRate\n",
    "        self.dropRate = dropRate\n",
    "\n",
    "        # self.inplanes is a global variable used across multiple\n",
    "        # helper functions\n",
    "        self.inplanes = growthRate * 2 \n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.dense1 = self._make_denseblock(block, n)\n",
    "        self.trans1 = self._make_transition(compressionRate)\n",
    "        self.dense2 = self._make_denseblock(block, n)\n",
    "        self.trans2 = self._make_transition(compressionRate)\n",
    "        self.dense3 = self._make_denseblock(block, n)\n",
    "        self.bn = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(self.inplanes, num_classes)\n",
    "\n",
    "        # Weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_denseblock(self, block, blocks):\n",
    "        layers = []\n",
    "        for i in range(blocks):\n",
    "            # Currently we fix the expansion ratio as the default value\n",
    "            layers.append(block(self.inplanes, growthRate=self.growthRate, dropRate=self.dropRate))\n",
    "            self.inplanes += self.growthRate\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_transition(self, compressionRate):\n",
    "        inplanes = self.inplanes\n",
    "        outplanes = int(math.floor(self.inplanes // compressionRate))\n",
    "        self.inplanes = outplanes\n",
    "        return Transition(inplanes, outplanes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.trans1(self.dense1(x)) \n",
    "        x = self.trans2(self.dense2(x)) \n",
    "        x = self.dense3(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def densenet(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a ResNet model.\n",
    "    \"\"\"\n",
    "    return DenseNet(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "435f7498",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'arch': 'densenet',\n",
    "    'depth': 52,\n",
    "    'growthRate': 12,\n",
    "    'compressionRate': 2,\n",
    "    'drop': 0,\n",
    "    'num_classes': 10,\n",
    "}\n",
    "\n",
    "def densenet_gn():\n",
    "    return densenet(\n",
    "            num_classes=config['num_classes'],\n",
    "            depth=config['depth'],\n",
    "            growthRate=config['growthRate'],\n",
    "            compressionRate=config['compressionRate'],\n",
    "            dropRate=config['drop'],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "\n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fang_train_data(trainset, num_workers=100, bias=0.5):\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    per_participant_list = defaultdict(list)\n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        \n",
    "        per_participant_list[selected_worker].extend([i])\n",
    "    \n",
    "    return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "799ffb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_federated_data(trainset, num_workers, distribution='fang', param=1, force=False):\n",
    "    if distribution == 'fang':\n",
    "        per_participant_list = get_fang_train_data(trainset, num_workers, bias=param)\n",
    "    elif distribution == 'dirichlet':\n",
    "        per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=param, force=force)\n",
    "\n",
    "    each_worker_idx = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_te_idx = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(6*w_len/7)\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        \n",
    "        each_worker_idx[worker_idx] = w_indices[tr_idx]\n",
    "        each_worker_te_idx[worker_idx] = w_indices[te_idx]\n",
    "    \n",
    "    global_test_idx = np.concatenate(each_worker_te_idx)\n",
    "    \n",
    "    return each_worker_idx, each_worker_te_idx, global_test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2590db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(dataset, indices, batch_size=32, shuffle=False):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acbb06d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/home/vshejwalkar_umass_edu/data/'\n",
    "# load the train dataset\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_train)\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_train)\n",
    "\n",
    "te_cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_test)\n",
    "te_cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84b05be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbfgs(S_k_list, Y_k_list, v):\n",
    "    curr_S_k = torch.stack(S_k_list).T\n",
    "    curr_Y_k = torch.stack(Y_k_list).T\n",
    "    S_k_time_Y_k = np.dot(curr_S_k.T.cpu().numpy(), curr_Y_k.cpu().numpy())\n",
    "    S_k_time_S_k = np.dot(curr_S_k.T.cpu().numpy(), curr_S_k.cpu().numpy())\n",
    "    R_k = np.triu(S_k_time_Y_k)\n",
    "    L_k = S_k_time_Y_k - R_k\n",
    "    sigma_k = np.dot(Y_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()) / (np.dot(S_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()))\n",
    "    D_k_diag = np.diag(S_k_time_Y_k)\n",
    "    upper_mat = np.concatenate((sigma_k * S_k_time_S_k, L_k), axis=1)\n",
    "    lower_mat = np.concatenate((L_k.T, -np.diag(D_k_diag)), axis=1)\n",
    "    mat = np.concatenate((upper_mat, lower_mat), axis=0)\n",
    "    mat_inv = np.linalg.inv(mat)\n",
    "\n",
    "    approx_prod = sigma_k * v.cpu().numpy()\n",
    "    approx_prod = approx_prod.T\n",
    "    p_mat = np.concatenate((np.dot(curr_S_k.T.cpu().numpy(), sigma_k * v.unsqueeze(0).T.cpu().numpy()), np.dot(curr_Y_k.T.cpu().numpy(), v.unsqueeze(0).T.cpu().numpy())), axis=0)\n",
    "    approx_prod -= np.dot(np.dot(np.concatenate((sigma_k * curr_S_k.cpu().numpy(), curr_Y_k.cpu().numpy()), axis=1), mat_inv), p_mat)\n",
    "\n",
    "    return approx_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(20):\n",
    "        # apply attack to compromised worker devices with randomness\n",
    "        ##random_12 = 1. + np.random.uniform(size=vi_shape)\n",
    "        ##random_12 = torch.Tensor(random_12).float().cuda()\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc5c00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_mean(old_gradients, user_grads, b=0, hvp=None):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    agg_grads = torch.mean(user_grads,dim=0)\n",
    "    \n",
    "    return agg_grads, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72fe92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(old_gradients, user_grads, b=0, hvp=None):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    agg_grads = torch.median(user_grads, 0)[0]\n",
    "    \n",
    "    return agg_grads, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2626a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimmed_mean(old_gradients, user_grads, b=0, hvp=None):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    agg_grads = tr_mean(user_grads, 20)\n",
    "    \n",
    "    return agg_grads, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67adc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(score, nobyz):\n",
    "    estimator = KMeans(n_clusters=2)\n",
    "    estimator.fit(score.reshape(-1, 1))\n",
    "    label_pred = estimator.labels_\n",
    "    if np.mean(score[label_pred==0])<np.mean(score[label_pred==1]):\n",
    "        #0 is the label of malicious clients\n",
    "        label_pred = 1 - label_pred\n",
    "    real_label=np.ones(100)\n",
    "    real_label[:nobyz]=0\n",
    "    acc=len(label_pred[label_pred==real_label])/100\n",
    "    recall=1-np.sum(label_pred[:nobyz])/nobyz\n",
    "    fpr=1-np.sum(label_pred[nobyz:])/(100-nobyz)\n",
    "    fnr=np.sum(label_pred[:nobyz])/nobyz\n",
    "    print(\"acc %0.4f; recall %0.4f; fpr %0.4f; fnr %0.4f;\" % (acc, recall, fpr, fnr))\n",
    "    print(silhouette_score(score.reshape(-1, 1), label_pred))\n",
    "\n",
    "def detection1(score, nobyz):\n",
    "    nrefs = 10\n",
    "    ks = range(1, 8)\n",
    "    gaps = np.zeros(len(ks))\n",
    "    gapDiff = np.zeros(len(ks) - 1)\n",
    "    sdk = np.zeros(len(ks))\n",
    "    min = np.min(score)\n",
    "    max = np.max(score)\n",
    "    score = (score - min)/(max-min)\n",
    "    for i, k in enumerate(ks):\n",
    "        estimator = KMeans(n_clusters=k)\n",
    "        estimator.fit(score.reshape(-1, 1))\n",
    "        label_pred = estimator.labels_\n",
    "        center = estimator.cluster_centers_\n",
    "        Wk = np.sum([np.square(score[m]-center[label_pred[m]]) for m in range(len(score))])\n",
    "        WkRef = np.zeros(nrefs)\n",
    "        for j in range(nrefs):\n",
    "            rand = np.random.uniform(0, 1, len(score))\n",
    "            estimator = KMeans(n_clusters=k)\n",
    "            estimator.fit(rand.reshape(-1, 1))\n",
    "            label_pred = estimator.labels_\n",
    "            center = estimator.cluster_centers_\n",
    "            WkRef[j] = np.sum([np.square(rand[m]-center[label_pred[m]]) for m in range(len(rand))])\n",
    "        gaps[i] = np.log(np.mean(WkRef)) - np.log(Wk)\n",
    "        sdk[i] = np.sqrt((1.0 + nrefs) / nrefs) * np.std(np.log(WkRef))\n",
    "\n",
    "        if i > 0:\n",
    "            gapDiff[i - 1] = gaps[i - 1] - gaps[i] + sdk[i]\n",
    "    #print(gapDiff)\n",
    "    for i in range(len(gapDiff)):\n",
    "        if gapDiff[i] >= 0:\n",
    "            select_k = i+1\n",
    "            break\n",
    "    if select_k == 1:\n",
    "        print('No attack detected!')\n",
    "        return 0\n",
    "    else:\n",
    "        print('Attack Detected!')\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "faa727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(trainloader, model, model_received, criterion, optimizer, pgd=False, eps=2):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    for batch_ind, (inputs, targets) in enumerate(trainloader):\n",
    "\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size()[0])\n",
    "        top1.update(prec1.item()/100.0, inputs.size()[0])\n",
    "        top5.update(prec5.item()/100.0, inputs.size()[0])\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if pgd:\n",
    "            curr_model = list(model.parameters())\n",
    "            curr_model_vec = parameters_to_vector(curr_model)\n",
    "\n",
    "            if torch.norm(curr_model_vec - model_received) > eps:\n",
    "                curr_model_vec = eps*(curr_model_vec - model_received)/torch.norm(curr_model_vec - model_received) + model_received\n",
    "                vector_to_parameters(curr_model_vec, curr_model)\n",
    "        \n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def test(testloader, model, criterion):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    for batch_ind, (inputs, targets) in enumerate(testloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        top1.update(prec1/100.0, inputs.size()[0])\n",
    "        top5.update(prec5/100.0, inputs.size()[0])\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116ed4e",
   "metadata": {},
   "source": [
    "# Fang: Match baseline accuracy with 72% clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4a25069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 benign_norm 10.792 val loss 613.905 val acc 0.100 best val_acc 0.100\n",
      "e 10 benign_norm 11.402 val loss 44.903 val acc 0.100 best val_acc 0.100\n",
      "e 20 benign_norm 5.068 val loss 2.528 val acc 0.195 best val_acc 0.195\n",
      "e 30 benign_norm 3.076 val loss 2.148 val acc 0.211 best val_acc 0.211\n",
      "e 40 benign_norm 2.764 val loss 2.069 val acc 0.244 best val_acc 0.244\n",
      "e 50 benign_norm 3.010 val loss 2.126 val acc 0.201 best val_acc 0.244\n",
      "e 60 benign_norm 3.775 val loss 2.027 val acc 0.266 best val_acc 0.266\n",
      "e 70 benign_norm 2.731 val loss 1.951 val acc 0.269 best val_acc 0.269\n",
      "e 80 benign_norm 2.997 val loss 1.925 val acc 0.294 best val_acc 0.294\n",
      "e 90 benign_norm 2.723 val loss 1.807 val acc 0.318 best val_acc 0.318\n",
      "e 100 benign_norm 2.770 val loss 1.860 val acc 0.325 best val_acc 0.325\n",
      "e 110 benign_norm 2.862 val loss 1.658 val acc 0.381 best val_acc 0.381\n",
      "e 120 benign_norm 2.840 val loss 1.738 val acc 0.360 best val_acc 0.381\n",
      "e 130 benign_norm 3.042 val loss 1.607 val acc 0.404 best val_acc 0.404\n",
      "e 140 benign_norm 2.820 val loss 1.631 val acc 0.393 best val_acc 0.404\n",
      "e 150 benign_norm 3.045 val loss 1.633 val acc 0.413 best val_acc 0.413\n",
      "e 160 benign_norm 3.207 val loss 1.671 val acc 0.380 best val_acc 0.413\n",
      "e 170 benign_norm 3.077 val loss 1.580 val acc 0.433 best val_acc 0.433\n",
      "e 180 benign_norm 2.982 val loss 1.496 val acc 0.461 best val_acc 0.461\n",
      "e 190 benign_norm 3.249 val loss 1.687 val acc 0.402 best val_acc 0.461\n",
      "e 200 benign_norm 3.137 val loss 1.572 val acc 0.416 best val_acc 0.461\n",
      "e 210 benign_norm 2.740 val loss 1.455 val acc 0.475 best val_acc 0.475\n",
      "e 220 benign_norm 2.795 val loss 1.521 val acc 0.440 best val_acc 0.475\n",
      "e 230 benign_norm 2.948 val loss 1.623 val acc 0.408 best val_acc 0.475\n",
      "e 240 benign_norm 2.935 val loss 1.480 val acc 0.472 best val_acc 0.475\n",
      "e 250 benign_norm 2.815 val loss 1.426 val acc 0.489 best val_acc 0.489\n",
      "e 260 benign_norm 2.848 val loss 1.443 val acc 0.478 best val_acc 0.489\n",
      "e 270 benign_norm 2.929 val loss 1.396 val acc 0.501 best val_acc 0.501\n",
      "e 280 benign_norm 3.005 val loss 1.404 val acc 0.509 best val_acc 0.509\n",
      "e 290 benign_norm 2.951 val loss 1.465 val acc 0.475 best val_acc 0.509\n",
      "e 300 benign_norm 2.809 val loss 1.388 val acc 0.515 best val_acc 0.515\n",
      "e 310 benign_norm 2.862 val loss 1.321 val acc 0.520 best val_acc 0.520\n",
      "e 320 benign_norm 2.932 val loss 1.479 val acc 0.488 best val_acc 0.520\n",
      "e 330 benign_norm 2.829 val loss 1.424 val acc 0.493 best val_acc 0.520\n",
      "e 340 benign_norm 2.959 val loss 1.442 val acc 0.485 best val_acc 0.520\n",
      "e 350 benign_norm 2.981 val loss 1.329 val acc 0.515 best val_acc 0.520\n",
      "e 360 benign_norm 2.900 val loss 1.400 val acc 0.497 best val_acc 0.520\n",
      "e 370 benign_norm 2.875 val loss 1.472 val acc 0.488 best val_acc 0.520\n",
      "e 380 benign_norm 2.811 val loss 1.464 val acc 0.491 best val_acc 0.520\n",
      "e 390 benign_norm 2.868 val loss 1.402 val acc 0.498 best val_acc 0.520\n",
      "e 400 benign_norm 3.006 val loss 1.361 val acc 0.523 best val_acc 0.523\n",
      "e 410 benign_norm 2.884 val loss 1.508 val acc 0.471 best val_acc 0.523\n",
      "e 420 benign_norm 2.879 val loss 1.428 val acc 0.507 best val_acc 0.523\n",
      "e 430 benign_norm 2.933 val loss 1.369 val acc 0.517 best val_acc 0.523\n",
      "e 440 benign_norm 3.016 val loss 1.241 val acc 0.561 best val_acc 0.561\n",
      "e 450 benign_norm 2.874 val loss 1.259 val acc 0.549 best val_acc 0.561\n",
      "e 460 benign_norm 3.014 val loss 1.287 val acc 0.546 best val_acc 0.561\n",
      "e 470 benign_norm 2.892 val loss 1.293 val acc 0.544 best val_acc 0.561\n",
      "e 480 benign_norm 2.802 val loss 1.358 val acc 0.532 best val_acc 0.561\n",
      "e 490 benign_norm 2.951 val loss 1.144 val acc 0.596 best val_acc 0.596\n",
      "e 500 benign_norm 2.983 val loss 1.347 val acc 0.532 best val_acc 0.596\n",
      "e 510 benign_norm 3.070 val loss 1.293 val acc 0.540 best val_acc 0.596\n",
      "e 520 benign_norm 3.066 val loss 1.265 val acc 0.568 best val_acc 0.596\n",
      "e 530 benign_norm 2.794 val loss 1.149 val acc 0.600 best val_acc 0.600\n",
      "e 540 benign_norm 2.880 val loss 1.250 val acc 0.568 best val_acc 0.600\n",
      "e 550 benign_norm 2.773 val loss 1.199 val acc 0.579 best val_acc 0.600\n",
      "e 560 benign_norm 2.991 val loss 1.157 val acc 0.592 best val_acc 0.600\n",
      "e 570 benign_norm 3.111 val loss 1.259 val acc 0.559 best val_acc 0.600\n",
      "e 580 benign_norm 3.303 val loss 1.301 val acc 0.556 best val_acc 0.600\n",
      "e 590 benign_norm 2.871 val loss 1.163 val acc 0.586 best val_acc 0.600\n",
      "e 600 benign_norm 2.939 val loss 1.109 val acc 0.607 best val_acc 0.607\n",
      "e 610 benign_norm 2.713 val loss 1.080 val acc 0.623 best val_acc 0.623\n",
      "e 620 benign_norm 3.298 val loss 1.286 val acc 0.562 best val_acc 0.623\n",
      "e 630 benign_norm 3.173 val loss 1.287 val acc 0.559 best val_acc 0.623\n",
      "e 640 benign_norm 2.897 val loss 1.089 val acc 0.620 best val_acc 0.623\n",
      "e 650 benign_norm 2.910 val loss 1.176 val acc 0.590 best val_acc 0.623\n",
      "e 660 benign_norm 3.158 val loss 1.265 val acc 0.567 best val_acc 0.623\n",
      "e 670 benign_norm 2.960 val loss 1.057 val acc 0.624 best val_acc 0.624\n",
      "e 680 benign_norm 2.840 val loss 1.114 val acc 0.606 best val_acc 0.624\n",
      "e 690 benign_norm 2.841 val loss 1.078 val acc 0.623 best val_acc 0.624\n",
      "e 700 benign_norm 3.261 val loss 1.246 val acc 0.576 best val_acc 0.624\n",
      "e 710 benign_norm 3.040 val loss 1.270 val acc 0.562 best val_acc 0.624\n",
      "e 720 benign_norm 3.069 val loss 1.103 val acc 0.613 best val_acc 0.624\n",
      "e 730 benign_norm 3.165 val loss 1.196 val acc 0.576 best val_acc 0.624\n",
      "e 740 benign_norm 3.042 val loss 1.115 val acc 0.612 best val_acc 0.624\n",
      "e 750 benign_norm 3.117 val loss 1.117 val acc 0.608 best val_acc 0.624\n",
      "e 760 benign_norm 2.770 val loss 1.055 val acc 0.629 best val_acc 0.629\n",
      "e 770 benign_norm 2.900 val loss 0.999 val acc 0.653 best val_acc 0.653\n",
      "e 780 benign_norm 3.196 val loss 1.076 val acc 0.626 best val_acc 0.653\n",
      "e 790 benign_norm 2.972 val loss 1.001 val acc 0.651 best val_acc 0.653\n",
      "e 800 benign_norm 3.018 val loss 1.054 val acc 0.631 best val_acc 0.653\n",
      "e 810 benign_norm 3.012 val loss 1.079 val acc 0.616 best val_acc 0.653\n",
      "e 820 benign_norm 2.866 val loss 1.148 val acc 0.607 best val_acc 0.653\n",
      "e 830 benign_norm 3.109 val loss 1.106 val acc 0.618 best val_acc 0.653\n",
      "e 840 benign_norm 3.263 val loss 1.075 val acc 0.624 best val_acc 0.653\n",
      "e 850 benign_norm 2.899 val loss 0.996 val acc 0.657 best val_acc 0.657\n",
      "e 860 benign_norm 2.843 val loss 1.065 val acc 0.630 best val_acc 0.657\n",
      "e 870 benign_norm 2.882 val loss 1.118 val acc 0.615 best val_acc 0.657\n",
      "e 880 benign_norm 2.888 val loss 1.053 val acc 0.632 best val_acc 0.657\n",
      "e 890 benign_norm 3.098 val loss 1.176 val acc 0.601 best val_acc 0.657\n",
      "e 900 benign_norm 3.212 val loss 1.132 val acc 0.612 best val_acc 0.657\n",
      "e 910 benign_norm 2.962 val loss 0.963 val acc 0.660 best val_acc 0.660\n",
      "e 920 benign_norm 3.215 val loss 1.045 val acc 0.639 best val_acc 0.660\n",
      "e 930 benign_norm 3.117 val loss 1.042 val acc 0.638 best val_acc 0.660\n",
      "e 940 benign_norm 2.965 val loss 1.129 val acc 0.615 best val_acc 0.660\n",
      "e 950 benign_norm 3.217 val loss 1.002 val acc 0.655 best val_acc 0.660\n",
      "e 960 benign_norm 3.170 val loss 1.123 val acc 0.610 best val_acc 0.660\n",
      "e 970 benign_norm 3.060 val loss 1.110 val acc 0.625 best val_acc 0.660\n",
      "e 980 benign_norm 2.960 val loss 1.032 val acc 0.642 best val_acc 0.660\n",
      "e 990 benign_norm 3.193 val loss 1.025 val acc 0.644 best val_acc 0.660\n",
      "e 999 benign_norm 3.240 val loss 1.104 val acc 0.622 best val_acc 0.660\n",
      "e 1000 benign_norm 3.197 val loss 1.040 val acc 0.648 best val_acc 0.660\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "local_epochs = 1\n",
    "batch_size = 32\n",
    "num_workers = 100\n",
    "\n",
    "local_lr = 1\n",
    "global_lr = .5\n",
    "nepochs = 1000\n",
    "\n",
    "nbyz = 28\n",
    "byz_type = 'full_trim'\n",
    "aggregation = 'trim'\n",
    "\n",
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = True\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=100, distribution=distribution, param=param, force=force)\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    batch_size = batch_size\n",
    "    train_loaders.append((pos, get_train(all_data, indices, len(indices))))\n",
    "# test_loaders = []\n",
    "# for pos, indices in each_worker_te_idx.items():\n",
    "#     batch_size = batch_size\n",
    "#     train_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "resume=False\n",
    "round_nclients = num_workers\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "#         optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.99**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    agg_update = torch.mean(user_updates, 0)\n",
    "\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * (0.999**epoch_num) * agg_update\n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "        is_best = best_global_acc < val_acc\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee6ac22c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 benign_norm 13.333 val loss 460.855 val acc 0.098 best val_acc 0.098\n",
      "e 10 benign_norm 12.520 val loss 220.687 val acc 0.101 best val_acc 0.101\n",
      "e 20 benign_norm 8.690 val loss 26.854 val acc 0.096 best val_acc 0.101\n",
      "e 30 benign_norm 5.268 val loss 5.023 val acc 0.135 best val_acc 0.135\n",
      "e 40 benign_norm 4.187 val loss 2.753 val acc 0.182 best val_acc 0.182\n",
      "e 50 benign_norm 5.872 val loss 3.432 val acc 0.193 best val_acc 0.193\n",
      "e 60 benign_norm 4.144 val loss 2.472 val acc 0.202 best val_acc 0.202\n",
      "e 70 benign_norm 6.582 val loss 2.644 val acc 0.198 best val_acc 0.202\n",
      "e 80 benign_norm 6.601 val loss 4.257 val acc 0.205 best val_acc 0.205\n",
      "e 90 benign_norm 5.436 val loss 2.807 val acc 0.196 best val_acc 0.205\n",
      "e 100 benign_norm 5.843 val loss 3.094 val acc 0.194 best val_acc 0.205\n",
      "e 110 benign_norm 4.684 val loss 2.475 val acc 0.206 best val_acc 0.206\n",
      "e 120 benign_norm 4.989 val loss 2.561 val acc 0.190 best val_acc 0.206\n",
      "e 130 benign_norm 15.140 val loss 182.539 val acc 0.098 best val_acc 0.206\n",
      "e 140 benign_norm 6.954 val loss 4.430 val acc 0.184 best val_acc 0.206\n",
      "e 150 benign_norm 4.941 val loss 2.414 val acc 0.235 best val_acc 0.235\n",
      "e 160 benign_norm 4.802 val loss 2.176 val acc 0.255 best val_acc 0.255\n",
      "e 170 benign_norm 5.000 val loss 2.109 val acc 0.248 best val_acc 0.255\n",
      "e 180 benign_norm 4.631 val loss 2.120 val acc 0.194 best val_acc 0.255\n",
      "e 190 benign_norm 4.755 val loss 2.150 val acc 0.179 best val_acc 0.255\n",
      "e 200 benign_norm 4.800 val loss 2.119 val acc 0.206 best val_acc 0.255\n",
      "e 210 benign_norm 4.985 val loss 1.961 val acc 0.289 best val_acc 0.289\n",
      "e 220 benign_norm 4.537 val loss 2.028 val acc 0.220 best val_acc 0.289\n",
      "e 230 benign_norm 6.823 val loss 2.449 val acc 0.185 best val_acc 0.289\n",
      "e 240 benign_norm 5.255 val loss 2.019 val acc 0.215 best val_acc 0.289\n",
      "e 250 benign_norm 4.903 val loss 2.118 val acc 0.272 best val_acc 0.289\n",
      "e 260 benign_norm 5.799 val loss 2.069 val acc 0.205 best val_acc 0.289\n",
      "e 270 benign_norm 4.898 val loss 2.120 val acc 0.259 best val_acc 0.289\n",
      "e 280 benign_norm 5.361 val loss 2.054 val acc 0.231 best val_acc 0.289\n",
      "e 290 benign_norm 5.668 val loss 2.103 val acc 0.203 best val_acc 0.289\n",
      "e 310 benign_norm 4.337 val loss 2.114 val acc 0.225 best val_acc 0.289\n",
      "e 320 benign_norm 4.588 val loss 2.050 val acc 0.222 best val_acc 0.289\n",
      "e 330 benign_norm 4.433 val loss 1.912 val acc 0.256 best val_acc 0.289\n",
      "e 340 benign_norm 4.859 val loss 1.882 val acc 0.251 best val_acc 0.289\n",
      "e 350 benign_norm 4.557 val loss 2.053 val acc 0.275 best val_acc 0.289\n",
      "e 360 benign_norm 6.057 val loss 2.033 val acc 0.238 best val_acc 0.289\n",
      "e 370 benign_norm 5.682 val loss 2.394 val acc 0.164 best val_acc 0.289\n",
      "e 380 benign_norm 4.063 val loss 1.938 val acc 0.242 best val_acc 0.289\n",
      "e 390 benign_norm 3.765 val loss 1.918 val acc 0.261 best val_acc 0.289\n",
      "e 400 benign_norm 4.238 val loss 2.107 val acc 0.200 best val_acc 0.289\n",
      "e 410 benign_norm 3.909 val loss 1.841 val acc 0.280 best val_acc 0.289\n",
      "e 420 benign_norm 5.337 val loss 2.528 val acc 0.248 best val_acc 0.289\n",
      "e 430 benign_norm 4.560 val loss 1.892 val acc 0.320 best val_acc 0.320\n",
      "e 440 benign_norm 3.686 val loss 1.922 val acc 0.268 best val_acc 0.320\n",
      "e 450 benign_norm 3.759 val loss 1.904 val acc 0.268 best val_acc 0.320\n",
      "e 460 benign_norm 3.804 val loss 1.789 val acc 0.311 best val_acc 0.320\n",
      "e 470 benign_norm 3.992 val loss 1.978 val acc 0.248 best val_acc 0.320\n",
      "e 480 benign_norm 3.713 val loss 1.785 val acc 0.327 best val_acc 0.327\n",
      "e 490 benign_norm 3.787 val loss 1.795 val acc 0.325 best val_acc 0.327\n",
      "e 500 benign_norm 4.436 val loss 1.836 val acc 0.330 best val_acc 0.330\n",
      "e 510 benign_norm 3.978 val loss 1.927 val acc 0.296 best val_acc 0.330\n",
      "e 520 benign_norm 3.579 val loss 1.793 val acc 0.329 best val_acc 0.330\n",
      "e 530 benign_norm 3.797 val loss 1.806 val acc 0.320 best val_acc 0.330\n",
      "e 540 benign_norm 3.836 val loss 1.759 val acc 0.346 best val_acc 0.346\n",
      "e 550 benign_norm 3.784 val loss 1.793 val acc 0.340 best val_acc 0.346\n",
      "e 560 benign_norm 4.200 val loss 1.935 val acc 0.270 best val_acc 0.346\n",
      "e 570 benign_norm 3.567 val loss 1.862 val acc 0.311 best val_acc 0.346\n",
      "e 580 benign_norm 3.563 val loss 1.756 val acc 0.341 best val_acc 0.346\n",
      "e 590 benign_norm 3.764 val loss 1.769 val acc 0.340 best val_acc 0.346\n",
      "e 600 benign_norm 3.661 val loss 1.828 val acc 0.323 best val_acc 0.346\n",
      "e 610 benign_norm 3.720 val loss 1.767 val acc 0.349 best val_acc 0.349\n",
      "e 620 benign_norm 3.650 val loss 1.788 val acc 0.343 best val_acc 0.349\n",
      "e 630 benign_norm 3.601 val loss 1.816 val acc 0.329 best val_acc 0.349\n",
      "e 640 benign_norm 3.828 val loss 1.742 val acc 0.365 best val_acc 0.365\n",
      "e 650 benign_norm 3.708 val loss 1.863 val acc 0.309 best val_acc 0.365\n",
      "e 660 benign_norm 6.299 val loss 2.901 val acc 0.158 best val_acc 0.365\n",
      "e 670 benign_norm 3.610 val loss 1.881 val acc 0.322 best val_acc 0.365\n",
      "e 680 benign_norm 3.453 val loss 1.724 val acc 0.363 best val_acc 0.365\n",
      "e 690 benign_norm 3.846 val loss 1.767 val acc 0.353 best val_acc 0.365\n",
      "e 700 benign_norm 3.708 val loss 1.787 val acc 0.343 best val_acc 0.365\n",
      "e 710 benign_norm 3.682 val loss 1.802 val acc 0.341 best val_acc 0.365\n",
      "e 720 benign_norm 3.682 val loss 1.828 val acc 0.331 best val_acc 0.365\n",
      "e 730 benign_norm 3.657 val loss 1.854 val acc 0.321 best val_acc 0.365\n",
      "e 740 benign_norm 3.704 val loss 1.813 val acc 0.340 best val_acc 0.365\n",
      "e 750 benign_norm 3.642 val loss 1.743 val acc 0.361 best val_acc 0.365\n",
      "e 760 benign_norm 9.461 val loss 3.605 val acc 0.154 best val_acc 0.365\n",
      "e 770 benign_norm 3.526 val loss 1.825 val acc 0.351 best val_acc 0.365\n",
      "e 780 benign_norm 3.377 val loss 1.727 val acc 0.365 best val_acc 0.365\n",
      "e 790 benign_norm 3.876 val loss 1.843 val acc 0.324 best val_acc 0.365\n",
      "e 800 benign_norm 3.715 val loss 1.809 val acc 0.335 best val_acc 0.365\n",
      "e 810 benign_norm 3.697 val loss 1.851 val acc 0.323 best val_acc 0.365\n",
      "e 820 benign_norm 3.690 val loss 1.900 val acc 0.310 best val_acc 0.365\n",
      "e 830 benign_norm 3.772 val loss 1.797 val acc 0.342 best val_acc 0.365\n",
      "e 840 benign_norm 3.758 val loss 1.791 val acc 0.348 best val_acc 0.365\n",
      "e 850 benign_norm 3.682 val loss 1.935 val acc 0.302 best val_acc 0.365\n",
      "e 860 benign_norm 3.547 val loss 1.845 val acc 0.326 best val_acc 0.365\n",
      "e 870 benign_norm 3.942 val loss 1.811 val acc 0.343 best val_acc 0.365\n",
      "e 880 benign_norm 3.691 val loss 1.842 val acc 0.325 best val_acc 0.365\n",
      "e 890 benign_norm 3.475 val loss 1.864 val acc 0.323 best val_acc 0.365\n",
      "e 900 benign_norm 3.486 val loss 1.825 val acc 0.340 best val_acc 0.365\n",
      "e 910 benign_norm 4.066 val loss 1.836 val acc 0.347 best val_acc 0.365\n",
      "e 920 benign_norm 4.040 val loss 1.915 val acc 0.305 best val_acc 0.365\n",
      "e 930 benign_norm 4.253 val loss 1.904 val acc 0.316 best val_acc 0.365\n",
      "e 940 benign_norm 3.296 val loss 1.859 val acc 0.330 best val_acc 0.365\n",
      "e 950 benign_norm 3.355 val loss 1.815 val acc 0.344 best val_acc 0.365\n",
      "e 960 benign_norm 3.335 val loss 1.809 val acc 0.349 best val_acc 0.365\n",
      "e 970 benign_norm 3.341 val loss 1.840 val acc 0.342 best val_acc 0.365\n",
      "e 980 benign_norm 3.879 val loss 1.861 val acc 0.339 best val_acc 0.365\n",
      "e 990 benign_norm 4.219 val loss 1.876 val acc 0.326 best val_acc 0.365\n",
      "e 999 benign_norm 3.223 val loss 1.833 val acc 0.345 best val_acc 0.365\n",
      "e 1000 benign_norm 3.248 val loss 1.710 val acc 0.381 best val_acc 0.381\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "local_epochs = 1\n",
    "batch_size = 32\n",
    "num_workers = 100\n",
    "\n",
    "local_lr = 1\n",
    "global_lr = .5\n",
    "nepochs = 1000\n",
    "\n",
    "nbyz = 28\n",
    "byz_type = 'full_trim'\n",
    "aggregation = 'trim'\n",
    "\n",
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .9\n",
    "force = True\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=100, distribution=distribution, param=param, force=force)\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    batch_size = batch_size\n",
    "    train_loaders.append((pos, get_train(all_data, indices, len(indices))))\n",
    "# test_loaders = []\n",
    "# for pos, indices in each_worker_te_idx.items():\n",
    "#     batch_size = batch_size\n",
    "#     train_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "resume=False\n",
    "round_nclients = num_workers\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "#         optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.99**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    agg_update = torch.mean(user_updates, 0)\n",
    "\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * (0.999**epoch_num) * agg_update\n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "        is_best = best_global_acc < val_acc\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7649f7",
   "metadata": {},
   "source": [
    "# Fang + Resnet20-GN - Match baseline accuracy with 72% clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc35a377",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 benign_norm 3.228 val loss 2.833 val acc 0.100 best val_acc 0.100\n",
      "e 10 benign_norm 1.400 val loss 2.321 val acc 0.158 best val_acc 0.158\n",
      "e 20 benign_norm 1.498 val loss 2.307 val acc 0.145 best val_acc 0.158\n",
      "e 30 benign_norm 1.471 val loss 2.276 val acc 0.161 best val_acc 0.161\n",
      "e 40 benign_norm 1.467 val loss 2.243 val acc 0.172 best val_acc 0.172\n",
      "e 50 benign_norm 1.496 val loss 2.202 val acc 0.175 best val_acc 0.175\n",
      "e 60 benign_norm 1.487 val loss 2.196 val acc 0.183 best val_acc 0.183\n",
      "e 70 benign_norm 1.529 val loss 2.171 val acc 0.189 best val_acc 0.189\n",
      "e 80 benign_norm 1.601 val loss 2.163 val acc 0.188 best val_acc 0.189\n",
      "e 90 benign_norm 1.526 val loss 2.115 val acc 0.198 best val_acc 0.198\n",
      "e 100 benign_norm 1.612 val loss 2.117 val acc 0.201 best val_acc 0.201\n",
      "e 110 benign_norm 1.519 val loss 2.103 val acc 0.196 best val_acc 0.201\n",
      "e 120 benign_norm 1.568 val loss 2.042 val acc 0.225 best val_acc 0.225\n",
      "e 130 benign_norm 1.779 val loss 2.056 val acc 0.222 best val_acc 0.225\n",
      "e 140 benign_norm 1.725 val loss 2.032 val acc 0.232 best val_acc 0.232\n",
      "e 150 benign_norm 1.699 val loss 1.978 val acc 0.243 best val_acc 0.243\n",
      "e 160 benign_norm 2.405 val loss 2.124 val acc 0.202 best val_acc 0.243\n",
      "e 170 benign_norm 1.984 val loss 2.008 val acc 0.226 best val_acc 0.243\n",
      "e 180 benign_norm 1.986 val loss 1.910 val acc 0.268 best val_acc 0.268\n",
      "e 190 benign_norm 1.758 val loss 1.903 val acc 0.273 best val_acc 0.273\n",
      "e 200 benign_norm 2.086 val loss 1.952 val acc 0.237 best val_acc 0.273\n",
      "e 210 benign_norm 1.920 val loss 1.867 val acc 0.283 best val_acc 0.283\n",
      "e 220 benign_norm 2.209 val loss 1.907 val acc 0.254 best val_acc 0.283\n",
      "e 230 benign_norm 1.727 val loss 1.953 val acc 0.275 best val_acc 0.283\n",
      "e 240 benign_norm 1.889 val loss 1.896 val acc 0.301 best val_acc 0.301\n",
      "e 250 benign_norm 2.030 val loss 1.872 val acc 0.290 best val_acc 0.301\n",
      "e 260 benign_norm 1.906 val loss 1.798 val acc 0.323 best val_acc 0.323\n",
      "e 270 benign_norm 2.115 val loss 1.819 val acc 0.303 best val_acc 0.323\n",
      "e 280 benign_norm 1.972 val loss 1.742 val acc 0.338 best val_acc 0.338\n",
      "e 290 benign_norm 1.980 val loss 1.778 val acc 0.337 best val_acc 0.338\n",
      "e 300 benign_norm 2.178 val loss 1.822 val acc 0.327 best val_acc 0.338\n",
      "e 310 benign_norm 2.158 val loss 1.723 val acc 0.344 best val_acc 0.344\n",
      "e 320 benign_norm 2.139 val loss 1.726 val acc 0.352 best val_acc 0.352\n",
      "e 330 benign_norm 2.188 val loss 1.687 val acc 0.363 best val_acc 0.363\n",
      "e 340 benign_norm 2.316 val loss 1.706 val acc 0.355 best val_acc 0.363\n",
      "e 350 benign_norm 2.186 val loss 1.656 val acc 0.369 best val_acc 0.369\n",
      "e 360 benign_norm 2.282 val loss 1.639 val acc 0.375 best val_acc 0.375\n",
      "e 370 benign_norm 2.247 val loss 1.650 val acc 0.375 best val_acc 0.375\n",
      "e 380 benign_norm 2.314 val loss 1.629 val acc 0.385 best val_acc 0.385\n",
      "e 390 benign_norm 2.615 val loss 1.691 val acc 0.366 best val_acc 0.385\n",
      "e 400 benign_norm 2.522 val loss 1.621 val acc 0.385 best val_acc 0.385\n",
      "e 410 benign_norm 2.609 val loss 1.661 val acc 0.383 best val_acc 0.385\n",
      "e 420 benign_norm 2.464 val loss 1.616 val acc 0.390 best val_acc 0.390\n",
      "e 430 benign_norm 2.440 val loss 1.577 val acc 0.405 best val_acc 0.405\n",
      "e 440 benign_norm 2.753 val loss 1.579 val acc 0.402 best val_acc 0.405\n",
      "e 450 benign_norm 2.586 val loss 1.604 val acc 0.394 best val_acc 0.405\n",
      "e 460 benign_norm 2.554 val loss 1.555 val acc 0.414 best val_acc 0.414\n",
      "e 470 benign_norm 2.749 val loss 1.571 val acc 0.413 best val_acc 0.414\n",
      "e 480 benign_norm 2.584 val loss 1.549 val acc 0.414 best val_acc 0.414\n",
      "e 490 benign_norm 2.795 val loss 1.548 val acc 0.424 best val_acc 0.424\n",
      "e 500 benign_norm 2.798 val loss 1.555 val acc 0.403 best val_acc 0.424\n",
      "e 510 benign_norm 2.803 val loss 1.571 val acc 0.405 best val_acc 0.424\n",
      "e 520 benign_norm 2.791 val loss 1.542 val acc 0.419 best val_acc 0.424\n",
      "e 530 benign_norm 2.908 val loss 1.505 val acc 0.421 best val_acc 0.424\n",
      "e 540 benign_norm 3.175 val loss 1.592 val acc 0.405 best val_acc 0.424\n",
      "e 550 benign_norm 2.812 val loss 1.510 val acc 0.421 best val_acc 0.424\n",
      "e 560 benign_norm 2.795 val loss 1.494 val acc 0.430 best val_acc 0.430\n",
      "e 570 benign_norm 3.060 val loss 1.506 val acc 0.430 best val_acc 0.430\n",
      "e 580 benign_norm 3.214 val loss 1.527 val acc 0.424 best val_acc 0.430\n",
      "e 590 benign_norm 3.008 val loss 1.472 val acc 0.438 best val_acc 0.438\n",
      "e 600 benign_norm 3.032 val loss 1.464 val acc 0.443 best val_acc 0.443\n",
      "e 610 benign_norm 3.187 val loss 1.474 val acc 0.447 best val_acc 0.447\n",
      "e 620 benign_norm 3.125 val loss 1.448 val acc 0.445 best val_acc 0.447\n",
      "e 630 benign_norm 3.126 val loss 1.462 val acc 0.455 best val_acc 0.455\n",
      "e 640 benign_norm 3.058 val loss 1.450 val acc 0.456 best val_acc 0.456\n",
      "e 650 benign_norm 3.222 val loss 1.464 val acc 0.444 best val_acc 0.456\n",
      "e 660 benign_norm 3.304 val loss 1.444 val acc 0.450 best val_acc 0.456\n",
      "e 670 benign_norm 3.224 val loss 1.424 val acc 0.463 best val_acc 0.463\n",
      "e 680 benign_norm 3.214 val loss 1.423 val acc 0.465 best val_acc 0.465\n",
      "e 690 benign_norm 3.237 val loss 1.416 val acc 0.459 best val_acc 0.465\n",
      "e 700 benign_norm 3.277 val loss 1.437 val acc 0.447 best val_acc 0.465\n",
      "e 710 benign_norm 3.171 val loss 1.407 val acc 0.460 best val_acc 0.465\n",
      "e 720 benign_norm 3.303 val loss 1.409 val acc 0.467 best val_acc 0.467\n",
      "e 730 benign_norm 3.308 val loss 1.417 val acc 0.459 best val_acc 0.467\n",
      "e 740 benign_norm 3.433 val loss 1.442 val acc 0.449 best val_acc 0.467\n",
      "e 760 benign_norm 3.512 val loss 1.418 val acc 0.464 best val_acc 0.467\n",
      "e 770 benign_norm 3.373 val loss 1.395 val acc 0.468 best val_acc 0.468\n",
      "e 780 benign_norm 3.418 val loss 1.384 val acc 0.472 best val_acc 0.472\n",
      "e 790 benign_norm 3.483 val loss 1.382 val acc 0.477 best val_acc 0.477\n",
      "e 800 benign_norm 3.581 val loss 1.391 val acc 0.474 best val_acc 0.477\n",
      "e 810 benign_norm 3.615 val loss 1.388 val acc 0.474 best val_acc 0.477\n",
      "e 820 benign_norm 3.669 val loss 1.398 val acc 0.471 best val_acc 0.477\n",
      "e 830 benign_norm 3.710 val loss 1.398 val acc 0.469 best val_acc 0.477\n",
      "e 840 benign_norm 3.658 val loss 1.388 val acc 0.474 best val_acc 0.477\n",
      "e 850 benign_norm 3.650 val loss 1.378 val acc 0.477 best val_acc 0.477\n",
      "e 860 benign_norm 3.753 val loss 1.376 val acc 0.483 best val_acc 0.483\n",
      "e 870 benign_norm 3.788 val loss 1.374 val acc 0.489 best val_acc 0.489\n",
      "e 880 benign_norm 3.763 val loss 1.373 val acc 0.486 best val_acc 0.489\n",
      "e 890 benign_norm 3.752 val loss 1.371 val acc 0.483 best val_acc 0.489\n",
      "e 900 benign_norm 3.839 val loss 1.377 val acc 0.481 best val_acc 0.489\n",
      "e 910 benign_norm 3.848 val loss 1.376 val acc 0.478 best val_acc 0.489\n",
      "e 920 benign_norm 3.835 val loss 1.371 val acc 0.483 best val_acc 0.489\n",
      "e 930 benign_norm 3.853 val loss 1.372 val acc 0.484 best val_acc 0.489\n",
      "e 940 benign_norm 3.909 val loss 1.381 val acc 0.484 best val_acc 0.489\n",
      "e 950 benign_norm 3.879 val loss 1.379 val acc 0.486 best val_acc 0.489\n",
      "e 960 benign_norm 3.886 val loss 1.375 val acc 0.486 best val_acc 0.489\n",
      "e 970 benign_norm 3.895 val loss 1.372 val acc 0.486 best val_acc 0.489\n",
      "e 980 benign_norm 3.975 val loss 1.370 val acc 0.488 best val_acc 0.489\n",
      "e 990 benign_norm 3.984 val loss 1.369 val acc 0.489 best val_acc 0.489\n",
      "e 999 benign_norm 3.967 val loss 1.374 val acc 0.487 best val_acc 0.489\n",
      "e 1000 benign_norm 3.975 val loss 1.369 val acc 0.490 best val_acc 0.490\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "local_epochs = 1\n",
    "batch_size = 32\n",
    "num_workers = 100\n",
    "\n",
    "local_lr = 1\n",
    "global_lr = 1\n",
    "nepochs = 1000\n",
    "\n",
    "nbyz = 28\n",
    "byz_type = 'full_trim'\n",
    "aggregation = 'trim'\n",
    "\n",
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = True\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=100, distribution=distribution, param=param, force=force)\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    batch_size = batch_size\n",
    "    train_loaders.append((pos, get_train(all_data, indices, len(indices))))\n",
    "# test_loaders = []\n",
    "# for pos, indices in each_worker_te_idx.items():\n",
    "#     batch_size = batch_size\n",
    "#     train_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "resume=False\n",
    "round_nclients = num_workers\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "\n",
    "fed_model = resnet20(False).cuda()\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "#         optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.99**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    agg_update = torch.mean(user_updates, 0)\n",
    "\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * (0.997**epoch_num) * agg_update\n",
    "    \n",
    "    fed_model = resnet20(False).cuda()    \n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "        is_best = best_global_acc < val_acc\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfe807f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 benign_norm 3.475 val loss 2.888 val acc 0.097 best val_acc 0.097\n",
      "e 10 benign_norm 1.370 val loss 2.338 val acc 0.126 best val_acc 0.126\n",
      "e 20 benign_norm 1.468 val loss 2.330 val acc 0.155 best val_acc 0.155\n",
      "e 30 benign_norm 1.569 val loss 2.327 val acc 0.121 best val_acc 0.155\n",
      "e 40 benign_norm 1.607 val loss 2.318 val acc 0.127 best val_acc 0.155\n",
      "e 50 benign_norm 1.570 val loss 2.260 val acc 0.157 best val_acc 0.157\n",
      "e 60 benign_norm 1.633 val loss 2.241 val acc 0.159 best val_acc 0.159\n",
      "e 70 benign_norm 1.710 val loss 2.251 val acc 0.164 best val_acc 0.164\n",
      "e 80 benign_norm 1.570 val loss 2.178 val acc 0.187 best val_acc 0.187\n",
      "e 90 benign_norm 1.817 val loss 2.213 val acc 0.167 best val_acc 0.187\n",
      "e 100 benign_norm 1.761 val loss 2.159 val acc 0.170 best val_acc 0.187\n",
      "e 110 benign_norm 1.734 val loss 2.170 val acc 0.204 best val_acc 0.204\n",
      "e 120 benign_norm 1.626 val loss 2.104 val acc 0.210 best val_acc 0.210\n",
      "e 130 benign_norm 2.127 val loss 2.164 val acc 0.183 best val_acc 0.210\n",
      "e 140 benign_norm 1.723 val loss 2.047 val acc 0.215 best val_acc 0.215\n",
      "e 150 benign_norm 1.775 val loss 2.029 val acc 0.228 best val_acc 0.228\n",
      "e 160 benign_norm 1.619 val loss 1.983 val acc 0.240 best val_acc 0.240\n",
      "e 170 benign_norm 1.651 val loss 1.968 val acc 0.229 best val_acc 0.240\n",
      "e 180 benign_norm 1.606 val loss 1.929 val acc 0.260 best val_acc 0.260\n",
      "e 190 benign_norm 1.819 val loss 1.961 val acc 0.248 best val_acc 0.260\n",
      "e 200 benign_norm 1.693 val loss 1.942 val acc 0.257 best val_acc 0.260\n",
      "e 210 benign_norm 2.020 val loss 2.014 val acc 0.212 best val_acc 0.260\n",
      "e 220 benign_norm 1.911 val loss 1.882 val acc 0.267 best val_acc 0.267\n",
      "e 230 benign_norm 4.013 val loss 2.299 val acc 0.177 best val_acc 0.267\n",
      "e 240 benign_norm 1.779 val loss 1.972 val acc 0.262 best val_acc 0.267\n",
      "e 250 benign_norm 1.732 val loss 1.853 val acc 0.299 best val_acc 0.299\n",
      "e 260 benign_norm 2.052 val loss 2.036 val acc 0.232 best val_acc 0.299\n",
      "e 270 benign_norm 1.931 val loss 1.885 val acc 0.287 best val_acc 0.299\n",
      "e 280 benign_norm 1.746 val loss 1.831 val acc 0.309 best val_acc 0.309\n",
      "e 290 benign_norm 2.053 val loss 1.962 val acc 0.276 best val_acc 0.309\n",
      "e 300 benign_norm 1.752 val loss 1.762 val acc 0.343 best val_acc 0.343\n",
      "e 310 benign_norm 2.477 val loss 1.975 val acc 0.283 best val_acc 0.343\n",
      "e 320 benign_norm 1.747 val loss 1.778 val acc 0.316 best val_acc 0.343\n",
      "e 330 benign_norm 1.713 val loss 1.708 val acc 0.361 best val_acc 0.361\n",
      "e 340 benign_norm 2.440 val loss 1.960 val acc 0.282 best val_acc 0.361\n",
      "e 350 benign_norm 1.812 val loss 1.657 val acc 0.379 best val_acc 0.379\n",
      "e 360 benign_norm 1.946 val loss 1.771 val acc 0.373 best val_acc 0.379\n",
      "e 370 benign_norm 1.931 val loss 1.653 val acc 0.383 best val_acc 0.383\n",
      "e 380 benign_norm 1.991 val loss 1.721 val acc 0.372 best val_acc 0.383\n",
      "e 390 benign_norm 1.879 val loss 1.695 val acc 0.367 best val_acc 0.383\n",
      "e 400 benign_norm 1.982 val loss 1.670 val acc 0.383 best val_acc 0.383\n",
      "e 410 benign_norm 1.989 val loss 1.609 val acc 0.401 best val_acc 0.401\n",
      "e 420 benign_norm 1.926 val loss 1.667 val acc 0.367 best val_acc 0.401\n",
      "e 430 benign_norm 1.763 val loss 1.541 val acc 0.418 best val_acc 0.418\n",
      "e 440 benign_norm 1.901 val loss 1.594 val acc 0.407 best val_acc 0.418\n",
      "e 450 benign_norm 1.779 val loss 1.527 val acc 0.427 best val_acc 0.427\n",
      "e 460 benign_norm 1.893 val loss 1.517 val acc 0.423 best val_acc 0.427\n",
      "e 470 benign_norm 1.873 val loss 1.574 val acc 0.415 best val_acc 0.427\n",
      "e 480 benign_norm 2.044 val loss 1.585 val acc 0.392 best val_acc 0.427\n",
      "e 490 benign_norm 2.040 val loss 1.579 val acc 0.420 best val_acc 0.427\n",
      "e 500 benign_norm 2.200 val loss 1.662 val acc 0.392 best val_acc 0.427\n",
      "e 510 benign_norm 1.814 val loss 1.532 val acc 0.423 best val_acc 0.427\n",
      "e 520 benign_norm 2.079 val loss 1.554 val acc 0.420 best val_acc 0.427\n",
      "e 530 benign_norm 2.156 val loss 1.597 val acc 0.405 best val_acc 0.427\n",
      "e 540 benign_norm 1.978 val loss 1.461 val acc 0.445 best val_acc 0.445\n",
      "e 550 benign_norm 2.296 val loss 1.597 val acc 0.416 best val_acc 0.445\n",
      "e 560 benign_norm 1.877 val loss 1.446 val acc 0.480 best val_acc 0.480\n",
      "e 570 benign_norm 2.169 val loss 1.539 val acc 0.438 best val_acc 0.480\n",
      "e 580 benign_norm 2.316 val loss 1.616 val acc 0.417 best val_acc 0.480\n",
      "e 590 benign_norm 2.019 val loss 1.466 val acc 0.446 best val_acc 0.480\n",
      "e 600 benign_norm 2.102 val loss 1.434 val acc 0.459 best val_acc 0.480\n",
      "e 610 benign_norm 2.183 val loss 1.501 val acc 0.447 best val_acc 0.480\n",
      "e 620 benign_norm 1.996 val loss 1.412 val acc 0.479 best val_acc 0.480\n",
      "e 630 benign_norm 2.291 val loss 1.519 val acc 0.437 best val_acc 0.480\n",
      "e 640 benign_norm 2.397 val loss 1.608 val acc 0.422 best val_acc 0.480\n",
      "e 650 benign_norm 2.063 val loss 1.396 val acc 0.497 best val_acc 0.497\n",
      "e 660 benign_norm 2.314 val loss 1.418 val acc 0.483 best val_acc 0.497\n",
      "e 670 benign_norm 2.151 val loss 1.402 val acc 0.484 best val_acc 0.497\n",
      "e 680 benign_norm 2.082 val loss 1.361 val acc 0.490 best val_acc 0.497\n",
      "e 690 benign_norm 2.100 val loss 1.340 val acc 0.510 best val_acc 0.510\n",
      "e 700 benign_norm 2.648 val loss 1.647 val acc 0.400 best val_acc 0.510\n",
      "e 710 benign_norm 2.185 val loss 1.395 val acc 0.495 best val_acc 0.510\n",
      "e 720 benign_norm 2.166 val loss 1.341 val acc 0.507 best val_acc 0.510\n",
      "e 730 benign_norm 2.240 val loss 1.371 val acc 0.513 best val_acc 0.513\n",
      "e 740 benign_norm 2.153 val loss 1.379 val acc 0.491 best val_acc 0.513\n",
      "e 750 benign_norm 2.210 val loss 1.369 val acc 0.512 best val_acc 0.513\n",
      "e 760 benign_norm 2.234 val loss 1.383 val acc 0.493 best val_acc 0.513\n",
      "e 770 benign_norm 2.271 val loss 1.313 val acc 0.533 best val_acc 0.533\n",
      "e 780 benign_norm 2.226 val loss 1.343 val acc 0.505 best val_acc 0.533\n",
      "e 790 benign_norm 2.111 val loss 1.270 val acc 0.542 best val_acc 0.542\n",
      "e 800 benign_norm 2.241 val loss 1.301 val acc 0.532 best val_acc 0.542\n",
      "e 810 benign_norm 2.148 val loss 1.279 val acc 0.552 best val_acc 0.552\n",
      "e 820 benign_norm 2.264 val loss 1.308 val acc 0.537 best val_acc 0.552\n",
      "e 830 benign_norm 2.165 val loss 1.311 val acc 0.528 best val_acc 0.552\n",
      "e 840 benign_norm 2.316 val loss 1.320 val acc 0.517 best val_acc 0.552\n",
      "e 850 benign_norm 2.238 val loss 1.322 val acc 0.539 best val_acc 0.552\n",
      "e 860 benign_norm 2.108 val loss 1.188 val acc 0.581 best val_acc 0.581\n",
      "e 870 benign_norm 2.267 val loss 1.310 val acc 0.539 best val_acc 0.581\n",
      "e 880 benign_norm 2.424 val loss 1.332 val acc 0.524 best val_acc 0.581\n",
      "e 890 benign_norm 2.472 val loss 1.356 val acc 0.522 best val_acc 0.581\n",
      "e 900 benign_norm 2.501 val loss 1.278 val acc 0.547 best val_acc 0.581\n",
      "e 910 benign_norm 2.306 val loss 1.254 val acc 0.562 best val_acc 0.581\n",
      "e 920 benign_norm 2.446 val loss 1.263 val acc 0.545 best val_acc 0.581\n",
      "e 930 benign_norm 2.387 val loss 1.275 val acc 0.549 best val_acc 0.581\n",
      "e 940 benign_norm 2.377 val loss 1.289 val acc 0.555 best val_acc 0.581\n",
      "e 950 benign_norm 2.566 val loss 1.254 val acc 0.563 best val_acc 0.581\n",
      "e 960 benign_norm 2.403 val loss 1.227 val acc 0.564 best val_acc 0.581\n",
      "e 970 benign_norm 2.583 val loss 1.291 val acc 0.552 best val_acc 0.581\n",
      "e 980 benign_norm 2.505 val loss 1.240 val acc 0.563 best val_acc 0.581\n",
      "e 990 benign_norm 2.408 val loss 1.247 val acc 0.570 best val_acc 0.581\n",
      "e 999 benign_norm 2.718 val loss 1.294 val acc 0.562 best val_acc 0.581\n",
      "e 1000 benign_norm 2.716 val loss 1.333 val acc 0.549 best val_acc 0.581\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "local_epochs = 1\n",
    "batch_size = 32\n",
    "num_workers = 100\n",
    "\n",
    "local_lr = 1\n",
    "global_lr = 1\n",
    "nepochs = 1000\n",
    "\n",
    "nbyz = 28\n",
    "byz_type = 'full_trim'\n",
    "aggregation = 'trim'\n",
    "\n",
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = True\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=100, distribution=distribution, param=param, force=force)\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    batch_size = batch_size\n",
    "    train_loaders.append((pos, get_train(all_data, indices, len(indices))))\n",
    "# test_loaders = []\n",
    "# for pos, indices in each_worker_te_idx.items():\n",
    "#     batch_size = batch_size\n",
    "#     train_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "resume=False\n",
    "round_nclients = num_workers\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "\n",
    "fed_model = resnet20(False).cuda()\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "#         optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.99**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    agg_update = torch.mean(user_updates, 0)\n",
    "\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * (0.999**epoch_num) * agg_update\n",
    "    \n",
    "    fed_model = resnet20(False).cuda()    \n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "        is_best = best_global_acc < val_acc\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc93eb",
   "metadata": {},
   "source": [
    "# Densenet-GN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1298c1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 benign_norm 11.542 val loss 105.101 val acc 0.106 best val_acc 0.106\n",
      "e 10 benign_norm 4.876 val loss 2.974 val acc 0.179 best val_acc 0.179\n",
      "e 20 benign_norm 7.705 val loss 2.018 val acc 0.256 best val_acc 0.256\n",
      "e 30 benign_norm 2.213 val loss 1.989 val acc 0.264 best val_acc 0.264\n",
      "e 40 benign_norm 2.219 val loss 1.908 val acc 0.279 best val_acc 0.279\n",
      "e 50 benign_norm 2.279 val loss 1.886 val acc 0.299 best val_acc 0.299\n",
      "e 60 benign_norm 7.466 val loss 1.746 val acc 0.335 best val_acc 0.335\n",
      "e 70 benign_norm 2.389 val loss 1.710 val acc 0.369 best val_acc 0.369\n",
      "e 80 benign_norm 2.452 val loss 1.802 val acc 0.326 best val_acc 0.369\n",
      "e 90 benign_norm 7.525 val loss 2.053 val acc 0.272 best val_acc 0.369\n",
      "e 100 benign_norm 2.578 val loss 1.925 val acc 0.315 best val_acc 0.369\n",
      "e 110 benign_norm 2.632 val loss 1.762 val acc 0.368 best val_acc 0.369\n",
      "e 120 benign_norm 2.231 val loss 1.593 val acc 0.400 best val_acc 0.400\n",
      "e 130 benign_norm 7.488 val loss 1.557 val acc 0.418 best val_acc 0.418\n",
      "e 140 benign_norm 2.513 val loss 1.961 val acc 0.325 best val_acc 0.418\n",
      "e 150 benign_norm 2.481 val loss 1.748 val acc 0.373 best val_acc 0.418\n",
      "e 160 benign_norm 2.828 val loss 1.690 val acc 0.367 best val_acc 0.418\n",
      "e 170 benign_norm 2.328 val loss 1.904 val acc 0.360 best val_acc 0.418\n",
      "e 180 benign_norm 2.314 val loss 1.427 val acc 0.455 best val_acc 0.455\n",
      "e 190 benign_norm 2.219 val loss 1.556 val acc 0.459 best val_acc 0.459\n",
      "e 200 benign_norm 2.845 val loss 1.868 val acc 0.386 best val_acc 0.459\n",
      "e 210 benign_norm 2.261 val loss 1.513 val acc 0.449 best val_acc 0.459\n",
      "e 220 benign_norm 2.732 val loss 1.432 val acc 0.474 best val_acc 0.474\n",
      "e 230 benign_norm 2.443 val loss 1.413 val acc 0.493 best val_acc 0.493\n",
      "e 240 benign_norm 2.789 val loss 1.620 val acc 0.439 best val_acc 0.493\n",
      "e 250 benign_norm 2.496 val loss 1.374 val acc 0.498 best val_acc 0.498\n",
      "e 260 benign_norm 7.537 val loss 1.605 val acc 0.425 best val_acc 0.498\n",
      "e 270 benign_norm 3.032 val loss 1.372 val acc 0.521 best val_acc 0.521\n",
      "e 280 benign_norm 2.963 val loss 1.302 val acc 0.531 best val_acc 0.531\n",
      "e 290 benign_norm 2.995 val loss 1.443 val acc 0.494 best val_acc 0.531\n",
      "e 300 benign_norm 3.339 val loss 1.296 val acc 0.533 best val_acc 0.533\n",
      "e 310 benign_norm 7.519 val loss 1.200 val acc 0.578 best val_acc 0.578\n",
      "e 320 benign_norm 2.454 val loss 1.477 val acc 0.511 best val_acc 0.578\n",
      "e 330 benign_norm 2.792 val loss 1.493 val acc 0.485 best val_acc 0.578\n",
      "e 340 benign_norm 2.354 val loss 1.177 val acc 0.587 best val_acc 0.587\n",
      "e 350 benign_norm 2.426 val loss 1.165 val acc 0.591 best val_acc 0.591\n",
      "e 360 benign_norm 2.430 val loss 1.224 val acc 0.555 best val_acc 0.591\n",
      "e 370 benign_norm 2.550 val loss 1.146 val acc 0.583 best val_acc 0.591\n",
      "e 380 benign_norm 2.644 val loss 1.340 val acc 0.520 best val_acc 0.591\n",
      "e 390 benign_norm 7.642 val loss 1.535 val acc 0.524 best val_acc 0.591\n",
      "e 400 benign_norm 2.540 val loss 1.501 val acc 0.507 best val_acc 0.591\n",
      "e 410 benign_norm 2.494 val loss 1.162 val acc 0.583 best val_acc 0.591\n",
      "e 420 benign_norm 2.628 val loss 1.470 val acc 0.523 best val_acc 0.591\n",
      "e 430 benign_norm 2.453 val loss 1.105 val acc 0.607 best val_acc 0.607\n",
      "e 440 benign_norm 7.555 val loss 1.082 val acc 0.620 best val_acc 0.620\n",
      "e 450 benign_norm 2.772 val loss 1.285 val acc 0.550 best val_acc 0.620\n",
      "e 460 benign_norm 3.072 val loss 1.910 val acc 0.497 best val_acc 0.620\n",
      "e 470 benign_norm 2.503 val loss 1.138 val acc 0.608 best val_acc 0.620\n",
      "e 480 benign_norm 7.577 val loss 1.201 val acc 0.572 best val_acc 0.620\n",
      "e 490 benign_norm 2.852 val loss 1.195 val acc 0.582 best val_acc 0.620\n",
      "e 500 benign_norm 2.641 val loss 1.217 val acc 0.583 best val_acc 0.620\n",
      "e 510 benign_norm 3.959 val loss 1.204 val acc 0.569 best val_acc 0.620\n",
      "e 520 benign_norm 3.116 val loss 1.101 val acc 0.606 best val_acc 0.620\n",
      "e 530 benign_norm 2.535 val loss 1.172 val acc 0.615 best val_acc 0.620\n",
      "e 540 benign_norm 4.000 val loss 1.186 val acc 0.589 best val_acc 0.620\n",
      "e 550 benign_norm 2.805 val loss 1.079 val acc 0.613 best val_acc 0.620\n",
      "e 560 benign_norm 2.771 val loss 1.540 val acc 0.553 best val_acc 0.620\n",
      "e 570 benign_norm 4.159 val loss 1.058 val acc 0.627 best val_acc 0.627\n",
      "e 580 benign_norm 2.666 val loss 1.340 val acc 0.571 best val_acc 0.627\n",
      "e 590 benign_norm 2.781 val loss 1.012 val acc 0.646 best val_acc 0.646\n",
      "e 600 benign_norm 7.678 val loss 1.234 val acc 0.608 best val_acc 0.646\n",
      "e 610 benign_norm 3.029 val loss 1.260 val acc 0.589 best val_acc 0.646\n",
      "e 620 benign_norm 2.761 val loss 1.048 val acc 0.632 best val_acc 0.646\n",
      "e 630 benign_norm 2.802 val loss 1.195 val acc 0.615 best val_acc 0.646\n",
      "e 640 benign_norm 2.914 val loss 1.054 val acc 0.632 best val_acc 0.646\n",
      "e 650 benign_norm 4.567 val loss 1.185 val acc 0.612 best val_acc 0.646\n",
      "e 660 benign_norm 2.849 val loss 1.175 val acc 0.609 best val_acc 0.646\n",
      "e 670 benign_norm 2.994 val loss 1.168 val acc 0.593 best val_acc 0.646\n",
      "e 680 benign_norm 2.734 val loss 1.005 val acc 0.656 best val_acc 0.656\n",
      "e 690 benign_norm 3.014 val loss 1.048 val acc 0.635 best val_acc 0.656\n",
      "e 700 benign_norm 2.634 val loss 0.953 val acc 0.668 best val_acc 0.668\n",
      "e 710 benign_norm 2.846 val loss 1.013 val acc 0.653 best val_acc 0.668\n",
      "e 720 benign_norm 2.880 val loss 1.445 val acc 0.591 best val_acc 0.668\n",
      "e 730 benign_norm 3.141 val loss 1.142 val acc 0.618 best val_acc 0.668\n",
      "e 740 benign_norm 2.868 val loss 1.053 val acc 0.648 best val_acc 0.668\n",
      "e 750 benign_norm 3.064 val loss 1.223 val acc 0.620 best val_acc 0.668\n",
      "e 760 benign_norm 3.227 val loss 1.042 val acc 0.639 best val_acc 0.668\n",
      "e 770 benign_norm 2.872 val loss 1.347 val acc 0.598 best val_acc 0.668\n",
      "e 780 benign_norm 3.033 val loss 0.944 val acc 0.675 best val_acc 0.675\n",
      "e 790 benign_norm 2.796 val loss 0.992 val acc 0.655 best val_acc 0.675\n",
      "e 800 benign_norm 2.742 val loss 0.993 val acc 0.670 best val_acc 0.675\n",
      "e 810 benign_norm 2.901 val loss 0.925 val acc 0.677 best val_acc 0.677\n",
      "e 820 benign_norm 2.955 val loss 1.031 val acc 0.642 best val_acc 0.677\n",
      "e 830 benign_norm 3.106 val loss 1.032 val acc 0.649 best val_acc 0.677\n",
      "e 840 benign_norm 3.421 val loss 0.989 val acc 0.663 best val_acc 0.677\n",
      "e 850 benign_norm 3.132 val loss 1.160 val acc 0.621 best val_acc 0.677\n",
      "e 860 benign_norm 2.990 val loss 0.949 val acc 0.665 best val_acc 0.677\n",
      "e 870 benign_norm 2.945 val loss 0.938 val acc 0.680 best val_acc 0.680\n",
      "e 880 benign_norm 3.139 val loss 1.095 val acc 0.644 best val_acc 0.680\n",
      "e 890 benign_norm 3.064 val loss 0.970 val acc 0.662 best val_acc 0.680\n",
      "e 900 benign_norm 2.911 val loss 0.948 val acc 0.683 best val_acc 0.683\n",
      "e 910 benign_norm 3.187 val loss 1.269 val acc 0.615 best val_acc 0.683\n",
      "e 920 benign_norm 3.086 val loss 1.020 val acc 0.660 best val_acc 0.683\n",
      "e 930 benign_norm 3.110 val loss 1.056 val acc 0.640 best val_acc 0.683\n",
      "e 940 benign_norm 3.007 val loss 0.942 val acc 0.681 best val_acc 0.683\n",
      "e 950 benign_norm 3.114 val loss 1.026 val acc 0.664 best val_acc 0.683\n",
      "e 960 benign_norm 3.093 val loss 0.958 val acc 0.671 best val_acc 0.683\n",
      "e 970 benign_norm 3.167 val loss 0.949 val acc 0.679 best val_acc 0.683\n",
      "e 980 benign_norm 3.198 val loss 0.967 val acc 0.667 best val_acc 0.683\n",
      "e 990 benign_norm 3.234 val loss 0.905 val acc 0.687 best val_acc 0.687\n",
      "e 999 benign_norm 3.143 val loss 1.007 val acc 0.664 best val_acc 0.687\n",
      "e 1000 benign_norm 3.213 val loss 1.105 val acc 0.643 best val_acc 0.687\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "local_epochs = 1\n",
    "batch_size = 32\n",
    "num_workers = 100\n",
    "\n",
    "local_lr = 1\n",
    "global_lr = 1\n",
    "nepochs = 1000\n",
    "\n",
    "nbyz = 28\n",
    "byz_type = 'full_trim'\n",
    "aggregation = 'trim'\n",
    "\n",
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = True\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=100, distribution=distribution, param=param, force=force)\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    batch_size = batch_size\n",
    "    train_loaders.append((pos, get_train(all_data, indices, len(indices))))\n",
    "# test_loaders = []\n",
    "# for pos, indices in each_worker_te_idx.items():\n",
    "#     batch_size = batch_size\n",
    "#     train_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "resume=False\n",
    "round_nclients = num_workers\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "\n",
    "fed_model = densenet_gn().cuda()\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "#         optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.99**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    agg_update = torch.mean(user_updates, 0)\n",
    "\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * (0.999**epoch_num) * agg_update\n",
    "    \n",
    "    fed_model = densenet_gn().cuda()    \n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "        is_best = best_global_acc < val_acc\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6894ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
