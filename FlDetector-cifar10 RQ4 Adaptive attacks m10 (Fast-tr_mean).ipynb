{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for CIFAR10 with Fang/Dirichlet distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e300ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu105/4483553/ipykernel_2188274/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "import copy\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77aa30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_models import *\n",
    "from cifar10_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "\n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fang_train_data(trainset, num_workers=100, bias=0.5, force=False):\n",
    "    dist_file = 'fang_nworkers%d_bias%.1f.pkl' % (num_workers, bias)\n",
    "    if not force and os.path.exists(dist_file):\n",
    "        print('Loading fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "        return pickle.load(open(dist_file, 'rb'))\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    per_participant_list = defaultdict(list)\n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        per_participant_list[selected_worker].extend([i])\n",
    "    \n",
    "    print('Saving fang distribution for num_workers %d and bias %.1f to memory' % (num_workers, bias))\n",
    "    pickle.dump(per_participant_list, open(dist_file, 'wb'))\n",
    "    return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799ffb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_federated_data(trainset, num_workers, distribution='fang', param=1, force=False):\n",
    "    if distribution == 'fang':\n",
    "        per_participant_list = get_fang_train_data(trainset, num_workers, bias=param, force=force)\n",
    "    elif distribution == 'dirichlet':\n",
    "        per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=param, force=force)\n",
    "\n",
    "    each_worker_idx = [[] for _ in range(num_workers)] \n",
    "    each_worker_te_idx = [[] for _ in range(num_workers)]\n",
    "    np.random.seed(0)\n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(5*w_len/6)\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        each_worker_idx[worker_idx] = w_indices[tr_idx]\n",
    "        each_worker_te_idx[worker_idx] = w_indices[te_idx]\n",
    "    global_test_idx = np.concatenate(each_worker_te_idx)\n",
    "    return each_worker_idx, each_worker_te_idx, global_test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2590db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(dataset, indices, batch_size=32, shuffle=False):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acbb06d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/home/vshejwalkar_umass_edu/data/'\n",
    "# load the train dataset\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_train)\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_train)\n",
    "\n",
    "te_cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_test)\n",
    "te_cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(trainloader, model, model_received, criterion, optimizer, pgd=False, eps=2):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size()[0])\n",
    "        top1.update(prec1.item()/100.0, inputs.size()[0])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if pgd:\n",
    "            curr_model = list(model.parameters())\n",
    "            curr_model_vec = parameters_to_vector(curr_model)\n",
    "            if torch.norm(curr_model_vec - model_received) > eps:\n",
    "                curr_model_vec = eps*(curr_model_vec - model_received)/torch.norm(curr_model_vec - model_received) + model_received\n",
    "                vector_to_parameters(curr_model_vec, curr_model)\n",
    "        \n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def test(testloader, model, criterion):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(testloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        top1.update(prec1/100.0, inputs.size()[0])\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f843efb",
   "metadata": {},
   "source": [
    "# resnet-BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bb2ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5202826d",
   "metadata": {},
   "source": [
    "# FLDetector utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cabefee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def fldetector(old_gradients, user_grads, b=0, hvp=None, agr='tr_mean'):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    if agr == 'average':\n",
    "        agg_grads = torch.mean(user_grads, 0)\n",
    "    elif agr == 'median':\n",
    "        agg_grads = torch.median(user_grads, 0)[0]\n",
    "    elif agr == 'tr_mean':\n",
    "        agg_grads = tr_mean(user_grads, b)\n",
    "    return agg_grads, distance\n",
    "\n",
    "def lbfgs(S_k_list, Y_k_list, v):\n",
    "    curr_S_k = torch.stack(S_k_list).T\n",
    "    curr_Y_k = torch.stack(Y_k_list).T\n",
    "    S_k_time_Y_k = np.dot(curr_S_k.T.cpu().numpy(), curr_Y_k.cpu().numpy())\n",
    "    S_k_time_S_k = np.dot(curr_S_k.T.cpu().numpy(), curr_S_k.cpu().numpy())\n",
    "    R_k = np.triu(S_k_time_Y_k)\n",
    "    L_k = S_k_time_Y_k - R_k\n",
    "    sigma_k = np.dot(Y_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()) / (np.dot(S_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()))\n",
    "    D_k_diag = np.diag(S_k_time_Y_k)\n",
    "    upper_mat = np.concatenate((sigma_k * S_k_time_S_k, L_k), axis=1)\n",
    "    lower_mat = np.concatenate((L_k.T, -np.diag(D_k_diag)), axis=1)\n",
    "    mat = np.concatenate((upper_mat, lower_mat), axis=0)\n",
    "    mat_inv = np.linalg.inv(mat)\n",
    "\n",
    "    approx_prod = sigma_k * v.cpu().numpy()\n",
    "    approx_prod = approx_prod.T\n",
    "    p_mat = np.concatenate((np.dot(curr_S_k.T.cpu().numpy(), sigma_k * v.unsqueeze(0).T.cpu().numpy()), np.dot(curr_Y_k.T.cpu().numpy(), v.unsqueeze(0).T.cpu().numpy())), axis=0)\n",
    "    approx_prod -= np.dot(np.dot(np.concatenate((sigma_k * curr_S_k.cpu().numpy(), curr_Y_k.cpu().numpy()), axis=1), mat_inv), p_mat)\n",
    "\n",
    "    return approx_prod\n",
    "\n",
    "def detection(score, nobyz, nworkers):\n",
    "    estimator = KMeans(n_clusters=2)\n",
    "    estimator.fit(score.reshape(-1, 1))\n",
    "    label_pred = estimator.labels_\n",
    "    if np.mean(score[label_pred==0])<np.mean(score[label_pred==1]):\n",
    "        #0 is the label of malicious clients\n",
    "        label_pred = 1 - label_pred\n",
    "    real_label=np.ones(nworkers)\n",
    "    real_label[:nobyz]=0\n",
    "    acc=len(label_pred[label_pred==real_label])/nworkers\n",
    "    recall=1-np.sum(label_pred[:nobyz])/nobyz\n",
    "    fpr=1-np.sum(label_pred[nobyz:])/(nworkers-nobyz)\n",
    "    fnr=np.sum(label_pred[:nobyz])/nobyz\n",
    "    auc = roc_auc_score(real_label, label_pred)\n",
    "    print(\"acc %0.4f; recall %0.4f; fpr %0.4f; fnr %0.4f; auc %.4f\" % (acc, recall, fpr, fnr, auc))\n",
    "    return acc, fpr, fnr, auc\n",
    "    # print(silhouette_score(score.reshape(-1, 1), label_pred))\n",
    "\n",
    "def detection1(score, nobyz):\n",
    "    nrefs = 10\n",
    "    ks = range(1, 8)\n",
    "    gaps = np.zeros(len(ks))\n",
    "    gapDiff = np.zeros(len(ks) - 1)\n",
    "    sdk = np.zeros(len(ks))\n",
    "    min = np.min(score)\n",
    "    max = np.max(score)\n",
    "    score = (score - min)/(max-min)\n",
    "    for i, k in enumerate(ks):\n",
    "        estimator = KMeans(n_clusters=k)\n",
    "        estimator.fit(score.reshape(-1, 1))\n",
    "        label_pred = estimator.labels_\n",
    "        center = estimator.cluster_centers_\n",
    "        Wk = np.sum([np.square(score[m]-center[label_pred[m]]) for m in range(len(score))])\n",
    "        WkRef = np.zeros(nrefs)\n",
    "        for j in range(nrefs):\n",
    "            rand = np.random.uniform(0, 1, len(score))\n",
    "            estimator = KMeans(n_clusters=k)\n",
    "            estimator.fit(rand.reshape(-1, 1))\n",
    "            label_pred = estimator.labels_\n",
    "            center = estimator.cluster_centers_\n",
    "            WkRef[j] = np.sum([np.square(rand[m]-center[label_pred[m]]) for m in range(len(rand))])\n",
    "        gaps[i] = np.log(np.mean(WkRef)) - np.log(Wk)\n",
    "        sdk[i] = np.sqrt((1.0 + nrefs) / nrefs) * np.std(np.log(WkRef))\n",
    "\n",
    "        if i > 0:\n",
    "            gapDiff[i - 1] = gaps[i - 1] - gaps[i] + sdk[i]\n",
    "    #print(gapDiff)\n",
    "    for i in range(len(gapDiff)):\n",
    "        if gapDiff[i] >= 0:\n",
    "            select_k = i+1\n",
    "            break\n",
    "    if select_k == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        # print('Attack Detected!')\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e25e2ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fang distribution for num_workers 100 and bias 0.5 from memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 100, 10038)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = False\n",
    "\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=100, distribution=distribution, param=param, force=force)\n",
    "\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    train_loaders.append((pos, get_train(all_data, indices, batch_size)))\n",
    "test_loaders = []\n",
    "for pos, indices in enumerate(each_worker_te_idx):\n",
    "    test_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "len(train_loaders), len(test_loaders), len(global_test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "771158c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r 0 nmal 10 | e 0 benign_norm 2695.255 val loss 2.308 val acc 0.099 best val_acc 0.099\n",
      "r 0 nmal 10 | e 5 benign_norm 276.572 val loss 2.110 val acc 0.179 best val_acc 0.179\n",
      "r 0 nmal 10 | e 10 benign_norm 274.452 val loss 1.727 val acc 0.364 best val_acc 0.364\n",
      "r 0 nmal 10 | e 15 benign_norm 274.350 val loss 2.034 val acc 0.348 best val_acc 0.399\n",
      "r 0 nmal 10 | e 20 benign_norm 274.948 val loss 1.964 val acc 0.380 best val_acc 0.399\n",
      "!!!! Stop at iteration: 22\n",
      "r 0 nmal 10 | e 22 benign_norm 275.277 val loss 2.013 val acc 0.380 best val_acc 0.399\n",
      "acc 0.6778; recall 1.0000; fpr 0.3625; fnr 0.0000; auc 0.8187\n",
      "r 1 nmal 10 | e 0 benign_norm 7436.722 val loss 2.316 val acc 0.099 best val_acc 0.099\n",
      "r 1 nmal 10 | e 5 benign_norm 277.830 val loss 2.228 val acc 0.156 best val_acc 0.156\n",
      "r 1 nmal 10 | e 10 benign_norm 275.389 val loss 1.747 val acc 0.353 best val_acc 0.353\n",
      "r 1 nmal 10 | e 15 benign_norm 274.175 val loss 1.632 val acc 0.397 best val_acc 0.411\n",
      "r 1 nmal 10 | e 20 benign_norm 275.201 val loss 1.428 val acc 0.462 best val_acc 0.462\n",
      "r 1 nmal 10 | e 25 benign_norm 275.887 val loss 1.411 val acc 0.487 best val_acc 0.487\n",
      "r 1 nmal 10 | e 30 benign_norm 277.325 val loss 1.316 val acc 0.528 best val_acc 0.533\n",
      "r 1 nmal 10 | e 35 benign_norm 279.843 val loss 1.177 val acc 0.579 best val_acc 0.579\n",
      "r 1 nmal 10 | e 40 benign_norm 282.792 val loss 1.183 val acc 0.580 best val_acc 0.606\n",
      "r 1 nmal 10 | e 45 benign_norm 285.146 val loss 1.117 val acc 0.607 best val_acc 0.615\n",
      "r 1 nmal 10 | e 50 benign_norm 288.901 val loss 1.030 val acc 0.634 best val_acc 0.634\n",
      "r 1 nmal 10 | e 55 benign_norm 295.611 val loss 1.034 val acc 0.638 best val_acc 0.638\n",
      "r 1 nmal 10 | e 60 benign_norm 302.566 val loss 1.033 val acc 0.643 best val_acc 0.652\n",
      "r 1 nmal 10 | e 65 benign_norm 308.708 val loss 0.940 val acc 0.670 best val_acc 0.670\n",
      "r 1 nmal 10 | e 70 benign_norm 321.099 val loss 0.959 val acc 0.669 best val_acc 0.678\n",
      "r 1 nmal 10 | e 75 benign_norm 327.574 val loss 0.918 val acc 0.685 best val_acc 0.685\n",
      "r 1 nmal 10 | e 80 benign_norm 347.311 val loss 0.881 val acc 0.694 best val_acc 0.694\n",
      "r 1 nmal 10 | e 85 benign_norm 351.482 val loss 0.889 val acc 0.696 best val_acc 0.699\n",
      "r 1 nmal 10 | e 90 benign_norm 369.968 val loss 0.886 val acc 0.698 best val_acc 0.705\n",
      "r 1 nmal 10 | e 95 benign_norm 392.227 val loss 0.826 val acc 0.715 best val_acc 0.715\n",
      "r 1 nmal 10 | e 100 benign_norm 400.396 val loss 0.838 val acc 0.713 best val_acc 0.715\n",
      "r 1 nmal 10 | e 105 benign_norm 428.908 val loss 0.836 val acc 0.715 best val_acc 0.715\n",
      "r 1 nmal 10 | e 110 benign_norm 439.295 val loss 0.829 val acc 0.720 best val_acc 0.720\n",
      "r 1 nmal 10 | e 115 benign_norm 471.326 val loss 0.803 val acc 0.727 best val_acc 0.727\n",
      "!!!! Stop at iteration: 118\n",
      "r 1 nmal 10 | e 118 benign_norm 539.873 val loss 0.830 val acc 0.722 best val_acc 0.729\n",
      "acc 0.8111; recall 1.0000; fpr 0.2125; fnr 0.0000; auc 0.8938\n",
      "r 2 nmal 10 | e 0 benign_norm 5930.674 val loss 2.308 val acc 0.099 best val_acc 0.099\n",
      "r 2 nmal 10 | e 5 benign_norm 276.593 val loss 2.194 val acc 0.145 best val_acc 0.145\n",
      "r 2 nmal 10 | e 10 benign_norm 274.476 val loss 1.688 val acc 0.382 best val_acc 0.382\n",
      "r 2 nmal 10 | e 15 benign_norm 274.282 val loss 1.646 val acc 0.397 best val_acc 0.408\n",
      "r 2 nmal 10 | e 20 benign_norm 275.134 val loss 1.590 val acc 0.430 best val_acc 0.430\n",
      "r 2 nmal 10 | e 25 benign_norm 276.153 val loss 1.417 val acc 0.496 best val_acc 0.496\n",
      "r 2 nmal 10 | e 30 benign_norm 277.621 val loss 1.405 val acc 0.510 best val_acc 0.512\n",
      "r 2 nmal 10 | e 35 benign_norm 280.425 val loss 1.324 val acc 0.539 best val_acc 0.544\n",
      "r 2 nmal 10 | e 40 benign_norm 282.317 val loss 1.275 val acc 0.559 best val_acc 0.575\n",
      "r 2 nmal 10 | e 45 benign_norm 286.962 val loss 1.145 val acc 0.600 best val_acc 0.600\n",
      "r 2 nmal 10 | e 50 benign_norm 292.518 val loss 1.141 val acc 0.601 best val_acc 0.601\n",
      "r 2 nmal 10 | e 55 benign_norm 295.797 val loss 1.132 val acc 0.616 best val_acc 0.617\n",
      "r 2 nmal 10 | e 65 benign_norm 313.822 val loss 1.038 val acc 0.646 best val_acc 0.654\n",
      "r 2 nmal 10 | e 70 benign_norm 320.447 val loss 1.051 val acc 0.650 best val_acc 0.663\n",
      "r 2 nmal 10 | e 75 benign_norm 332.009 val loss 0.956 val acc 0.674 best val_acc 0.674\n",
      "r 2 nmal 10 | e 80 benign_norm 343.904 val loss 0.948 val acc 0.676 best val_acc 0.683\n",
      "r 2 nmal 10 | e 85 benign_norm 359.179 val loss 0.970 val acc 0.673 best val_acc 0.683\n",
      "r 2 nmal 10 | e 90 benign_norm 373.320 val loss 0.944 val acc 0.681 best val_acc 0.684\n",
      "r 2 nmal 10 | e 95 benign_norm 383.523 val loss 0.914 val acc 0.691 best val_acc 0.691\n",
      "r 2 nmal 10 | e 100 benign_norm 418.671 val loss 0.963 val acc 0.680 best val_acc 0.694\n",
      "r 2 nmal 10 | e 105 benign_norm 409.828 val loss 0.937 val acc 0.690 best val_acc 0.702\n",
      "r 2 nmal 10 | e 110 benign_norm 430.133 val loss 0.933 val acc 0.689 best val_acc 0.706\n",
      "r 2 nmal 10 | e 115 benign_norm 469.787 val loss 0.909 val acc 0.697 best val_acc 0.706\n",
      "r 2 nmal 10 | e 120 benign_norm 466.719 val loss 0.896 val acc 0.701 best val_acc 0.709\n",
      "r 2 nmal 10 | e 125 benign_norm 503.461 val loss 0.896 val acc 0.703 best val_acc 0.709\n",
      "r 2 nmal 10 | e 130 benign_norm 572.115 val loss 0.883 val acc 0.707 best val_acc 0.713\n",
      "r 2 nmal 10 | e 135 benign_norm 540.152 val loss 0.884 val acc 0.706 best val_acc 0.715\n",
      "r 2 nmal 10 | e 140 benign_norm 579.189 val loss 0.858 val acc 0.715 best val_acc 0.720\n",
      "r 2 nmal 10 | e 145 benign_norm 681.245 val loss 0.840 val acc 0.720 best val_acc 0.726\n",
      "r 2 nmal 10 | e 150 benign_norm 638.704 val loss 0.873 val acc 0.713 best val_acc 0.726\n",
      "r 2 nmal 10 | e 155 benign_norm 719.981 val loss 0.845 val acc 0.716 best val_acc 0.726\n",
      "r 2 nmal 10 | e 160 benign_norm 719.771 val loss 0.808 val acc 0.729 best val_acc 0.729\n",
      "r 2 nmal 10 | e 165 benign_norm 724.554 val loss 0.804 val acc 0.730 best val_acc 0.733\n",
      "r 2 nmal 10 | e 170 benign_norm 740.886 val loss 0.865 val acc 0.715 best val_acc 0.737\n",
      "r 2 nmal 10 | e 175 benign_norm 980.237 val loss 0.855 val acc 0.717 best val_acc 0.737\n",
      "r 2 nmal 10 | e 180 benign_norm 833.888 val loss 0.812 val acc 0.728 best val_acc 0.737\n",
      "!!!! Stop at iteration: 184\n",
      "r 2 nmal 10 | e 184 benign_norm 1378.978 val loss 0.842 val acc 0.722 best val_acc 0.739\n",
      "acc 0.9000; recall 1.0000; fpr 0.1125; fnr 0.0000; auc 0.9437\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_malicious = [10]\n",
    "n_runs = 3\n",
    "results={}\n",
    "for n_mal in n_malicious:\n",
    "    nbyz = n_mal\n",
    "    results[n_mal] = {'acc': [], 'fpr': [], 'fnr': [], 'auc': [], 'stop_e': []}\n",
    "    for run in range(n_runs):\n",
    "        torch.cuda.empty_cache()\n",
    "        nepochs=200\n",
    "        local_epochs = 2\n",
    "        batch_size = 16\n",
    "        num_workers = 100\n",
    "        local_lr = 0.1\n",
    "        global_lr = 1\n",
    "        agr = 'tr_mean'\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = resnet20().cuda()\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        best_accs_per_round = []\n",
    "        accs_per_round = []\n",
    "        loss_per_round = []\n",
    "        home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "        # FLDetector initializations\n",
    "        weight_record = []\n",
    "        grad_record = []\n",
    "        test_grads = []\n",
    "        old_grad_list = []\n",
    "        malicious_scores = np.zeros((1, num_workers-(20-nbyz)))\n",
    "        start_detection_epoch = 10\n",
    "        window_size = 10\n",
    "        assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "        # Adaptive attack initializers\n",
    "        good_distance_range = np.zeros((1, nbyz))\n",
    "        attack_type = 'NDSS21'\n",
    "        dev_type = 'std'\n",
    "        \n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(20-nbyz, num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_grads=[]\n",
    "            benign_norm = 0\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "            weight = model_received\n",
    "\n",
    "            if (epoch_num > start_detection_epoch):\n",
    "                hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "                hvp = np.squeeze(hvp)\n",
    "            else:\n",
    "                hvp = None\n",
    "\n",
    "            good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "            if hvp is not None:\n",
    "                pred_grad = copy.deepcopy(good_old_grads)\n",
    "                distance = []\n",
    "                for i in range(len(good_old_grads)):\n",
    "                    pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "                good_distance_range = np.concatenate(\n",
    "                    (good_distance_range, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "            if attack_type != 'none' and (epoch_num < start_detection_epoch):\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "                # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "            elif epoch_num > start_detection_epoch:\n",
    "                if attack_type == 'full_trim':\n",
    "                    user_grads = full_trim(user_grads, nbyz)\n",
    "                elif attack_type == 'none':\n",
    "                    pass\n",
    "                else:\n",
    "                    if attack_type == 'NDSS21':\n",
    "                        distance_bound = np.random.choice(np.mean(good_distance_range[-1:], 0))\n",
    "                        model_re = torch.mean(good_current_grads, dim=0)\n",
    "                        if dev_type == 'unit_vec':\n",
    "                            deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                        elif dev_type == 'sign':\n",
    "                            deviation = torch.sign(model_re)\n",
    "                        elif dev_type == 'std':\n",
    "                            deviation = torch.std(good_current_grads, 0)\n",
    "                        # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_range[-1]))) / torch.norm(deviation))\n",
    "                        noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                    elif attack_type == 'mod_trim':\n",
    "                        mal_grads= full_trim(user_grads[:nbyz], nbyz)\n",
    "                        pass\n",
    "                    else:\n",
    "                        noise = torch.zeros(hvp.shape).to(device)\n",
    "                    for m in range(nbyz):\n",
    "                        user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "            agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "            if distance is not None and epoch_num > (start_detection_epoch - window_size):\n",
    "                malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "            if malicious_scores.shape[0] >= window_size+1:\n",
    "                if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                    print('!!!! Stop at iteration:', epoch_num)\n",
    "                    print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "                    acc, fpr, fnr, auc = detection(\n",
    "                        np.sum(malicious_scores[-window_size:], axis=0), nbyz, len(user_grads))\n",
    "                    results[n_mal]['acc'].append(acc)\n",
    "                    results[n_mal]['fpr'].append(fpr)\n",
    "                    results[n_mal]['fnr'].append(fnr)\n",
    "                    results[n_mal]['auc'].append(auc)\n",
    "                    results[n_mal]['stop_e'].append(epoch_num)\n",
    "                    break\n",
    "                    break\n",
    "\n",
    "            if epoch_num > (start_detection_epoch - window_size):\n",
    "                weight_record.append(weight - last_weight)\n",
    "                grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "            if (len(weight_record) > window_size):\n",
    "                del weight_record[0]\n",
    "                del grad_record[0]\n",
    "\n",
    "            last_weight = weight\n",
    "            last_grad = agg_grads\n",
    "            old_grad_list = user_grads\n",
    "            good_old_grads = good_current_grads\n",
    "            del user_grads\n",
    "\n",
    "            model_received = model_received + global_lr * agg_grads\n",
    "            fed_model = resnet20().cuda()\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "            best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "            accs_per_round.append(val_acc.cpu().item())\n",
    "            loss_per_round.append(val_loss.cpu().item())\n",
    "\n",
    "            if epoch_num%5==0 or epoch_num==nepochs-1:\n",
    "                print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "            if math.isnan(val_loss) or val_loss > 100000:\n",
    "                print('val loss %f... exit'%val_loss)\n",
    "                break\n",
    "            epoch_num+=1\n",
    "\n",
    "        final_accs_per_client=[]\n",
    "        for i in range(num_workers):\n",
    "            client_loss, client_acc = test(test_loaders[i][1], fed_model, criterion)\n",
    "            final_accs_per_client.append(client_acc.cpu().item())\n",
    "        all_results = collections.OrderedDict(\n",
    "            final_accs_per_client=np.array(final_accs_per_client),\n",
    "            accs_per_round=np.array(accs_per_round),\n",
    "            best_accs_per_round=np.array(best_accs_per_round),\n",
    "            loss_per_round=np.array(loss_per_round),\n",
    "            results=results\n",
    "        )\n",
    "        pickle.dump(all_results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq4_adaptive_cifar10_fast_trmean_NDSS21_std_m%d_r%d.pkl' % (nbyz, run)), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4e6d834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r 0 nmal 10 | e 0 benign_norm 1132.689 val loss 2.313 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 10 | e 5 benign_norm 276.867 val loss 2.207 val acc 0.185 best val_acc 0.185\n",
      "r 0 nmal 10 | e 10 benign_norm 274.338 val loss 1.661 val acc 0.382 best val_acc 0.382\n",
      "r 0 nmal 10 | e 15 benign_norm 273.673 val loss 1.694 val acc 0.384 best val_acc 0.384\n",
      "r 0 nmal 10 | e 20 benign_norm 274.293 val loss 1.673 val acc 0.413 best val_acc 0.414\n",
      "r 0 nmal 10 | e 25 benign_norm 275.130 val loss 1.481 val acc 0.473 best val_acc 0.473\n",
      "r 0 nmal 10 | e 30 benign_norm 276.635 val loss 1.411 val acc 0.511 best val_acc 0.511\n",
      "r 0 nmal 10 | e 35 benign_norm 278.580 val loss 1.329 val acc 0.539 best val_acc 0.561\n",
      "r 0 nmal 10 | e 40 benign_norm 280.458 val loss 1.314 val acc 0.553 best val_acc 0.561\n",
      "r 0 nmal 10 | e 45 benign_norm 284.015 val loss 1.302 val acc 0.563 best val_acc 0.574\n",
      "r 0 nmal 10 | e 50 benign_norm 288.785 val loss 1.247 val acc 0.585 best val_acc 0.585\n",
      "r 0 nmal 10 | e 55 benign_norm 294.493 val loss 1.129 val acc 0.609 best val_acc 0.609\n",
      "r 0 nmal 10 | e 60 benign_norm 298.786 val loss 1.143 val acc 0.610 best val_acc 0.612\n",
      "r 0 nmal 10 | e 65 benign_norm 306.346 val loss 1.172 val acc 0.610 best val_acc 0.623\n",
      "r 0 nmal 10 | e 70 benign_norm 317.194 val loss 1.063 val acc 0.643 best val_acc 0.643\n",
      "r 0 nmal 10 | e 75 benign_norm 325.991 val loss 1.002 val acc 0.661 best val_acc 0.661\n",
      "r 0 nmal 10 | e 80 benign_norm 337.594 val loss 0.973 val acc 0.672 best val_acc 0.672\n",
      "r 0 nmal 10 | e 85 benign_norm 353.999 val loss 1.030 val acc 0.660 best val_acc 0.672\n",
      "r 0 nmal 10 | e 90 benign_norm 364.377 val loss 1.000 val acc 0.670 best val_acc 0.679\n",
      "r 0 nmal 10 | e 95 benign_norm 380.371 val loss 0.984 val acc 0.675 best val_acc 0.682\n",
      "r 0 nmal 10 | e 100 benign_norm 388.430 val loss 0.924 val acc 0.694 best val_acc 0.694\n",
      "r 0 nmal 10 | e 105 benign_norm 402.661 val loss 0.895 val acc 0.700 best val_acc 0.700\n",
      "r 0 nmal 10 | e 110 benign_norm 425.154 val loss 0.935 val acc 0.694 best val_acc 0.706\n",
      "r 0 nmal 10 | e 115 benign_norm 428.215 val loss 0.905 val acc 0.702 best val_acc 0.715\n",
      "r 0 nmal 10 | e 120 benign_norm 468.307 val loss 0.859 val acc 0.715 best val_acc 0.715\n",
      "r 0 nmal 10 | e 125 benign_norm 487.041 val loss 0.900 val acc 0.706 best val_acc 0.715\n",
      "r 0 nmal 10 | e 130 benign_norm 488.285 val loss 0.852 val acc 0.721 best val_acc 0.728\n",
      "r 0 nmal 10 | e 135 benign_norm 541.869 val loss 0.816 val acc 0.728 best val_acc 0.729\n",
      "r 0 nmal 10 | e 140 benign_norm 529.184 val loss 0.790 val acc 0.739 best val_acc 0.739\n",
      "!!!! Stop at iteration: 145\n",
      "r 0 nmal 10 | e 145 benign_norm 555.257 val loss 0.767 val acc 0.744 best val_acc 0.744\n",
      "acc 0.8778; recall 0.0000; fpr 0.0125; fnr 1.0000; auc 0.4938\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_malicious = [10]\n",
    "n_runs = 1\n",
    "results={}\n",
    "for n_mal in n_malicious:\n",
    "    nbyz = n_mal\n",
    "    results[n_mal] = {'acc': [], 'fpr': [], 'fnr': [], 'auc': [], 'stop_e': []}\n",
    "    for run in range(n_runs):\n",
    "        torch.cuda.empty_cache()\n",
    "        nepochs=200\n",
    "        local_epochs = 2\n",
    "        batch_size = 16\n",
    "        num_workers = 100\n",
    "        local_lr = 0.1\n",
    "        global_lr = 1\n",
    "        agr = 'tr_mean'\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = resnet20().cuda()\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        best_accs_per_round = []\n",
    "        accs_per_round = []\n",
    "        loss_per_round = []\n",
    "        home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "        # FLDetector initializations\n",
    "        weight_record = []\n",
    "        grad_record = []\n",
    "        test_grads = []\n",
    "        old_grad_list = []\n",
    "        malicious_scores = np.zeros((1, num_workers-(20-nbyz)))\n",
    "        start_detection_epoch = 10\n",
    "        window_size = 10\n",
    "        assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "        # Adaptive attack initializers\n",
    "        good_distance_range = np.zeros((1, nbyz))\n",
    "        attack_type = 'NDSS21'\n",
    "        dev_type = 'std'\n",
    "        \n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(20-nbyz, num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_grads=[]\n",
    "            benign_norm = 0\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "            weight = model_received\n",
    "\n",
    "            if (epoch_num > start_detection_epoch):\n",
    "                hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "                hvp = np.squeeze(hvp)\n",
    "            else:\n",
    "                hvp = None\n",
    "\n",
    "            good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "            if hvp is not None:\n",
    "                pred_grad = copy.deepcopy(good_old_grads)\n",
    "                distance = []\n",
    "                for i in range(len(good_old_grads)):\n",
    "                    pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "                good_distance_range = np.concatenate(\n",
    "                    (good_distance_range, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "            if attack_type != 'none' and (epoch_num < start_detection_epoch):\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "                # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "            elif epoch_num > start_detection_epoch:\n",
    "                if attack_type == 'full_trim':\n",
    "                    user_grads = full_trim(user_grads, nbyz)\n",
    "                elif attack_type == 'none':\n",
    "                    pass\n",
    "                else:\n",
    "                    if attack_type == 'NDSS21':\n",
    "                        distance_bound = np.mean(np.mean(good_distance_range[-1:], 0))\n",
    "                        model_re = torch.mean(good_current_grads, dim=0)\n",
    "                        if dev_type == 'unit_vec':\n",
    "                            deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                        elif dev_type == 'sign':\n",
    "                            deviation = torch.sign(model_re)\n",
    "                        elif dev_type == 'std':\n",
    "                            deviation = torch.std(good_current_grads, 0)\n",
    "                        # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_range[-1]))) / torch.norm(deviation))\n",
    "                        noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                    elif attack_type == 'mod_trim':\n",
    "                        mal_grads= full_trim(user_grads[:nbyz], nbyz)\n",
    "                        pass\n",
    "                    else:\n",
    "                        noise = torch.zeros(hvp.shape).to(device)\n",
    "                    for m in range(nbyz):\n",
    "                        user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "            agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "            if distance is not None and epoch_num > (start_detection_epoch - window_size):\n",
    "                malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "            if malicious_scores.shape[0] >= window_size+1:\n",
    "                if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                    print('!!!! Stop at iteration:', epoch_num)\n",
    "                    print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "                    acc, fpr, fnr, auc = detection(\n",
    "                        np.sum(malicious_scores[-window_size:], axis=0), nbyz, len(user_grads))\n",
    "                    results[n_mal]['acc'].append(acc)\n",
    "                    results[n_mal]['fpr'].append(fpr)\n",
    "                    results[n_mal]['fnr'].append(fnr)\n",
    "                    results[n_mal]['auc'].append(auc)\n",
    "                    results[n_mal]['stop_e'].append(epoch_num)\n",
    "                    break\n",
    "                    break\n",
    "\n",
    "            if epoch_num > (start_detection_epoch - window_size):\n",
    "                weight_record.append(weight - last_weight)\n",
    "                grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "            if (len(weight_record) > window_size):\n",
    "                del weight_record[0]\n",
    "                del grad_record[0]\n",
    "\n",
    "            last_weight = weight\n",
    "            last_grad = agg_grads\n",
    "            old_grad_list = user_grads\n",
    "            good_old_grads = good_current_grads\n",
    "            del user_grads\n",
    "\n",
    "            model_received = model_received + global_lr * agg_grads\n",
    "            fed_model = resnet20().cuda()\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "            if is_best:\n",
    "                best_model = copy.deepcopy(fed_model)\n",
    "\n",
    "            best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "            accs_per_round.append(val_acc.cpu().item())\n",
    "            loss_per_round.append(val_loss.cpu().item())\n",
    "\n",
    "            if epoch_num%5==0 or epoch_num==nepochs-1:\n",
    "                print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "            if math.isnan(val_loss) or val_loss > 100000:\n",
    "                print('val loss %f... exit'%val_loss)\n",
    "                break\n",
    "            epoch_num+=1\n",
    "\n",
    "        final_accs_per_client=[]\n",
    "        for i in range(num_workers):\n",
    "            client_loss, client_acc = test(test_loaders[i][1], best_model, criterion)\n",
    "            final_accs_per_client.append(client_acc.cpu().item())\n",
    "        all_results = collections.OrderedDict(\n",
    "            final_accs_per_client=np.array(final_accs_per_client),\n",
    "            accs_per_round=np.array(accs_per_round),\n",
    "            best_accs_per_round=np.array(best_accs_per_round),\n",
    "            loss_per_round=np.array(loss_per_round),\n",
    "            results=results\n",
    "        )\n",
    "        pickle.dump(all_results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq4_adaptive_cifar10_fast_trmean_NDSS21_std_m%d_r%d_personalized.pkl' % (nbyz, run)), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbcb183",
   "metadata": {},
   "source": [
    "# With dirichlet distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95990b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 10043)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "distribution='dirichlet'\n",
    "param = .1\n",
    "force = False\n",
    "\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=100, distribution=distribution, param=param, force=force)\n",
    "\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    train_loaders.append((pos, get_train(all_data, indices, batch_size)))\n",
    "test_loaders = []\n",
    "for pos, indices in enumerate(each_worker_te_idx):\n",
    "    test_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "len(train_loaders), len(test_loaders), len(global_test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b738e",
   "metadata": {},
   "source": [
    "# CIFAR10 + ResNetBN + Dirichlet-0.1 baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5d263d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 val loss 2.313 val acc 0.100 best val_acc 0.100\n",
      "e 10 val loss 1.978 val acc 0.292 best val_acc 0.292\n",
      "e 20 val loss 1.725 val acc 0.397 best val_acc 0.397\n",
      "e 30 val loss 1.512 val acc 0.497 best val_acc 0.497\n",
      "e 40 val loss 1.354 val acc 0.553 best val_acc 0.553\n",
      "e 50 val loss 1.231 val acc 0.593 best val_acc 0.593\n",
      "e 60 val loss 1.141 val acc 0.625 best val_acc 0.631\n",
      "e 70 val loss 1.090 val acc 0.640 best val_acc 0.653\n",
      "e 80 val loss 1.008 val acc 0.666 best val_acc 0.679\n",
      "e 90 val loss 0.948 val acc 0.693 best val_acc 0.695\n",
      "e 100 val loss 0.926 val acc 0.696 best val_acc 0.704\n",
      "e 110 val loss 0.869 val acc 0.720 best val_acc 0.722\n",
      "e 120 val loss 0.878 val acc 0.713 best val_acc 0.731\n",
      "e 130 val loss 0.824 val acc 0.737 best val_acc 0.740\n",
      "e 140 val loss 0.803 val acc 0.735 best val_acc 0.747\n",
      "e 150 val loss 0.790 val acc 0.741 best val_acc 0.756\n",
      "e 160 val loss 0.768 val acc 0.749 best val_acc 0.760\n",
      "e 170 val loss 0.746 val acc 0.759 best val_acc 0.764\n",
      "e 180 val loss 0.744 val acc 0.761 best val_acc 0.767\n",
      "e 190 val loss 0.734 val acc 0.759 best val_acc 0.772\n",
      "e 199 val loss 0.691 val acc 0.778 best val_acc 0.778\n",
      "e 200 val loss 0.715 val acc 0.769 best val_acc 0.778\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.1\n",
    "global_lr = 1\n",
    "agr = 'average'\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "nbyz = 0\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(20-nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_grads=[]\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "    agg_grads = torch.mean(user_grads, 0)\n",
    "    del user_grads\n",
    "    model_received = model_received + global_lr * agg_grads\n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    if is_best:\n",
    "        best_model = copy.deepcopy(fed_model)\n",
    "\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, val_loss, val_acc, best_global_acc))\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb760c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 val loss 2.306 val acc 0.099 best val_acc 0.099\n",
      "e 10 val loss 1.989 val acc 0.246 best val_acc 0.246\n",
      "e 20 val loss 1.635 val acc 0.415 best val_acc 0.415\n",
      "e 30 val loss 1.424 val acc 0.504 best val_acc 0.504\n",
      "e 40 val loss 1.290 val acc 0.553 best val_acc 0.553\n",
      "e 50 val loss 1.188 val acc 0.595 best val_acc 0.595\n",
      "e 60 val loss 1.094 val acc 0.632 best val_acc 0.632\n",
      "e 70 val loss 1.025 val acc 0.656 best val_acc 0.656\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 8\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    train_loaders.append((pos, get_train(all_data, indices, batch_size)))\n",
    "num_workers = 100\n",
    "local_lr = 0.1\n",
    "global_lr = 1\n",
    "agr = 'average'\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "nbyz = 0\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(20-nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_grads=[]\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "    agg_grads = torch.mean(user_grads, 0)\n",
    "    del user_grads\n",
    "    model_received = model_received + global_lr * agg_grads\n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    if is_best:\n",
    "        best_model = copy.deepcopy(fed_model)\n",
    "\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, val_loss, val_acc, best_global_acc))\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc66b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r 0 nmal 10 | e 0 benign_norm 2390.534 val loss 2.315 val acc 0.099 best val_acc 0.099\n",
      "r 0 nmal 10 | e 10 benign_norm 282.932 val loss 2.265 val acc 0.132 best val_acc 0.132\n",
      "r 0 nmal 10 | e 20 benign_norm 280.659 val loss 1.850 val acc 0.330 best val_acc 0.330\n",
      "r 0 nmal 10 | e 30 benign_norm 283.818 val loss 1.682 val acc 0.368 best val_acc 0.370\n",
      "r 0 nmal 10 | e 40 benign_norm 288.321 val loss 1.553 val acc 0.419 best val_acc 0.420\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_malicious = [10]\n",
    "n_runs = 3\n",
    "results={}\n",
    "for n_mal in n_malicious:\n",
    "    nbyz = n_mal\n",
    "    results[n_mal] = {'acc': [], 'fpr': [], 'fnr': [], 'auc': [], 'stop_e': []}\n",
    "    for run in range(n_runs):\n",
    "        torch.cuda.empty_cache()\n",
    "        nepochs=200\n",
    "        local_epochs = 2\n",
    "        batch_size = 16\n",
    "        num_workers = 100\n",
    "        local_lr = 0.1\n",
    "        global_lr = 1\n",
    "        agr = 'tr_mean'\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = resnet20().cuda()\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        best_accs_per_round = []\n",
    "        accs_per_round = []\n",
    "        loss_per_round = []\n",
    "        home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "        # FLDetector initializations\n",
    "        weight_record = []\n",
    "        grad_record = []\n",
    "        test_grads = []\n",
    "        old_grad_list = []\n",
    "        malicious_scores = np.zeros((1, num_workers-(20-nbyz)))\n",
    "        start_detection_epoch = 10\n",
    "        window_size = 10\n",
    "        assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "        # Adaptive attack initializers\n",
    "        good_distance_range = np.zeros((1, nbyz))\n",
    "        attack_type = 'NDSS21'\n",
    "        dev_type = 'std'\n",
    "        \n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(20-nbyz, num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_grads=[]\n",
    "            benign_norm = 0\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "            weight = model_received\n",
    "\n",
    "            if (epoch_num > start_detection_epoch):\n",
    "                hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "                hvp = np.squeeze(hvp)\n",
    "            else:\n",
    "                hvp = None\n",
    "\n",
    "            good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "            if hvp is not None:\n",
    "                pred_grad = copy.deepcopy(good_old_grads)\n",
    "                distance = []\n",
    "                for i in range(len(good_old_grads)):\n",
    "                    pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "                good_distance_range = np.concatenate(\n",
    "                    (good_distance_range, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "            if attack_type != 'none' and (epoch_num < start_detection_epoch):\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "                # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "            elif epoch_num > start_detection_epoch:\n",
    "                if attack_type == 'full_trim':\n",
    "                    user_grads = full_trim(user_grads, nbyz)\n",
    "                elif attack_type == 'none':\n",
    "                    pass\n",
    "                else:\n",
    "                    if attack_type == 'NDSS21':\n",
    "                        distance_bound = np.mean(np.mean(good_distance_range[-1:], 0))\n",
    "                        model_re = torch.mean(good_current_grads, dim=0)\n",
    "                        if dev_type == 'unit_vec':\n",
    "                            deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                        elif dev_type == 'sign':\n",
    "                            deviation = torch.sign(model_re)\n",
    "                        elif dev_type == 'std':\n",
    "                            deviation = torch.std(good_current_grads, 0)\n",
    "                        # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_range[-1]))) / torch.norm(deviation))\n",
    "                        noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                    elif attack_type == 'mod_trim':\n",
    "                        mal_grads= full_trim(user_grads[:nbyz], nbyz)\n",
    "                        pass\n",
    "                    else:\n",
    "                        noise = torch.zeros(hvp.shape).to(device)\n",
    "                    for m in range(nbyz):\n",
    "                        user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "            agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "            if distance is not None and epoch_num > (start_detection_epoch - window_size):\n",
    "                malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "            if malicious_scores.shape[0] >= window_size+1:\n",
    "                if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                    print('!!!! Stop at iteration:', epoch_num)\n",
    "                    print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "                    acc, fpr, fnr, auc = detection(\n",
    "                        np.sum(malicious_scores[-window_size:], axis=0), nbyz, len(user_grads))\n",
    "                    results[n_mal]['acc'].append(acc)\n",
    "                    results[n_mal]['fpr'].append(fpr)\n",
    "                    results[n_mal]['fnr'].append(fnr)\n",
    "                    results[n_mal]['auc'].append(auc)\n",
    "                    results[n_mal]['stop_e'].append(epoch_num)\n",
    "                    break\n",
    "                    break\n",
    "\n",
    "            if epoch_num > (start_detection_epoch - window_size):\n",
    "                weight_record.append(weight - last_weight)\n",
    "                grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "            if (len(weight_record) > window_size):\n",
    "                del weight_record[0]\n",
    "                del grad_record[0]\n",
    "\n",
    "            last_weight = weight\n",
    "            last_grad = agg_grads\n",
    "            old_grad_list = user_grads\n",
    "            good_old_grads = good_current_grads\n",
    "            del user_grads\n",
    "\n",
    "            model_received = model_received + global_lr * agg_grads\n",
    "            fed_model = resnet20().cuda()\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "            if is_best:\n",
    "                best_model = copy.deepcopy(fed_model)\n",
    "\n",
    "            best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "            accs_per_round.append(val_acc.cpu().item())\n",
    "            loss_per_round.append(val_loss.cpu().item())\n",
    "\n",
    "            if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "                print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "            if math.isnan(val_loss) or val_loss > 100000:\n",
    "                print('val loss %f... exit'%val_loss)\n",
    "                break\n",
    "            epoch_num+=1\n",
    "\n",
    "        final_accs_per_client=[]\n",
    "        for i in range(num_workers):\n",
    "            client_loss, client_acc = test(test_loaders[i][1], best_model, criterion)\n",
    "            final_accs_per_client.append(client_acc.cpu().item())\n",
    "        all_results = collections.OrderedDict(\n",
    "            final_accs_per_client=np.array(final_accs_per_client),\n",
    "            accs_per_round=np.array(accs_per_round),\n",
    "            best_accs_per_round=np.array(best_accs_per_round),\n",
    "            loss_per_round=np.array(loss_per_round),\n",
    "            results=results\n",
    "        )\n",
    "        pickle.dump(all_results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq4_adaptive_cifar10_fast_trmean_NDSS21_std_m%d_r%d_dirichlet0.1.pkl' % (nbyz, run)), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c71a06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
