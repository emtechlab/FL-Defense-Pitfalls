{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for Fashion MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821b1014",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu044/6605966/ipykernel_2703437/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a52c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "            \n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_train_data(trainset, num_workers=100, bias=0.5):\n",
    "    np.random.seed(42)\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        \n",
    "        if not len(each_worker_data[selected_worker]):\n",
    "            each_worker_data[selected_worker] = x[None, :]\n",
    "        else:\n",
    "            each_worker_data[selected_worker]= torch.concat((each_worker_data[selected_worker], x[None, :]))\n",
    "        \n",
    "        each_worker_label[selected_worker].append(y)\n",
    "    \n",
    "    each_worker_tr_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_tr_label = [[] for _ in range(num_workers)]\n",
    "    each_worker_val_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_val_label = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "\n",
    "    for i in range(num_workers):\n",
    "        w_len = len(each_worker_data[i])\n",
    "        len_tr = int(5 * w_len / 7)\n",
    "        len_val = int(1 * w_len / 7)\n",
    "        len_te = w_len - (len_tr + len_val)\n",
    "        # tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        # te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        w = np.arange(w_len)\n",
    "        np.random.shuffle(w)\n",
    "        tr_idx, val_idx, te_idx = w[:len_tr], w[len_tr : (len_tr + len_val)], w[-len_te:]\n",
    "        each_worker_tr_data[i] = each_worker_data[i][tr_idx]\n",
    "        each_worker_tr_label[i] = torch.Tensor(each_worker_label[i])[tr_idx]\n",
    "        each_worker_val_data[i] = each_worker_data[i][val_idx]\n",
    "        each_worker_val_label[i] = torch.Tensor(each_worker_label[i])[val_idx]\n",
    "        each_worker_te_data[i] = each_worker_data[i][te_idx]\n",
    "        each_worker_te_label[i] = torch.Tensor(each_worker_label[i])[te_idx]\n",
    "        \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    del each_worker_data, each_worker_label\n",
    "    return each_worker_tr_data, each_worker_tr_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f36f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_data_dirichlet(trainset, num_workers, alpha=1, force=False):\n",
    "    per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=alpha, force=force)\n",
    "    \n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(6*w_len/7)\n",
    "        len_te = w_len - len_tr\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        \n",
    "        for idx in w_indices[tr_idx]:\n",
    "            each_worker_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_data[worker_idx] = torch.stack(each_worker_data[worker_idx])\n",
    "        each_worker_label[worker_idx] = torch.Tensor(each_worker_label[worker_idx]).long()\n",
    "        \n",
    "        for idx in w_indices[te_idx]:\n",
    "            each_worker_te_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_te_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_te_data[worker_idx] = torch.stack(each_worker_te_data[worker_idx])\n",
    "        each_worker_te_label[worker_idx] = torch.Tensor(each_worker_te_label[worker_idx]).long()\n",
    "    \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    \n",
    "    return each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6b5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 30, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(30, 50, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(1250, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "#         print(x.shape)\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae94ee06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266060"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cnn()\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb5ef3",
   "metadata": {},
   "source": [
    "# Good FedAvg baseline Fashion MNIST + Fang distribution + 80 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24164616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(train_data, labels, model, optimizer, batch_size=20):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    len_t = (len(train_data) // batch_size)\n",
    "    if len(train_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    r=np.arange(len(train_data))\n",
    "    np.random.shuffle(r)\n",
    "    \n",
    "    train_data = train_data[r]\n",
    "    labels = labels[r]\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "\n",
    "        inputs = train_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "        targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "\n",
    "def test(test_data, labels, model, criterion, use_cuda, debug_='MEDIUM', batch_size=64):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    len_t = (len(test_data) // batch_size)\n",
    "    if len(test_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ind in range(len_t):\n",
    "            # measure data loading time\n",
    "            inputs = test_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "            targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092fa012",
   "metadata": {},
   "source": [
    "# Mean without any attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58533a3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Processing FCJ 0.1 run 0\n",
      "FCJ 0.1 r 0 e 0 val loss 0.953 val acc 68.258 best val_acc 68.258\n",
      "FCJ 0.1 r 0 e 10 val loss 0.450 val acc 83.181 best val_acc 83.181\n",
      "FCJ 0.1 r 0 e 20 val loss 0.379 val acc 86.386 best val_acc 86.386\n",
      "FCJ 0.1 r 0 e 30 val loss 0.342 val acc 87.458 best val_acc 87.507\n",
      "FCJ 0.1 r 0 e 40 val loss 0.315 val acc 88.172 best val_acc 88.172\n",
      "FCJ 0.1 r 0 e 49 val loss 0.302 val acc 88.797 best val_acc 88.797\n",
      "FCJ 0.1 r 0 e 50 val loss 0.301 val acc 88.817 best val_acc 88.817\n",
      "===> Processing FCJ 0.1 run 1\n",
      "FCJ 0.1 r 1 e 0 val loss 0.922 val acc 68.853 best val_acc 68.853\n",
      "FCJ 0.1 r 1 e 10 val loss 0.451 val acc 82.864 best val_acc 82.864\n",
      "FCJ 0.1 r 1 e 20 val loss 0.379 val acc 86.148 best val_acc 86.148\n",
      "FCJ 0.1 r 1 e 30 val loss 0.343 val acc 87.200 best val_acc 87.249\n",
      "FCJ 0.1 r 1 e 49 val loss 0.300 val acc 88.946 best val_acc 88.946\n",
      "FCJ 0.1 r 1 e 50 val loss 0.300 val acc 88.897 best val_acc 88.946\n",
      "===> Processing FCJ 0.1 run 2\n",
      "FCJ 0.1 r 2 e 0 val loss 1.107 val acc 64.785 best val_acc 64.785\n",
      "FCJ 0.1 r 2 e 10 val loss 0.452 val acc 83.052 best val_acc 83.052\n",
      "FCJ 0.1 r 2 e 20 val loss 0.384 val acc 85.682 best val_acc 85.741\n",
      "FCJ 0.1 r 2 e 30 val loss 0.350 val acc 87.021 best val_acc 87.021\n",
      "FCJ 0.1 r 2 e 40 val loss 0.327 val acc 87.636 best val_acc 87.636\n",
      "FCJ 0.1 r 2 e 49 val loss 0.315 val acc 88.113 best val_acc 88.113\n",
      "FCJ 0.1 r 2 e 50 val loss 0.314 val acc 88.202 best val_acc 88.202\n",
      "===> Processing FCJ 0.3 run 0\n",
      "FCJ 0.3 r 0 e 0 val loss 1.076 val acc 64.880 best val_acc 64.880\n",
      "FCJ 0.3 r 0 e 10 val loss 0.469 val acc 82.644 best val_acc 82.644\n",
      "FCJ 0.3 r 0 e 20 val loss 0.397 val acc 85.432 best val_acc 85.432\n",
      "FCJ 0.3 r 0 e 30 val loss 0.362 val acc 86.603 best val_acc 86.603\n",
      "FCJ 0.3 r 0 e 40 val loss 0.340 val acc 87.546 best val_acc 87.566\n",
      "FCJ 0.3 r 0 e 49 val loss 0.325 val acc 88.062 best val_acc 88.062\n",
      "FCJ 0.3 r 0 e 50 val loss 0.324 val acc 88.300 best val_acc 88.300\n",
      "===> Processing FCJ 0.3 run 1\n",
      "FCJ 0.3 r 1 e 0 val loss 1.085 val acc 66.220 best val_acc 66.220\n",
      "FCJ 0.3 r 1 e 10 val loss 0.468 val acc 82.544 best val_acc 82.544\n",
      "FCJ 0.3 r 1 e 30 val loss 0.374 val acc 86.226 best val_acc 86.276\n",
      "FCJ 0.3 r 1 e 40 val loss 0.352 val acc 87.218 best val_acc 87.218\n",
      "FCJ 0.3 r 1 e 49 val loss 0.339 val acc 87.715 best val_acc 87.764\n",
      "FCJ 0.3 r 1 e 50 val loss 0.337 val acc 87.863 best val_acc 87.863\n",
      "===> Processing FCJ 0.3 run 2\n",
      "FCJ 0.3 r 2 e 0 val loss 1.039 val acc 69.376 best val_acc 69.376\n",
      "FCJ 0.3 r 2 e 10 val loss 0.479 val acc 81.919 best val_acc 81.919\n",
      "FCJ 0.3 r 2 e 20 val loss 0.410 val acc 84.896 best val_acc 84.896\n",
      "FCJ 0.3 r 2 e 30 val loss 0.377 val acc 85.909 best val_acc 85.968\n",
      "FCJ 0.3 r 2 e 40 val loss 0.356 val acc 87.060 best val_acc 87.060\n",
      "FCJ 0.3 r 2 e 49 val loss 0.340 val acc 87.556 best val_acc 87.556\n",
      "FCJ 0.3 r 2 e 50 val loss 0.340 val acc 87.576 best val_acc 87.576\n",
      "===> Processing FCJ 0.5 run 0\n",
      "FCJ 0.5 r 0 e 0 val loss 1.507 val acc 57.504 best val_acc 57.504\n",
      "FCJ 0.5 r 0 e 10 val loss 0.479 val acc 81.510 best val_acc 81.510\n",
      "FCJ 0.5 r 0 e 20 val loss 0.406 val acc 85.289 best val_acc 85.289\n",
      "FCJ 0.5 r 0 e 30 val loss 0.365 val acc 86.618 best val_acc 86.668\n",
      "FCJ 0.5 r 0 e 40 val loss 0.342 val acc 87.551 best val_acc 87.551\n",
      "FCJ 0.5 r 0 e 49 val loss 0.324 val acc 87.938 best val_acc 87.977\n",
      "FCJ 0.5 r 0 e 50 val loss 0.323 val acc 88.007 best val_acc 88.007\n",
      "===> Processing FCJ 0.5 run 1\n",
      "FCJ 0.5 r 1 e 0 val loss 1.468 val acc 62.077 best val_acc 62.077\n",
      "FCJ 0.5 r 1 e 10 val loss 0.478 val acc 81.579 best val_acc 81.579\n",
      "FCJ 0.5 r 1 e 20 val loss 0.409 val acc 85.001 best val_acc 85.001\n",
      "FCJ 0.5 r 1 e 30 val loss 0.370 val acc 86.480 best val_acc 86.480\n",
      "FCJ 0.5 r 1 e 40 val loss 0.348 val acc 87.422 best val_acc 87.422\n",
      "FCJ 0.5 r 1 e 49 val loss 0.329 val acc 88.146 best val_acc 88.146\n",
      "FCJ 0.5 r 1 e 50 val loss 0.328 val acc 88.077 best val_acc 88.146\n",
      "===> Processing FCJ 0.5 run 2\n",
      "FCJ 0.5 r 2 e 0 val loss 1.368 val acc 59.677 best val_acc 59.677\n",
      "FCJ 0.5 r 2 e 10 val loss 0.505 val acc 80.151 best val_acc 80.151\n",
      "FCJ 0.5 r 2 e 20 val loss 0.429 val acc 83.762 best val_acc 83.762\n",
      "FCJ 0.5 r 2 e 30 val loss 0.383 val acc 85.894 best val_acc 85.894\n",
      "FCJ 0.5 r 2 e 40 val loss 0.356 val acc 86.985 best val_acc 86.985\n",
      "FCJ 0.7 r 0 e 10 val loss 0.552 val acc 79.189 best val_acc 79.189\n",
      "FCJ 0.7 r 0 e 20 val loss 0.477 val acc 82.431 best val_acc 82.431\n",
      "FCJ 0.7 r 0 e 30 val loss 0.435 val acc 84.255 best val_acc 84.255\n",
      "FCJ 0.7 r 0 e 40 val loss 0.405 val acc 85.118 best val_acc 85.138\n",
      "FCJ 0.7 r 0 e 49 val loss 0.385 val acc 86.189 best val_acc 86.189\n",
      "FCJ 0.7 r 0 e 50 val loss 0.382 val acc 86.278 best val_acc 86.278\n",
      "===> Processing FCJ 0.7 run 1\n",
      "FCJ 0.7 r 1 e 0 val loss 1.767 val acc 50.248 best val_acc 50.248\n",
      "FCJ 0.7 r 1 e 10 val loss 0.537 val acc 79.417 best val_acc 79.417\n",
      "FCJ 0.7 r 1 e 20 val loss 0.468 val acc 82.857 best val_acc 82.857\n",
      "FCJ 0.7 r 1 e 30 val loss 0.423 val acc 84.543 best val_acc 84.543\n",
      "FCJ 0.7 r 1 e 40 val loss 0.394 val acc 85.594 best val_acc 85.594\n",
      "FCJ 0.7 r 1 e 49 val loss 0.375 val acc 86.268 best val_acc 86.268\n",
      "FCJ 0.7 r 1 e 50 val loss 0.374 val acc 86.337 best val_acc 86.337\n",
      "===> Processing FCJ 0.7 run 2\n",
      "FCJ 0.7 r 2 e 0 val loss 1.761 val acc 53.688 best val_acc 53.688\n",
      "FCJ 0.7 r 2 e 10 val loss 0.541 val acc 79.348 best val_acc 79.348\n",
      "FCJ 0.7 r 2 e 20 val loss 0.467 val acc 82.689 best val_acc 82.689\n",
      "FCJ 0.7 r 2 e 30 val loss 0.421 val acc 84.830 best val_acc 84.830\n",
      "FCJ 0.7 r 2 e 40 val loss 0.393 val acc 85.723 best val_acc 85.723\n",
      "FCJ 0.7 r 2 e 49 val loss 0.375 val acc 86.347 best val_acc 86.347\n",
      "FCJ 0.7 r 2 e 50 val loss 0.373 val acc 86.486 best val_acc 86.486\n",
      "===> Processing FCJ 0.9 run 0\n",
      "FCJ 0.9 r 0 e 0 val loss 2.144 val acc 35.723 best val_acc 35.723\n",
      "FCJ 0.9 r 0 e 20 val loss 0.598 val acc 76.555 best val_acc 76.555\n",
      "FCJ 0.9 r 0 e 30 val loss 0.545 val acc 78.965 best val_acc 78.965\n",
      "FCJ 0.9 r 0 e 40 val loss 0.509 val acc 80.651 best val_acc 80.651\n",
      "FCJ 0.9 r 0 e 49 val loss 0.484 val acc 82.148 best val_acc 82.148\n",
      "FCJ 0.9 r 0 e 50 val loss 0.482 val acc 82.406 best val_acc 82.406\n",
      "===> Processing FCJ 0.9 run 1\n",
      "FCJ 0.9 r 1 e 0 val loss 2.180 val acc 35.952 best val_acc 35.952\n",
      "FCJ 0.9 r 1 e 10 val loss 0.730 val acc 73.163 best val_acc 73.163\n",
      "FCJ 0.9 r 1 e 20 val loss 0.600 val acc 76.763 best val_acc 76.763\n",
      "FCJ 0.9 r 1 e 30 val loss 0.543 val acc 79.252 best val_acc 79.252\n",
      "FCJ 0.9 r 1 e 40 val loss 0.507 val acc 80.988 best val_acc 80.988\n",
      "FCJ 0.9 r 1 e 49 val loss 0.481 val acc 82.138 best val_acc 82.138\n",
      "FCJ 0.9 r 1 e 50 val loss 0.479 val acc 82.228 best val_acc 82.228\n",
      "===> Processing FCJ 0.9 run 2\n",
      "FCJ 0.9 r 2 e 0 val loss 2.174 val acc 33.472 best val_acc 33.472\n",
      "FCJ 0.9 r 2 e 10 val loss 0.737 val acc 72.488 best val_acc 72.488\n",
      "FCJ 0.9 r 2 e 20 val loss 0.610 val acc 76.178 best val_acc 76.227\n",
      "FCJ 0.9 r 2 e 30 val loss 0.550 val acc 78.508 best val_acc 78.508\n",
      "FCJ 0.9 r 2 e 40 val loss 0.512 val acc 80.611 best val_acc 80.611\n",
      "FCJ 0.9 r 2 e 49 val loss 0.486 val acc 81.960 best val_acc 81.960\n",
      "FCJ 0.9 r 2 e 50 val loss 0.484 val acc 82.099 best val_acc 82.099\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "distribution='fang'\n",
    "params = [.1, .3, .5, .7, .9]\n",
    "\n",
    "for fcj_p in params:\n",
    "    for run in range(3):\n",
    "        print('===> Processing FCJ %.1f run %d' % (fcj_p, run))\n",
    "        each_worker_data, each_worker_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_train_data(all_data, num_workers=num_workers, bias=fcj_p)\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = cnn().to(device)\n",
    "        fed_model.apply(init_weights)\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(nbyz, num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_updates=[]\n",
    "            benign_norm = 0\n",
    "\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(\n",
    "                        each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                        (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "            agg_update = torch.mean(user_updates, 0)\n",
    "            del user_updates\n",
    "            model_received = model_received + global_lr * agg_update\n",
    "            fed_model = cnn().to(device)\n",
    "            fed_model.apply(init_weights)\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "            if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "                print('FCJ %.1f r %d e %d val loss %.3f val acc %.3f best val_acc %.3f'% (fcj_p, run, epoch_num, val_loss, val_acc, best_global_acc))\n",
    "            epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c2130",
   "metadata": {},
   "source": [
    "# NDSS attack on FCJ + TrMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f118ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndss21_attack_trmean(all_updates, n_attackers, dev_type='sign', threshold=5.0, threshold_diff=1e-5, fast_ndss=False):\n",
    "    \n",
    "    model_re = torch.mean(all_updates, 0)\n",
    "    \n",
    "    if dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    if fast_ndss:\n",
    "        return (model_re - threshold * deviation)\n",
    "\n",
    "    lamda = torch.Tensor([threshold]).to(device)  # compute_lambda_our(all_updates, model_re, n_attackers)\n",
    "\n",
    "    threshold_diff = threshold_diff\n",
    "    prev_loss = -1\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        agg_grads = tr_mean(mal_updates, n_attackers)\n",
    "\n",
    "        loss = torch.norm(agg_grads - model_re)\n",
    "\n",
    "        if prev_loss < loss:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "        prev_loss = loss\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "352bf63c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Processing NDSS attack on FCJ 0.1 run 0\n",
      "NDSS on FCJ 0.1 + TrMean | r 0 e 0 val loss 0.997 val acc 69.041 best val_acc 69.041\n",
      "NDSS on FCJ 0.1 + TrMean | r 0 e 10 val loss 0.550 val acc 78.776 best val_acc 78.776\n",
      "NDSS on FCJ 0.1 + TrMean | r 0 e 20 val loss 0.503 val acc 81.008 best val_acc 81.008\n",
      "NDSS on FCJ 0.1 + TrMean | r 0 e 30 val loss 0.484 val acc 82.318 best val_acc 82.447\n",
      "NDSS on FCJ 0.1 + TrMean | r 0 e 40 val loss 0.462 val acc 83.737 best val_acc 83.737\n",
      "NDSS on FCJ 0.1 + TrMean | r 0 e 49 val loss 0.446 val acc 84.580 best val_acc 84.580\n",
      "NDSS on FCJ 0.1 + TrMean | r 0 e 50 val loss 0.449 val acc 84.570 best val_acc 84.580\n",
      "===> Processing NDSS attack on FCJ 0.1 run 1\n",
      "NDSS on FCJ 0.1 + TrMean | r 1 e 0 val loss 1.169 val acc 69.409 best val_acc 69.409\n",
      "NDSS on FCJ 0.1 + TrMean | r 1 e 10 val loss 0.549 val acc 79.292 best val_acc 79.292\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m local_lr, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(local_epochs):\n\u001b[0;32m---> 48\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43meach_worker_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43meach_worker_label\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m params \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (name, param) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mitems()):\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_data, labels, model, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     38\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     39\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 40\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (losses\u001b[38;5;241m.\u001b[39mavg, top1\u001b[38;5;241m.\u001b[39mavg)\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torch/optim/sgd.py:151\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m             momentum_buffer_list\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum_buffer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 151\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torch/optim/sgd.py:202\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 202\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m     \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torch/optim/sgd.py:245\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m         d_p \u001b[38;5;241m=\u001b[39m buf\n\u001b[0;32m--> 245\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "fast_ndss = True\n",
    "params = [.1, .3, .5, .7, .9]\n",
    "\n",
    "results = {}\n",
    "for fcj_p in params:\n",
    "    results[fcj_p] = []\n",
    "    for run in range(3):\n",
    "        print('===> Processing NDSS attack on FCJ %.1f run %d' % (fcj_p, run))\n",
    "        \n",
    "        each_worker_data, each_worker_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_train_data(all_data, num_workers=num_workers, bias=fcj_p)\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = cnn().to(device)\n",
    "        fed_model.apply(init_weights)\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_updates=[]\n",
    "            benign_norm = 0\n",
    "\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(\n",
    "                        each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                        (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "            mal_update = ndss21_attack_trmean(user_updates[:nbyz], 5, dev_type='sign', threshold=20.0, threshold_diff=1e-5, fast_ndss=fast_ndss)\n",
    "            user_updates[:nbyz] = torch.stack([mal_update]*nbyz)\n",
    "            agg_update = tr_mean(user_updates, nbyz)\n",
    "\n",
    "            del user_updates\n",
    "            model_received = model_received + global_lr * agg_update\n",
    "            fed_model = cnn().to(device)\n",
    "            fed_model.apply(init_weights)\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "            if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "                print('NDSS on FCJ %.1f + TrMean | r %d e %d val loss %.3f val acc %.3f best val_acc %.3f'% (fcj_p, run, epoch_num, val_loss, val_acc, best_global_acc))\n",
    "            epoch_num+=1\n",
    "        results[fcj_p].append(best_global_acc)\n",
    "\n",
    "for fcj_p in params:\n",
    "    print('======= Results for NDSS attack on FCJ %.1f  =====' % fcj_p)\n",
    "    print(results[fcj_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2051d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Processing NDSS attack on FCJ 0.1 run 0\n",
      "NDSS on FCJ 0.1 + TrMean | r 0 e 0 val loss 1.088 val acc 66.829 best val_acc 66.829\n",
      "NDSS on FCJ 0.1 + TrMean | r 0 e 10 val loss 0.566 val acc 78.795 best val_acc 78.795\n",
      "NDSS on FCJ 0.1 + TrMean | r 0 e 20 val loss 0.503 val acc 81.117 best val_acc 81.177\n",
      "NDSS on FCJ 0.1 + TrMean | r 0 e 30 val loss 0.475 val acc 82.923 best val_acc 82.923\n",
      "NDSS on FCJ 0.1 + TrMean | r 0 e 40 val loss 0.461 val acc 83.767 best val_acc 83.786\n",
      "NDSS on FCJ 0.1 + TrMean | r 0 e 49 val loss 0.454 val acc 84.352 best val_acc 84.392\n",
      "NDSS on FCJ 0.1 + TrMean | r 0 e 50 val loss 0.455 val acc 84.451 best val_acc 84.451\n",
      "===> Processing NDSS attack on FCJ 0.1 run 1\n",
      "NDSS on FCJ 0.1 + TrMean | r 1 e 0 val loss 1.121 val acc 66.551 best val_acc 66.551\n",
      "NDSS on FCJ 0.1 + TrMean | r 1 e 10 val loss 0.541 val acc 79.440 best val_acc 79.440\n",
      "NDSS on FCJ 0.1 + TrMean | r 1 e 20 val loss 0.498 val acc 80.998 best val_acc 80.998\n",
      "NDSS on FCJ 0.1 + TrMean | r 1 e 30 val loss 0.474 val acc 82.219 best val_acc 82.328\n",
      "NDSS on FCJ 0.1 + TrMean | r 1 e 40 val loss 0.454 val acc 83.896 best val_acc 83.896\n",
      "NDSS on FCJ 0.1 + TrMean | r 1 e 49 val loss 0.448 val acc 84.392 best val_acc 84.461\n",
      "NDSS on FCJ 0.1 + TrMean | r 1 e 50 val loss 0.453 val acc 84.461 best val_acc 84.461\n",
      "===> Processing NDSS attack on FCJ 0.1 run 2\n",
      "NDSS on FCJ 0.1 + TrMean | r 2 e 0 val loss 1.115 val acc 67.206 best val_acc 67.206\n",
      "NDSS on FCJ 0.1 + TrMean | r 2 e 10 val loss 0.544 val acc 79.569 best val_acc 79.569\n",
      "NDSS on FCJ 0.1 + TrMean | r 2 e 20 val loss 0.501 val acc 81.306 best val_acc 81.306\n",
      "NDSS on FCJ 0.1 + TrMean | r 2 e 30 val loss 0.484 val acc 82.844 best val_acc 82.864\n",
      "NDSS on FCJ 0.1 + TrMean | r 2 e 40 val loss 0.464 val acc 83.786 best val_acc 83.826\n",
      "NDSS on FCJ 0.1 + TrMean | r 2 e 49 val loss 0.458 val acc 84.521 best val_acc 84.521\n",
      "NDSS on FCJ 0.1 + TrMean | r 2 e 50 val loss 0.461 val acc 84.679 best val_acc 84.679\n",
      "===> Processing NDSS attack on FCJ 0.3 run 0\n",
      "NDSS on FCJ 0.3 + TrMean | r 0 e 0 val loss 1.243 val acc 65.367 best val_acc 65.367\n",
      "NDSS on FCJ 0.3 + TrMean | r 0 e 10 val loss 0.579 val acc 78.496 best val_acc 78.496\n",
      "NDSS on FCJ 0.3 + TrMean | r 0 e 20 val loss 0.523 val acc 81.274 best val_acc 81.274\n",
      "NDSS on FCJ 0.3 + TrMean | r 0 e 30 val loss 0.514 val acc 81.929 best val_acc 81.929\n",
      "NDSS on FCJ 0.3 + TrMean | r 0 e 40 val loss 0.500 val acc 82.882 best val_acc 82.882\n",
      "NDSS on FCJ 0.3 + TrMean | r 0 e 49 val loss 0.489 val acc 83.666 best val_acc 83.666\n",
      "NDSS on FCJ 0.3 + TrMean | r 0 e 50 val loss 0.491 val acc 83.438 best val_acc 83.666\n",
      "===> Processing NDSS attack on FCJ 0.3 run 1\n",
      "NDSS on FCJ 0.3 + TrMean | r 1 e 0 val loss 1.298 val acc 65.813 best val_acc 65.813\n",
      "NDSS on FCJ 0.3 + TrMean | r 1 e 10 val loss 0.586 val acc 77.831 best val_acc 77.900\n",
      "NDSS on FCJ 0.3 + TrMean | r 1 e 20 val loss 0.534 val acc 80.183 best val_acc 80.183\n",
      "NDSS on FCJ 0.3 + TrMean | r 1 e 30 val loss 0.525 val acc 81.185 best val_acc 81.373\n",
      "NDSS on FCJ 0.3 + TrMean | r 1 e 40 val loss 0.508 val acc 82.108 best val_acc 82.118\n",
      "NDSS on FCJ 0.3 + TrMean | r 1 e 49 val loss 0.498 val acc 82.912 best val_acc 82.912\n",
      "NDSS on FCJ 0.3 + TrMean | r 1 e 50 val loss 0.503 val acc 82.931 best val_acc 82.931\n",
      "===> Processing NDSS attack on FCJ 0.3 run 2\n",
      "NDSS on FCJ 0.3 + TrMean | r 2 e 0 val loss 1.398 val acc 65.585 best val_acc 65.585\n",
      "NDSS on FCJ 0.3 + TrMean | r 2 e 10 val loss 0.579 val acc 78.287 best val_acc 78.386\n",
      "NDSS on FCJ 0.3 + TrMean | r 2 e 20 val loss 0.543 val acc 80.441 best val_acc 80.441\n",
      "NDSS on FCJ 0.3 + TrMean | r 2 e 30 val loss 0.538 val acc 81.284 best val_acc 81.364\n",
      "NDSS on FCJ 0.3 + TrMean | r 2 e 40 val loss 0.515 val acc 82.485 best val_acc 82.485\n",
      "NDSS on FCJ 0.3 + TrMean | r 2 e 49 val loss 0.500 val acc 83.090 best val_acc 83.130\n",
      "NDSS on FCJ 0.3 + TrMean | r 2 e 50 val loss 0.502 val acc 83.130 best val_acc 83.130\n",
      "===> Processing NDSS attack on FCJ 0.5 run 0\n",
      "NDSS on FCJ 0.5 + TrMean | r 0 e 0 val loss 1.788 val acc 52.058 best val_acc 52.058\n",
      "NDSS on FCJ 0.5 + TrMean | r 0 e 10 val loss 0.594 val acc 77.135 best val_acc 77.135\n",
      "NDSS on FCJ 0.5 + TrMean | r 0 e 20 val loss 0.547 val acc 80.339 best val_acc 80.339\n",
      "NDSS on FCJ 0.5 + TrMean | r 0 e 30 val loss 0.522 val acc 81.450 best val_acc 81.609\n",
      "NDSS on FCJ 0.5 + TrMean | r 0 e 40 val loss 0.496 val acc 82.849 best val_acc 83.107\n",
      "NDSS on FCJ 0.5 + TrMean | r 0 e 49 val loss 0.482 val acc 83.722 best val_acc 83.771\n",
      "NDSS on FCJ 0.5 + TrMean | r 0 e 50 val loss 0.483 val acc 83.652 best val_acc 83.771\n",
      "===> Processing NDSS attack on FCJ 0.5 run 1\n",
      "NDSS on FCJ 0.5 + TrMean | r 1 e 0 val loss 1.829 val acc 45.720 best val_acc 45.720\n",
      "NDSS on FCJ 0.5 + TrMean | r 1 e 10 val loss 0.597 val acc 77.433 best val_acc 77.433\n",
      "NDSS on FCJ 0.5 + TrMean | r 1 e 20 val loss 0.558 val acc 80.081 best val_acc 80.081\n",
      "NDSS on FCJ 0.5 + TrMean | r 1 e 30 val loss 0.527 val acc 81.678 best val_acc 81.817\n",
      "NDSS on FCJ 0.5 + TrMean | r 1 e 40 val loss 0.502 val acc 82.829 best val_acc 82.829\n",
      "NDSS on FCJ 0.5 + TrMean | r 1 e 49 val loss 0.487 val acc 83.633 best val_acc 83.633\n",
      "NDSS on FCJ 0.5 + TrMean | r 1 e 50 val loss 0.483 val acc 83.633 best val_acc 83.633\n",
      "===> Processing NDSS attack on FCJ 0.5 run 2\n",
      "NDSS on FCJ 0.5 + TrMean | r 2 e 0 val loss 1.602 val acc 58.337 best val_acc 58.337\n",
      "NDSS on FCJ 0.5 + TrMean | r 2 e 10 val loss 0.599 val acc 77.522 best val_acc 77.522\n",
      "NDSS on FCJ 0.5 + TrMean | r 2 e 20 val loss 0.550 val acc 79.972 best val_acc 79.972\n",
      "NDSS on FCJ 0.5 + TrMean | r 2 e 30 val loss 0.527 val acc 81.401 best val_acc 81.649\n",
      "NDSS on FCJ 0.5 + TrMean | r 2 e 40 val loss 0.508 val acc 82.561 best val_acc 82.611\n",
      "NDSS on FCJ 0.5 + TrMean | r 2 e 49 val loss 0.491 val acc 83.305 best val_acc 83.345\n",
      "NDSS on FCJ 0.5 + TrMean | r 2 e 50 val loss 0.494 val acc 83.325 best val_acc 83.345\n",
      "===> Processing NDSS attack on FCJ 0.7 run 0\n",
      "NDSS on FCJ 0.7 + TrMean | r 0 e 0 val loss 2.165 val acc 27.890 best val_acc 27.890\n",
      "NDSS on FCJ 0.7 + TrMean | r 0 e 10 val loss 0.635 val acc 76.968 best val_acc 76.968\n",
      "NDSS on FCJ 0.7 + TrMean | r 0 e 20 val loss 0.586 val acc 79.942 best val_acc 79.942\n",
      "NDSS on FCJ 0.7 + TrMean | r 0 e 30 val loss 0.575 val acc 80.944 best val_acc 80.944\n",
      "NDSS on FCJ 0.7 + TrMean | r 0 e 40 val loss 0.551 val acc 82.035 best val_acc 82.302\n",
      "NDSS on FCJ 0.7 + TrMean | r 0 e 49 val loss 0.558 val acc 82.669 best val_acc 82.669\n",
      "NDSS on FCJ 0.7 + TrMean | r 0 e 50 val loss 0.548 val acc 82.996 best val_acc 82.996\n",
      "===> Processing NDSS attack on FCJ 0.7 run 1\n",
      "NDSS on FCJ 0.7 + TrMean | r 1 e 0 val loss 2.162 val acc 36.635 best val_acc 36.635\n",
      "NDSS on FCJ 0.7 + TrMean | r 1 e 10 val loss 0.683 val acc 75.600 best val_acc 75.600\n",
      "NDSS on FCJ 0.7 + TrMean | r 1 e 20 val loss 0.604 val acc 79.288 best val_acc 79.288\n",
      "NDSS on FCJ 0.7 + TrMean | r 1 e 30 val loss 0.574 val acc 80.904 best val_acc 80.904\n",
      "NDSS on FCJ 0.7 + TrMean | r 1 e 40 val loss 0.546 val acc 82.332 best val_acc 82.332\n",
      "NDSS on FCJ 0.7 + TrMean | r 1 e 49 val loss 0.542 val acc 82.867 best val_acc 83.026\n",
      "NDSS on FCJ 0.7 + TrMean | r 1 e 50 val loss 0.541 val acc 83.224 best val_acc 83.224\n",
      "===> Processing NDSS attack on FCJ 0.7 run 2\n",
      "NDSS on FCJ 0.7 + TrMean | r 2 e 0 val loss 2.106 val acc 38.449 best val_acc 38.449\n",
      "NDSS on FCJ 0.7 + TrMean | r 2 e 10 val loss 0.681 val acc 75.253 best val_acc 75.253\n",
      "NDSS on FCJ 0.7 + TrMean | r 2 e 20 val loss 0.612 val acc 78.792 best val_acc 78.792\n",
      "NDSS on FCJ 0.7 + TrMean | r 2 e 30 val loss 0.589 val acc 80.835 best val_acc 80.835\n",
      "NDSS on FCJ 0.7 + TrMean | r 2 e 40 val loss 0.562 val acc 81.995 best val_acc 81.995\n",
      "NDSS on FCJ 0.7 + TrMean | r 2 e 49 val loss 0.569 val acc 82.322 best val_acc 82.441\n",
      "NDSS on FCJ 0.7 + TrMean | r 2 e 50 val loss 0.558 val acc 82.560 best val_acc 82.560\n",
      "===> Processing NDSS attack on FCJ 0.9 run 0\n",
      "NDSS on FCJ 0.9 + TrMean | r 0 e 0 val loss 2.316 val acc 14.440 best val_acc 14.440\n",
      "NDSS on FCJ 0.9 + TrMean | r 0 e 10 val loss 0.927 val acc 58.951 best val_acc 58.951\n",
      "NDSS on FCJ 0.9 + TrMean | r 0 e 20 val loss 0.773 val acc 70.297 best val_acc 70.297\n",
      "NDSS on FCJ 0.9 + TrMean | r 0 e 30 val loss 0.723 val acc 73.907 best val_acc 73.907\n",
      "NDSS on FCJ 0.9 + TrMean | r 0 e 40 val loss 0.730 val acc 75.067 best val_acc 75.265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDSS on FCJ 0.9 + TrMean | r 0 e 49 val loss 0.734 val acc 76.227 best val_acc 76.237\n",
      "NDSS on FCJ 0.9 + TrMean | r 0 e 50 val loss 0.733 val acc 76.307 best val_acc 76.307\n",
      "===> Processing NDSS attack on FCJ 0.9 run 1\n",
      "NDSS on FCJ 0.9 + TrMean | r 1 e 0 val loss 2.316 val acc 27.403 best val_acc 27.403\n",
      "NDSS on FCJ 0.9 + TrMean | r 1 e 10 val loss 0.923 val acc 63.017 best val_acc 63.017\n",
      "NDSS on FCJ 0.9 + TrMean | r 1 e 20 val loss 0.806 val acc 69.959 best val_acc 69.959\n",
      "NDSS on FCJ 0.9 + TrMean | r 1 e 30 val loss 0.764 val acc 72.419 best val_acc 72.419\n",
      "NDSS on FCJ 0.9 + TrMean | r 1 e 40 val loss 0.785 val acc 73.748 best val_acc 73.966\n",
      "NDSS on FCJ 0.9 + TrMean | r 1 e 49 val loss 0.767 val acc 75.245 best val_acc 75.245\n",
      "NDSS on FCJ 0.9 + TrMean | r 1 e 50 val loss 0.768 val acc 75.364 best val_acc 75.364\n",
      "===> Processing NDSS attack on FCJ 0.9 run 2\n",
      "NDSS on FCJ 0.9 + TrMean | r 2 e 0 val loss 2.344 val acc 27.145 best val_acc 27.145\n",
      "NDSS on FCJ 0.9 + TrMean | r 2 e 10 val loss 0.979 val acc 56.848 best val_acc 56.848\n",
      "NDSS on FCJ 0.9 + TrMean | r 2 e 20 val loss 0.808 val acc 69.166 best val_acc 69.166\n",
      "NDSS on FCJ 0.9 + TrMean | r 2 e 30 val loss 0.754 val acc 72.984 best val_acc 72.984\n",
      "NDSS on FCJ 0.9 + TrMean | r 2 e 40 val loss 0.746 val acc 74.254 best val_acc 75.017\n",
      "NDSS on FCJ 0.9 + TrMean | r 2 e 49 val loss 0.734 val acc 75.870 best val_acc 75.940\n",
      "NDSS on FCJ 0.9 + TrMean | r 2 e 50 val loss 0.711 val acc 76.624 best val_acc 76.624\n",
      "======= Results for NDSS attack on FCJ 0.1  =====\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "tensor(0.1455, device='cuda:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fcj_p \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m======= Results for NDSS attack on FCJ \u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[38;5;124m  =====\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m fcj_p)\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfcj_p\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: tensor(0.1455, device='cuda:0')"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "fast_ndss = True\n",
    "params = [.1, .3, .5, .7, .9]\n",
    "\n",
    "results = {}\n",
    "for fcj_p in params:\n",
    "    results[fcj_p] = []\n",
    "    for run in range(3):\n",
    "        print('===> Processing NDSS attack on FCJ %.1f run %d' % (fcj_p, run))\n",
    "        \n",
    "        each_worker_data, each_worker_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_train_data(all_data, num_workers=num_workers, bias=fcj_p)\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = cnn().to(device)\n",
    "        fed_model.apply(init_weights)\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_updates=[]\n",
    "            benign_norm = 0\n",
    "\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(\n",
    "                        each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                        (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "            mal_update = ndss21_attack_trmean(user_updates[:nbyz], 5, dev_type='sign', threshold=100.0, threshold_diff=1e-5, fast_ndss=fast_ndss)\n",
    "            user_updates[:nbyz] = torch.stack([mal_update]*nbyz)\n",
    "            agg_update = tr_mean(user_updates, nbyz)\n",
    "\n",
    "            del user_updates\n",
    "            model_received = model_received + global_lr * agg_update\n",
    "            fed_model = cnn().to(device)\n",
    "            fed_model.apply(init_weights)\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "            if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "                print('NDSS on FCJ %.1f + TrMean | r %d e %d val loss %.3f val acc %.3f best val_acc %.3f'% (fcj_p, run, epoch_num, val_loss, val_acc, best_global_acc))\n",
    "            epoch_num+=1\n",
    "        results[fcj_p].append(best_global_acc)\n",
    "\n",
    "for fcj_p in params:\n",
    "    print('======= Results for NDSS attack on FCJ %.1f  =====' % fcj_p)\n",
    "    print(results[fcj_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53437176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.1: [84.45128000073548, 84.46120261956737, 84.67949989320361],\n",
       " 0.3: [83.66577353235677, 82.93142799300973, 83.12989977857146],\n",
       " 0.5: [83.77145122978, 83.63257614013139, 83.34490624416348],\n",
       " 0.7: [82.99623239832266, 83.22427125499996, 82.55998412432379],\n",
       " 0.9: [76.3066547616635, 75.36447485488972, 76.62402064013101]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4f485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
