{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for CIFAR10 with Fang distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e300ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu065/7176229/ipykernel_1169661/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "import copy\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77aa30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_models import *\n",
    "from cifar10_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "\n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fang_train_data(trainset, num_workers=100, bias=0.5, force=False):\n",
    "    dist_file = 'fang_nworkers%d_bias%.1f.pkl' % (num_workers, bias)\n",
    "    if not force and os.path.exists(dist_file):\n",
    "        print('Loading fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "        return pickle.load(open(dist_file, 'rb'))\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    per_participant_list = defaultdict(list)\n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        per_participant_list[selected_worker].extend([i])\n",
    "    \n",
    "    print('Saving fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "    pickle.dump(per_participant_list, open(dist_file, 'wb'))\n",
    "    return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799ffb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_federated_data(trainset, num_workers, distribution='fang', param=1, force=False):\n",
    "    if distribution == 'fang':\n",
    "        per_participant_list = get_fang_train_data(trainset, num_workers, bias=param, force=force)\n",
    "    elif distribution == 'dirichlet':\n",
    "        per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=param, force=force)\n",
    "\n",
    "    each_worker_idx = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_te_idx = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(5*w_len/6)\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        \n",
    "        each_worker_idx[worker_idx] = w_indices[tr_idx]\n",
    "        each_worker_te_idx[worker_idx] = w_indices[te_idx]\n",
    "    \n",
    "    global_test_idx = np.concatenate(each_worker_te_idx)\n",
    "    \n",
    "    return each_worker_idx, each_worker_te_idx, global_test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2590db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(dataset, indices, batch_size=32, shuffle=False):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acbb06d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/home/vshejwalkar_umass_edu/data/'\n",
    "# load the train dataset\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_train)\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_train)\n",
    "\n",
    "te_cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_test)\n",
    "te_cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(trainloader, model, model_received, criterion, optimizer, pgd=False, eps=2):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    for batch_ind, (inputs, targets) in enumerate(trainloader):\n",
    "\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size()[0])\n",
    "        top1.update(prec1.item()/100.0, inputs.size()[0])\n",
    "        top5.update(prec5.item()/100.0, inputs.size()[0])\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if pgd:\n",
    "            curr_model = list(model.parameters())\n",
    "            curr_model_vec = parameters_to_vector(curr_model)\n",
    "\n",
    "            if torch.norm(curr_model_vec - model_received) > eps:\n",
    "                curr_model_vec = eps*(curr_model_vec - model_received)/torch.norm(curr_model_vec - model_received) + model_received\n",
    "                vector_to_parameters(curr_model_vec, curr_model)\n",
    "        \n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def test(testloader, model, criterion):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    for batch_ind, (inputs, targets) in enumerate(testloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        top1.update(prec1/100.0, inputs.size()[0])\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62958a8",
   "metadata": {},
   "source": [
    "# Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "382ff809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def alexnet(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a397e863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2472266"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = alexnet()\n",
    "sum(p.numel() for p in m.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ca7c704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fang distribution for num_workers 100 and bias 0.5 from memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 100, 10038)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "batch_size = 10000\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = False\n",
    "\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=num_workers, distribution=distribution, param=param, force=force)\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    train_loaders.append((pos, get_train(all_data, indices, batch_size)))\n",
    "test_loaders = []\n",
    "for pos, indices in enumerate(each_worker_te_idx):\n",
    "    test_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "len(train_loaders), len(test_loaders), len(global_test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9ee41f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vshejwalkar_umass_edu/fedrecover/SGD.py:103: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  d_p.add_(weight_decay, p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.0500 | e 0 val loss 2.303 val acc 0.099 best val_acc 0.099\n",
      "lr 0.0500 | e 50 val loss 2.162 val acc 0.211 best val_acc 0.211\n",
      "lr 0.0500 | e 100 val loss 1.990 val acc 0.253 best val_acc 0.280\n",
      "lr 0.0500 | e 150 val loss 1.692 val acc 0.351 best val_acc 0.351\n",
      "lr 0.0500 | e 200 val loss 1.441 val acc 0.461 best val_acc 0.461\n",
      "lr 0.0500 | e 250 val loss 1.393 val acc 0.484 best val_acc 0.501\n",
      "lr 0.0500 | e 300 val loss 1.237 val acc 0.549 best val_acc 0.549\n",
      "lr 0.0500 | e 350 val loss 1.129 val acc 0.594 best val_acc 0.601\n",
      "lr 0.0500 | e 400 val loss 1.301 val acc 0.526 best val_acc 0.621\n",
      "lr 0.0500 | e 450 val loss 1.231 val acc 0.561 best val_acc 0.646\n",
      "lr 0.0500 | e 500 val loss 1.129 val acc 0.626 best val_acc 0.665\n",
      "lr 0.0500 | e 550 val loss 1.093 val acc 0.629 best val_acc 0.676\n",
      "lr 0.0500 | e 600 val loss 1.199 val acc 0.604 best val_acc 0.688\n",
      "lr 0.0500 | e 650 val loss 1.004 val acc 0.665 best val_acc 0.693\n",
      "lr 0.0500 | e 700 val loss 0.981 val acc 0.693 best val_acc 0.695\n",
      "lr 0.0500 | e 750 val loss 2.159 val acc 0.182 best val_acc 0.698\n",
      "lr 0.0500 | e 800 val loss 1.448 val acc 0.457 best val_acc 0.698\n",
      "lr 0.0500 | e 850 val loss 1.283 val acc 0.535 best val_acc 0.698\n",
      "lr 0.0500 | e 900 val loss 0.986 val acc 0.657 best val_acc 0.698\n",
      "lr 0.0500 | e 950 val loss 0.979 val acc 0.679 best val_acc 0.698\n",
      "lr 0.0500 | e 1000 val loss 1.026 val acc 0.657 best val_acc 0.698\n",
      "lr 0.0500 | e 1050 val loss 1.084 val acc 0.686 best val_acc 0.698\n",
      "lr 0.0500 | e 1100 val loss 1.074 val acc 0.681 best val_acc 0.698\n",
      "lr 0.0500 | e 1150 val loss 1.361 val acc 0.672 best val_acc 0.698\n",
      "lr 0.0500 | e 1200 val loss 1.010 val acc 0.681 best val_acc 0.698\n",
      "lr 0.0500 | e 1250 val loss 1.108 val acc 0.679 best val_acc 0.698\n",
      "lr 0.0500 | e 1300 val loss 1.102 val acc 0.680 best val_acc 0.698\n",
      "lr 0.0500 | e 1350 val loss 1.270 val acc 0.683 best val_acc 0.698\n",
      "lr 0.0500 | e 1400 val loss 1.760 val acc 0.683 best val_acc 0.698\n",
      "lr 0.0500 | e 1450 val loss 44.288 val acc 0.098 best val_acc 0.698\n",
      "val loss 304155.125000... exit\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "nepochs=2000\n",
    "num_workers = 100\n",
    "nbyz = 20\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "global_lrs = [.05]\n",
    "\n",
    "for global_lr in global_lrs:\n",
    "    \n",
    "    fed_model = alexnet().cuda()\n",
    "    optimizer = SGD(fed_model.parameters(), lr = global_lr, momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        torch.cuda.empty_cache()\n",
    "        round_clients = np.arange(nbyz, num_workers)\n",
    "        round_benign = round_clients\n",
    "        user_updates=[]\n",
    "        benign_norm = 0\n",
    "        user_grads = []\n",
    "\n",
    "        for client_idx in range(nbyz, num_workers):\n",
    "            net_ = copy.deepcopy(fed_model)\n",
    "            net_.zero_grad()\n",
    "            for _, (inputs, targets) in enumerate(train_loaders[client_idx][1]):\n",
    "                output = net_(inputs.float().cuda())\n",
    "                loss = criterion(output, targets.long().cuda())\n",
    "                loss.backward(retain_graph = True)\n",
    "            param_grad=[]\n",
    "            for param in net_.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "            user_updates=param_grad[None, :] if len(user_updates)==0 else torch.cat((user_updates,param_grad[None,:]), 0)\n",
    "            del net_\n",
    "\n",
    "        agg_grads = torch.mean(user_updates, 0)\n",
    "        del user_updates\n",
    "        start_idx=0\n",
    "        optimizer.zero_grad()\n",
    "        model_grads=[]\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "        optimizer.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "        is_best = best_global_acc < val_acc\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if epoch_num%50==0 or epoch_num==nepochs-1:\n",
    "            print('lr %.4f | e %d val loss %.3f val acc %.3f best val_acc %.3f'% (global_lr, epoch_num, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "        if math.isnan(val_loss) or val_loss > 100000:\n",
    "            print('val loss %f... exit'%val_loss)\n",
    "            break\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2b793a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.0500 | e 0 val loss 2.303 val acc 0.098 best val_acc 0.098\n",
      "lr 0.0500 | e 50 val loss 2.299 val acc 0.223 best val_acc 0.231\n",
      "lr 0.0500 | e 100 val loss 2.048 val acc 0.226 best val_acc 0.240\n",
      "lr 0.0500 | e 150 val loss 1.826 val acc 0.291 best val_acc 0.309\n",
      "lr 0.0500 | e 200 val loss 1.699 val acc 0.348 best val_acc 0.363\n",
      "lr 0.0500 | e 250 val loss 2.990 val acc 0.125 best val_acc 0.371\n",
      "lr 0.0500 | e 300 val loss 2.312 val acc 0.145 best val_acc 0.371\n",
      "lr 0.0500 | e 350 val loss 2.364 val acc 0.122 best val_acc 0.371\n",
      "lr 0.0500 | e 400 val loss 2.124 val acc 0.206 best val_acc 0.371\n",
      "lr 0.0500 | e 450 val loss 2.112 val acc 0.208 best val_acc 0.371\n",
      "lr 0.0500 | e 500 val loss 2.220 val acc 0.182 best val_acc 0.371\n",
      "lr 0.0500 | e 550 val loss 3.809 val acc 0.109 best val_acc 0.371\n",
      "lr 0.0500 | e 600 val loss 2.380 val acc 0.145 best val_acc 0.371\n",
      "lr 0.0500 | e 650 val loss 2.279 val acc 0.183 best val_acc 0.371\n",
      "lr 0.0500 | e 700 val loss 2.347 val acc 0.147 best val_acc 0.371\n",
      "lr 0.0500 | e 750 val loss 2.104 val acc 0.230 best val_acc 0.371\n",
      "lr 0.0500 | e 800 val loss 2.248 val acc 0.207 best val_acc 0.371\n",
      "lr 0.0500 | e 850 val loss 2.101 val acc 0.254 best val_acc 0.371\n",
      "lr 0.0500 | e 900 val loss 1.983 val acc 0.282 best val_acc 0.371\n",
      "lr 0.0500 | e 950 val loss 2.158 val acc 0.230 best val_acc 0.371\n",
      "lr 0.0500 | e 999 val loss 1.884 val acc 0.318 best val_acc 0.371\n",
      "lr 0.0500 | e 1000 val loss 1.894 val acc 0.316 best val_acc 0.371\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "nepochs=1000\n",
    "num_workers = 100\n",
    "nbyz = 20\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "global_lrs = [.05]\n",
    "\n",
    "for global_lr in global_lrs:\n",
    "    \n",
    "    fed_model = alexnet().cuda()\n",
    "    optimizer = SGD(fed_model.parameters(), lr = global_lr, momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        torch.cuda.empty_cache()\n",
    "        round_clients = np.arange(nbyz, num_workers)\n",
    "        round_benign = round_clients\n",
    "        user_updates=[]\n",
    "        benign_norm = 0\n",
    "        user_grads = []\n",
    "\n",
    "        for client_idx in range(nbyz, num_workers):\n",
    "            net_ = copy.deepcopy(fed_model)\n",
    "            net_.zero_grad()\n",
    "            for _, (inputs, targets) in enumerate(train_loaders[client_idx][1]):\n",
    "                output = net_(inputs.float().cuda())\n",
    "                loss = criterion(output, targets.long().cuda())\n",
    "                loss.backward(retain_graph = True)\n",
    "            param_grad=[]\n",
    "            for param in net_.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "            user_updates=param_grad[None, :] if len(user_updates)==0 else torch.cat((user_updates,param_grad[None,:]), 0)\n",
    "            del net_\n",
    "\n",
    "        user_updates[:nbyz] = full_trim(user_updates[:nbyz], nbyz)\n",
    "        agg_grads = torch.mean(user_updates, 0)\n",
    "        del user_updates\n",
    "        start_idx=0\n",
    "        optimizer.zero_grad()\n",
    "        model_grads=[]\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "        optimizer.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "        is_best = best_global_acc < val_acc\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if epoch_num%50==0 or epoch_num==nepochs-1:\n",
    "            print('lr %.4f | e %d val loss %.3f val acc %.3f best val_acc %.3f'% (global_lr, epoch_num, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "        if math.isnan(val_loss) or val_loss > 100000:\n",
    "            print('val loss %f... exit'%val_loss)\n",
    "            break\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6317bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndss21_attack_trmean(all_updates, n_attackers, dev_type='sign', threshold=5.0, threshold_diff=1e-5, fast_ndss=True):\n",
    "    \n",
    "    model_re = torch.mean(all_updates, 0)\n",
    "    \n",
    "    if dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "    if fast_ndss:\n",
    "        return (model_re - threshold * deviation)\n",
    "\n",
    "    lamda = torch.Tensor([threshold]).to(device)  # compute_lambda_our(all_updates, model_re, n_attackers)\n",
    "\n",
    "    threshold_diff = threshold_diff\n",
    "    prev_loss = -1\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        agg_grads = tr_mean(mal_updates, n_attackers)\n",
    "\n",
    "        loss = torch.norm(agg_grads - model_re)\n",
    "\n",
    "        if prev_loss < loss:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "        prev_loss = loss\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3488c56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.0500 | e 0 val loss 2.303 val acc 0.099 best val_acc 0.099\n",
      "lr 0.0500 | e 50 val loss 12.147 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 100 val loss 24.451 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 150 val loss 36.761 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 200 val loss 49.069 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 250 val loss 61.373 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 300 val loss 73.674 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 350 val loss 85.972 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 400 val loss 98.267 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 450 val loss 110.559 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 500 val loss 122.847 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 550 val loss 135.133 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 600 val loss 147.416 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 650 val loss 159.695 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 700 val loss 171.972 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 750 val loss 184.245 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 800 val loss 196.515 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 850 val loss 208.783 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 900 val loss 221.047 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 950 val loss 233.308 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 999 val loss 245.321 val acc 0.100 best val_acc 0.100\n",
      "lr 0.0500 | e 1000 val loss 245.566 val acc 0.100 best val_acc 0.100\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "nepochs=1000\n",
    "num_workers = 100\n",
    "nbyz = 20\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "global_lrs = [.05]\n",
    "\n",
    "for global_lr in global_lrs:\n",
    "    \n",
    "    fed_model = alexnet().cuda()\n",
    "    optimizer = SGD(fed_model.parameters(), lr = global_lr, momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        torch.cuda.empty_cache()\n",
    "        round_clients = np.arange(nbyz, num_workers)\n",
    "        round_benign = round_clients\n",
    "        user_updates=[]\n",
    "        benign_norm = 0\n",
    "        user_grads = []\n",
    "\n",
    "        for client_idx in range(nbyz, num_workers):\n",
    "            net_ = copy.deepcopy(fed_model)\n",
    "            net_.zero_grad()\n",
    "            for _, (inputs, targets) in enumerate(train_loaders[client_idx][1]):\n",
    "                output = net_(inputs.float().cuda())\n",
    "                loss = criterion(output, targets.long().cuda())\n",
    "                loss.backward(retain_graph = True)\n",
    "            param_grad=[]\n",
    "            for param in net_.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "            user_updates=param_grad[None, :] if len(user_updates)==0 else torch.cat((user_updates,param_grad[None,:]), 0)\n",
    "            del net_\n",
    "\n",
    "        mal_update = ndss21_attack_trmean(user_updates[:nbyz], 15, dev_type='std', threshold=20.0, threshold_diff=1e-5)\n",
    "        user_updates[:nbyz] = torch.stack([mal_update] * nbyz)\n",
    "        \n",
    "        agg_grads = torch.mean(user_updates, 0)\n",
    "        \n",
    "        del user_updates\n",
    "        start_idx=0\n",
    "        optimizer.zero_grad()\n",
    "        model_grads=[]\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "        optimizer.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "        is_best = best_global_acc < val_acc\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if epoch_num%50==0 or epoch_num==nepochs-1:\n",
    "            print('lr %.4f | e %d val loss %.3f val acc %.3f best val_acc %.3f'% (global_lr, epoch_num, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "        if math.isnan(val_loss) or val_loss > 100000:\n",
    "            print('val loss %f... exit'%val_loss)\n",
    "            break\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf63c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
