{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for Fashion-MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821b1014",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu083/4287636/ipykernel_411122/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a52c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "            \n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_train_data(trainset, num_workers=100, bias=0.5):\n",
    "    np.random.seed(0)\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        \n",
    "        if not len(each_worker_data[selected_worker]):\n",
    "            each_worker_data[selected_worker] = x[None, :]\n",
    "        else:\n",
    "            each_worker_data[selected_worker]= torch.concat((each_worker_data[selected_worker], x[None, :]))\n",
    "        \n",
    "        each_worker_label[selected_worker].append(y)\n",
    "    \n",
    "    each_worker_tr_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_tr_label = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    for i in range(num_workers):\n",
    "        w_len = len(each_worker_data[i])\n",
    "        len_tr = int(6 * w_len / 7)\n",
    "        len_te = w_len - len_tr\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        each_worker_tr_data[i] = each_worker_data[i][tr_idx]\n",
    "        each_worker_tr_label[i] = torch.Tensor(each_worker_label[i])[tr_idx]\n",
    "        \n",
    "        each_worker_te_data[i] = each_worker_data[i][te_idx]\n",
    "        each_worker_te_label[i] = torch.Tensor(each_worker_label[i])[te_idx]\n",
    "        \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    del each_worker_data, each_worker_label\n",
    "    return each_worker_tr_data, each_worker_tr_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f36f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_data_dirichlet(trainset, num_workers, alpha=1, force=False):\n",
    "    per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=alpha, force=force)\n",
    "    \n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(6*w_len/7)\n",
    "        len_te = w_len - len_tr\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        \n",
    "        for idx in tr_idx:\n",
    "            each_worker_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_data[worker_idx] = torch.stack(each_worker_data[worker_idx])\n",
    "        each_worker_label[worker_idx] = torch.Tensor(each_worker_label[worker_idx]).long()\n",
    "        \n",
    "        for idx in te_idx:\n",
    "            each_worker_te_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_te_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_te_data[worker_idx] = torch.stack(each_worker_te_data[worker_idx])\n",
    "        each_worker_te_label[worker_idx] = torch.Tensor(each_worker_te_label[worker_idx]).long()\n",
    "    \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    \n",
    "    return each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6b5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 30, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(30, 50, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(800, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4af9e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def fldetector(old_gradients, user_grads, b=0, hvp=None, agr='tr_mean'):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    if agr == 'average':\n",
    "        agg_grads = torch.mean(user_grads, 0)\n",
    "    elif agr == 'median':\n",
    "        agg_grads = torch.median(user_grads, 0)[0]\n",
    "    elif agr == 'tr_mean':\n",
    "        agg_grads = tr_mean(user_grads, b)\n",
    "    return agg_grads, distance\n",
    "\n",
    "def lbfgs(S_k_list, Y_k_list, v):\n",
    "    curr_S_k = torch.stack(S_k_list).T\n",
    "    curr_Y_k = torch.stack(Y_k_list).T\n",
    "    S_k_time_Y_k = np.dot(curr_S_k.T.cpu().numpy(), curr_Y_k.cpu().numpy())\n",
    "    S_k_time_S_k = np.dot(curr_S_k.T.cpu().numpy(), curr_S_k.cpu().numpy())\n",
    "    R_k = np.triu(S_k_time_Y_k)\n",
    "    L_k = S_k_time_Y_k - R_k\n",
    "    sigma_k = np.dot(Y_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()) / (np.dot(S_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()))\n",
    "    D_k_diag = np.diag(S_k_time_Y_k)\n",
    "    upper_mat = np.concatenate((sigma_k * S_k_time_S_k, L_k), axis=1)\n",
    "    lower_mat = np.concatenate((L_k.T, -np.diag(D_k_diag)), axis=1)\n",
    "    mat = np.concatenate((upper_mat, lower_mat), axis=0)\n",
    "    mat_inv = np.linalg.inv(mat)\n",
    "\n",
    "    approx_prod = sigma_k * v.cpu().numpy()\n",
    "    approx_prod = approx_prod.T\n",
    "    p_mat = np.concatenate((np.dot(curr_S_k.T.cpu().numpy(), sigma_k * v.unsqueeze(0).T.cpu().numpy()), np.dot(curr_Y_k.T.cpu().numpy(), v.unsqueeze(0).T.cpu().numpy())), axis=0)\n",
    "    approx_prod -= np.dot(np.dot(np.concatenate((sigma_k * curr_S_k.cpu().numpy(), curr_Y_k.cpu().numpy()), axis=1), mat_inv), p_mat)\n",
    "\n",
    "    return approx_prod\n",
    "\n",
    "def detection(score, nobyz, nworkers):\n",
    "    estimator = KMeans(n_clusters=2)\n",
    "    estimator.fit(score.reshape(-1, 1))\n",
    "    label_pred = estimator.labels_\n",
    "    if np.mean(score[label_pred==0])<np.mean(score[label_pred==1]):\n",
    "        #0 is the label of malicious clients\n",
    "        label_pred = 1 - label_pred\n",
    "    real_label=np.ones(nworkers)\n",
    "    real_label[:nobyz]=0\n",
    "    acc=len(label_pred[label_pred==real_label])/nworkers\n",
    "    recall=1-np.sum(label_pred[:nobyz])/nobyz\n",
    "    fpr=1-np.sum(label_pred[nobyz:])/(nworkers-nobyz)\n",
    "    fnr=np.sum(label_pred[:nobyz])/nobyz\n",
    "    auc = roc_auc_score(real_label, label_pred)\n",
    "    print(\"acc %0.4f; recall %0.4f; fpr %0.4f; fnr %0.4f; auc %.4f\" % (acc, recall, fpr, fnr, auc))\n",
    "    return acc, fpr, fnr, auc\n",
    "    # print(silhouette_score(score.reshape(-1, 1), label_pred))\n",
    "\n",
    "def detection1(score, nobyz):\n",
    "    nrefs = 10\n",
    "    ks = range(1, 8)\n",
    "    gaps = np.zeros(len(ks))\n",
    "    gapDiff = np.zeros(len(ks) - 1)\n",
    "    sdk = np.zeros(len(ks))\n",
    "    min = np.min(score)\n",
    "    max = np.max(score)\n",
    "    score = (score - min)/(max-min)\n",
    "    for i, k in enumerate(ks):\n",
    "        estimator = KMeans(n_clusters=k)\n",
    "        estimator.fit(score.reshape(-1, 1))\n",
    "        label_pred = estimator.labels_\n",
    "        center = estimator.cluster_centers_\n",
    "        Wk = np.sum([np.square(score[m]-center[label_pred[m]]) for m in range(len(score))])\n",
    "        WkRef = np.zeros(nrefs)\n",
    "        for j in range(nrefs):\n",
    "            rand = np.random.uniform(0, 1, len(score))\n",
    "            estimator = KMeans(n_clusters=k)\n",
    "            estimator.fit(rand.reshape(-1, 1))\n",
    "            label_pred = estimator.labels_\n",
    "            center = estimator.cluster_centers_\n",
    "            WkRef[j] = np.sum([np.square(rand[m]-center[label_pred[m]]) for m in range(len(rand))])\n",
    "        gaps[i] = np.log(np.mean(WkRef)) - np.log(Wk)\n",
    "        sdk[i] = np.sqrt((1.0 + nrefs) / nrefs) * np.std(np.log(WkRef))\n",
    "\n",
    "        if i > 0:\n",
    "            gapDiff[i - 1] = gaps[i - 1] - gaps[i] + sdk[i]\n",
    "    #print(gapDiff)\n",
    "    for i in range(len(gapDiff)):\n",
    "        if gapDiff[i] >= 0:\n",
    "            select_k = i+1\n",
    "            break\n",
    "    if select_k == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        # print('Attack Detected!')\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c35e83a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(train_data, labels, model, optimizer, batch_size=20):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    len_t = (len(train_data) // batch_size)\n",
    "    if len(train_data)%batch_size:\n",
    "        len_t += 1\n",
    "    r=np.arange(len(train_data))\n",
    "    np.random.shuffle(r)\n",
    "    train_data = train_data[r]\n",
    "    labels = labels[r]\n",
    "    for ind in range(len_t):\n",
    "        inputs = train_data[ind * batch_size:(ind + 1) * batch_size].cuda()\n",
    "        targets = labels[ind * batch_size:(ind + 1) * batch_size].cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "\n",
    "def test(test_data, labels, model, criterion, use_cuda, debug_='MEDIUM', batch_size=64):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    model.eval()\n",
    "    len_t = (len(test_data) // batch_size)\n",
    "    if len(test_data)%batch_size:\n",
    "        len_t += 1\n",
    "    with torch.no_grad():\n",
    "        for ind in range(len_t):\n",
    "            inputs = test_data[ind * batch_size:(ind + 1) * batch_size].cuda()\n",
    "            targets = labels[ind * batch_size:(ind + 1) * batch_size].cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "    return (losses.avg, top1.avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ace04bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = True\n",
    "\n",
    "if distribution=='fang':\n",
    "    each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_train_data(all_data, num_workers=100, bias=param)\n",
    "elif distribution == 'dirichlet':\n",
    "    each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_data_dirichlet(all_data, num_workers, alpha=param, force=force)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8e118f",
   "metadata": {},
   "source": [
    "## Adaptive attack on FLD-trmean + Fang-0.5 distribution + m = {5, 10, 15, 20, 25}%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc3e44",
   "metadata": {},
   "source": [
    "# Slow baseline results\n",
    "\n",
    "## NDSS21 + sign\n",
    "### run 1\n",
    "Attack Detected!\n",
    "Stop at iteration: 64\n",
    "acc 0.4600; recall 1.0000; fpr 0.5684; fnr 0.0000;\n",
    "0.6788342693682597\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 43\n",
    "acc 0.7000; recall 0.0000; fpr 0.2222; fnr 1.0000;\n",
    "0.7644290534161355\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 41\n",
    "acc 0.5500; recall 0.0000; fpr 0.3529; fnr 1.0000;\n",
    "0.7505490125167779\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 94\n",
    "acc 0.5000; recall 0.0000; fpr 0.3750; fnr 1.0000;\n",
    "0.7077810947725642\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 117\n",
    "acc 0.3200; recall 0.0000; fpr 0.5733; fnr 1.0000;\n",
    "0.6880209854246435\n",
    "\n",
    "### run 2\n",
    "Attack Detected!\n",
    "Stop at iteration: 23\n",
    "acc 0.7500; recall 0.0000; fpr 0.2105; fnr 1.0000;\n",
    "0.7481008408585612\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 79\n",
    "acc 0.5500; recall 1.0000; fpr 0.5000; fnr 0.0000;\n",
    "0.661830352499663\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 88\n",
    "acc 0.4700; recall 1.0000; fpr 0.6235; fnr 0.0000;\n",
    "0.6758088873698362\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 20\n",
    "acc 0.4000; recall 1.0000; fpr 0.7500; fnr 0.0000;\n",
    "0.7748910520994794\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 20\n",
    "acc 0.6500; recall 0.0000; fpr 0.1333; fnr 1.0000;\n",
    "0.8865329174556073\n",
    "\n",
    "### run 3\n",
    "Attack Detected!\n",
    "Stop at iteration: 27\n",
    "acc 0.4800; recall 0.0000; fpr 0.4947; fnr 1.0000;\n",
    "0.67263498090412\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 80\n",
    "acc 0.5200; recall 1.0000; fpr 0.5333; fnr 0.0000;\n",
    "0.6759156220510705\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 67\n",
    "acc 0.5100; recall 1.0000; fpr 0.5765; fnr 0.0000;\n",
    "0.7016027680297964\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 123\n",
    "acc 0.5000; recall 0.0000; fpr 0.3750; fnr 1.0000;\n",
    "0.7239521247264237\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 45\n",
    "acc 0.4500; recall 1.0000; fpr 0.7333; fnr 0.0000;\n",
    "0.7853833062558434\n",
    "\n",
    "## NDSS21 + unit_vec\n",
    "### run 1\n",
    "Attack Detected!\n",
    "Stop at iteration: 74\n",
    "acc 0.4700; recall 1.0000; fpr 0.5579; fnr 0.0000;\n",
    "0.6862680900318041\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 73\n",
    "acc 0.5100; recall 1.0000; fpr 0.5444; fnr 0.0000;\n",
    "0.6832874125787594\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 31\n",
    "acc 0.4600; recall 1.0000; fpr 0.6353; fnr 0.0000;\n",
    "0.6872566346844493\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 20\n",
    "acc 0.4000; recall 1.0000; fpr 0.7500; fnr 0.0000;\n",
    "0.8132205038798791\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 34\n",
    "acc 0.5900; recall 1.0000; fpr 0.5467; fnr 0.0000;\n",
    "0.710151115569618\n",
    "\n",
    "### run 2\n",
    "\n",
    "\n",
    "### run 3\n",
    "\n",
    "## NDSS21 + std\n",
    "### run 1\n",
    "Attack Detected!\n",
    "Stop at iteration: 73\n",
    "acc 0.4700; recall 1.0000; fpr 0.5579; fnr 0.0000;\n",
    "0.6855585577340589\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 117\n",
    "acc 0.5500; recall 1.0000; fpr 0.5000; fnr 0.0000;\n",
    "0.6487281301141049\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 39\n",
    "acc 0.7500; recall 1.0000; fpr 0.2941; fnr 0.0000;\n",
    "0.7275960144614209\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 104\n",
    "acc 0.4500; recall 1.0000; fpr 0.6875; fnr 0.0000;\n",
    "0.6946783860345377\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 84\n",
    "acc 0.6300; recall 0.0000; fpr 0.1600; fnr 1.0000;\n",
    "0.7973424083044303\n",
    "\n",
    "### run 2\n",
    "\n",
    "\n",
    "### run 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69ed6241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop at iteration: 66\n",
      "e 66 | val_loss 1.851 val acc 46.083 | best val_acc 46.083\n",
      "acc 0.4235; recall 1.0000; fpr 0.6125; fnr 0.0000; auc 0.6937\n",
      "Stop at iteration: 71\n",
      "e 71 | val_loss 1.872 val acc 45.784 | best val_acc 45.784\n",
      "acc 0.4235; recall 1.0000; fpr 0.6125; fnr 0.0000; auc 0.6937\n",
      "Stop at iteration: 90\n",
      "e 90 | val_loss 1.906 val acc 46.421 | best val_acc 46.421\n",
      "acc 0.4706; recall 1.0000; fpr 0.5625; fnr 0.0000; auc 0.7188\n",
      "Stop at iteration: 76\n",
      "e 76 | val_loss 1.488 val acc 54.236 | best val_acc 54.236\n",
      "acc 0.5882; recall 0.0000; fpr 0.3750; fnr 1.0000; auc 0.3125\n",
      "Stop at iteration: 88\n",
      "e 88 | val_loss 1.562 val acc 57.591 | best val_acc 57.591\n",
      "acc 0.6471; recall 1.0000; fpr 0.3750; fnr 0.0000; auc 0.8125\n",
      "Stop at iteration: 66\n",
      "e 66 | val_loss 1.796 val acc 48.163 | best val_acc 48.163\n",
      "acc 0.5294; recall 1.0000; fpr 0.5000; fnr 0.0000; auc 0.7500\n",
      "Stop at iteration: 54\n",
      "e 54 | val_loss 1.957 val acc 45.426 | best val_acc 45.436\n",
      "acc 0.3176; recall 1.0000; fpr 0.7250; fnr 0.0000; auc 0.6375\n",
      "Stop at iteration: 93\n",
      "e 93 | val_loss 1.745 val acc 48.741 | best val_acc 48.741\n",
      "acc 0.4353; recall 1.0000; fpr 0.6000; fnr 0.0000; auc 0.7000\n",
      "Stop at iteration: 87\n",
      "e 87 | val_loss 1.800 val acc 47.576 | best val_acc 47.576\n",
      "acc 0.5294; recall 1.0000; fpr 0.5000; fnr 0.0000; auc 0.7500\n",
      "Stop at iteration: 85\n",
      "e 85 | val_loss 1.772 val acc 48.910 | best val_acc 48.910\n",
      "acc 0.6471; recall 1.0000; fpr 0.3750; fnr 0.0000; auc 0.8125\n",
      "Stop at iteration: 82\n",
      "e 82 | val_loss 1.709 val acc 49.258 | best val_acc 49.258\n",
      "acc 0.4556; recall 1.0000; fpr 0.6125; fnr 0.0000; auc 0.6937\n",
      "Stop at iteration: 70\n",
      "e 70 | val_loss 1.828 val acc 53.748 | best val_acc 53.748\n",
      "acc 0.3444; recall 0.0000; fpr 0.6125; fnr 1.0000; auc 0.1938\n",
      "Stop at iteration: 103\n",
      "e 103 | val_loss 1.636 val acc 60.538 | best val_acc 60.538\n",
      "acc 0.3444; recall 0.0000; fpr 0.6125; fnr 1.0000; auc 0.1938\n",
      "Stop at iteration: 86\n",
      "e 86 | val_loss 1.551 val acc 59.930 | best val_acc 59.930\n",
      "acc 0.5556; recall 0.0000; fpr 0.3750; fnr 1.0000; auc 0.3125\n",
      "Stop at iteration: 103\n",
      "e 103 | val_loss 1.798 val acc 45.744 | best val_acc 45.744\n",
      "acc 0.4111; recall 0.0000; fpr 0.5375; fnr 1.0000; auc 0.2313\n",
      "Stop at iteration: 92\n",
      "e 92 | val_loss 1.851 val acc 45.376 | best val_acc 45.376\n",
      "acc 0.4444; recall 0.0000; fpr 0.5000; fnr 1.0000; auc 0.2500\n",
      "Stop at iteration: 21\n",
      "e 21 | val_loss 2.246 val acc 27.944 | best val_acc 27.944\n",
      "acc 0.5778; recall 0.0000; fpr 0.3500; fnr 1.0000; auc 0.3250\n",
      "Stop at iteration: 77\n",
      "e 77 | val_loss 1.872 val acc 58.407 | best val_acc 58.965\n",
      "acc 0.4444; recall 1.0000; fpr 0.6250; fnr 0.0000; auc 0.6875\n",
      "Stop at iteration: 38\n",
      "e 38 | val_loss 2.145 val acc 33.907 | best val_acc 33.907\n",
      "acc 0.2444; recall 1.0000; fpr 0.8500; fnr 0.0000; auc 0.5750\n",
      "Stop at iteration: 94\n",
      "e 94 | val_loss 1.733 val acc 51.857 | best val_acc 51.857\n",
      "acc 0.4889; recall 1.0000; fpr 0.5750; fnr 0.0000; auc 0.7125\n",
      "Stop at iteration: 97\n",
      "e 97 | val_loss 1.690 val acc 59.393 | best val_acc 59.393\n",
      "acc 0.4211; recall 0.0000; fpr 0.5000; fnr 1.0000; auc 0.2500\n",
      "Stop at iteration: 56\n",
      "e 56 | val_loss 2.028 val acc 43.435 | best val_acc 43.435\n",
      "acc 0.4421; recall 1.0000; fpr 0.6625; fnr 0.0000; auc 0.6687\n",
      "Stop at iteration: 118\n",
      "e 118 | val_loss 1.427 val acc 60.777 | best val_acc 60.777\n",
      "acc 0.5263; recall 0.0000; fpr 0.3750; fnr 1.0000; auc 0.3125\n",
      "Stop at iteration: 64\n",
      "e 64 | val_loss 2.167 val acc 38.965 | best val_acc 38.965\n",
      "acc 0.3263; recall 1.0000; fpr 0.8000; fnr 0.0000; auc 0.6000\n",
      "Stop at iteration: 106\n",
      "e 106 | val_loss 1.759 val acc 62.081 | best val_acc 62.081\n",
      "acc 0.4842; recall 1.0000; fpr 0.6125; fnr 0.0000; auc 0.6937\n",
      "Stop at iteration: 84\n",
      "e 84 | val_loss 1.710 val acc 57.601 | best val_acc 57.601\n",
      "acc 0.4842; recall 1.0000; fpr 0.6125; fnr 0.0000; auc 0.6937\n",
      "Stop at iteration: 109\n",
      "e 109 | val_loss 1.425 val acc 59.024 | best val_acc 59.024\n",
      "acc 0.5895; recall 1.0000; fpr 0.4875; fnr 0.0000; auc 0.7562\n",
      "Stop at iteration: 28\n",
      "e 28 | val_loss 2.226 val acc 26.670 | best val_acc 26.670\n",
      "acc 0.5789; recall 1.0000; fpr 0.5000; fnr 0.0000; auc 0.7500\n",
      "Stop at iteration: 86\n",
      "e 86 | val_loss 1.740 val acc 59.721 | best val_acc 59.721\n",
      "acc 0.3368; recall 0.0000; fpr 0.6000; fnr 1.0000; auc 0.2000\n",
      "Stop at iteration: 24\n",
      "e 24 | val_loss 2.260 val acc 18.696 | best val_acc 18.696\n",
      "acc 0.7895; recall 1.0000; fpr 0.2500; fnr 0.0000; auc 0.8750\n",
      "Stop at iteration: 137\n",
      "e 137 | val_loss 1.255 val acc 65.286 | best val_acc 65.286\n",
      "acc 0.5000; recall 0.0000; fpr 0.3750; fnr 1.0000; auc 0.3125\n",
      "Stop at iteration: 27\n",
      "e 27 | val_loss 2.273 val acc 21.882 | best val_acc 21.882\n",
      "acc 0.5300; recall 1.0000; fpr 0.5875; fnr 0.0000; auc 0.7063\n",
      "Stop at iteration: 43\n",
      "e 43 | val_loss 2.245 val acc 13.738 | best val_acc 13.738\n",
      "acc 0.5400; recall 1.0000; fpr 0.5750; fnr 0.0000; auc 0.7125\n",
      "Stop at iteration: 59\n",
      "e 59 | val_loss 2.024 val acc 49.318 | best val_acc 49.318\n",
      "acc 0.4100; recall 1.0000; fpr 0.7375; fnr 0.0000; auc 0.6312\n",
      "Stop at iteration: 99\n",
      "e 99 | val_loss 1.721 val acc 57.073 | best val_acc 57.073\n",
      "acc 0.3700; recall 0.0000; fpr 0.5375; fnr 1.0000; auc 0.2313\n",
      "Stop at iteration: 40\n",
      "e 40 | val_loss 2.201 val acc 19.622 | best val_acc 19.622\n",
      "acc 0.3800; recall 1.0000; fpr 0.7750; fnr 0.0000; auc 0.6125\n",
      "Stop at iteration: 20\n",
      "e 20 | val_loss 2.293 val acc 17.501 | best val_acc 17.501\n",
      "acc 0.5100; recall 1.0000; fpr 0.6125; fnr 0.0000; auc 0.6937\n",
      "Stop at iteration: 20\n",
      "e 20 | val_loss 2.285 val acc 21.234 | best val_acc 21.234\n",
      "acc 0.3900; recall 1.0000; fpr 0.7625; fnr 0.0000; auc 0.6188\n",
      "Stop at iteration: 110\n",
      "e 110 | val_loss 1.590 val acc 54.604 | best val_acc 54.604\n",
      "acc 0.4100; recall 0.0000; fpr 0.4875; fnr 1.0000; auc 0.2562\n",
      "Stop at iteration: 57\n",
      "e 57 | val_loss 2.020 val acc 51.896 | best val_acc 51.896\n",
      "acc 0.4000; recall 1.0000; fpr 0.7500; fnr 0.0000; auc 0.6250\n",
      "Stop at iteration: 91\n",
      "e 91 | val_loss 1.796 val acc 39.502 | best val_acc 39.502\n",
      "acc 0.2857; recall 0.0000; fpr 0.6250; fnr 1.0000; auc 0.1875\n",
      "Stop at iteration: 83\n",
      "e 83 | val_loss 1.762 val acc 50.453 | best val_acc 50.453\n",
      "acc 0.2952; recall 0.0000; fpr 0.6125; fnr 1.0000; auc 0.1938\n",
      "Stop at iteration: 79\n",
      "e 79 | val_loss 1.692 val acc 50.592 | best val_acc 50.592\n",
      "acc 0.2857; recall 0.0000; fpr 0.6250; fnr 1.0000; auc 0.1875\n",
      "Stop at iteration: 61\n",
      "e 61 | val_loss 2.018 val acc 36.078 | best val_acc 41.453\n",
      "acc 0.3143; recall 0.0000; fpr 0.5875; fnr 1.0000; auc 0.2062\n",
      "Stop at iteration: 82\n",
      "e 82 | val_loss 1.789 val acc 53.081 | best val_acc 53.320\n",
      "acc 0.2952; recall 0.0000; fpr 0.6125; fnr 1.0000; auc 0.1938\n",
      "Stop at iteration: 55\n",
      "e 55 | val_loss 2.158 val acc 36.645 | best val_acc 37.412\n",
      "acc 0.5429; recall 1.0000; fpr 0.6000; fnr 0.0000; auc 0.7000\n",
      "Stop at iteration: 27\n",
      "e 27 | val_loss 2.228 val acc 22.778 | best val_acc 24.679\n",
      "acc 0.4286; recall 1.0000; fpr 0.7500; fnr 0.0000; auc 0.6250\n",
      "Stop at iteration: 74\n",
      "e 74 | val_loss 1.889 val acc 40.528 | best val_acc 40.528\n",
      "acc 0.2857; recall 0.0000; fpr 0.6250; fnr 1.0000; auc 0.1875\n",
      "Stop at iteration: 35\n",
      "e 35 | val_loss 2.203 val acc 20.956 | best val_acc 20.956\n",
      "acc 0.3524; recall 1.0000; fpr 0.8500; fnr 0.0000; auc 0.5750\n",
      "Stop at iteration: 21\n",
      "e 21 | val_loss 2.303 val acc 10.164 | best val_acc 10.164\n",
      "acc 0.5238; recall 1.0000; fpr 0.6250; fnr 0.0000; auc 0.6875\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SLOW BASELINE\n",
    "'''\n",
    "n_malicious = [5, 10, 15, 20, 25]\n",
    "n_runs=10\n",
    "results = {}\n",
    "for n_mal in n_malicious:\n",
    "    results[n_mal] = {'acc': [], 'fpr': [], 'fnr': [], 'auc': []}\n",
    "    for run in range(n_runs):\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            # delete all variables\n",
    "            del weight_record, grad_record, old_grad_list, num_workers, malicious_scores, start_detection_epoch, window_size\n",
    "            del good_distance_rage\n",
    "            del nepochs, num_workers, local_lr, local_epochs, global_lr, agr\n",
    "            del attack_type, dev_type, fed_model\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # FL training initializations\n",
    "        nepochs=1000\n",
    "        num_workers = 100\n",
    "        use_cuda = True\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        local_lr=1\n",
    "        global_lr=0.01\n",
    "        agr='tr_mean' # ['average', 'median', 'tr_mean']\n",
    "\n",
    "        # Attack initializations\n",
    "        attack_type = 'NDSS21'\n",
    "        dev_type = 'sign'\n",
    "        nbyz = n_mal ################### Changing variable ###################\n",
    "\n",
    "        # FLD initializations\n",
    "        weight_record = []\n",
    "        grad_record = []\n",
    "        old_grad_list = []\n",
    "        malicious_scores = np.zeros((1, num_workers-(20-nbyz)))\n",
    "        start_detection_epoch = 10\n",
    "        window_size = 10\n",
    "        assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "        good_distance_rage = np.zeros((1, nbyz))\n",
    "        best_val_acc = 0.0\n",
    "\n",
    "        # FL model initializations\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "        fed_model = cnn().to(device)\n",
    "        fed_model.apply(init_weights)\n",
    "\n",
    "        for e in range(nepochs):\n",
    "            cnn_optimizer = SGD(fed_model.parameters(), lr = global_lr)\n",
    "            user_grads=[]\n",
    "            round_clients = np.arange((20-nbyz), num_workers)\n",
    "            round_benign = round_clients\n",
    "            for i in np.arange((20-nbyz), num_workers):\n",
    "                net_ = copy.deepcopy(fed_model)\n",
    "                net_.zero_grad()\n",
    "                output = net_(each_worker_data[i].reshape(-1, 1, 28, 28).cuda())\n",
    "                loss = criterion(output, each_worker_label[i].long().cuda())\n",
    "                loss.backward(retain_graph = True)\n",
    "                param_grad=[]\n",
    "                for param in net_.parameters():\n",
    "                    param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "                user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "                del net_\n",
    "\n",
    "            weight = []\n",
    "            for param in fed_model.parameters():\n",
    "                weight = param.data.view(-1) if not len(weight) else torch.cat((weight, param.data.view(-1)))\n",
    "\n",
    "            if (e > start_detection_epoch):\n",
    "                hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "                hvp = np.squeeze(hvp)\n",
    "            else:\n",
    "                hvp = None\n",
    "\n",
    "            good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "            if hvp is not None:\n",
    "                pred_grad = copy.deepcopy(good_old_grads)\n",
    "                distance = []\n",
    "                for i in range(len(good_old_grads)):\n",
    "                    pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "                good_distance_rage = np.concatenate(\n",
    "                    (good_distance_rage, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "            if attack_type != 'none' and (e < start_detection_epoch):\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "                # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "            elif e > start_detection_epoch:\n",
    "                if attack_type == 'full_trim':\n",
    "                    user_grads = full_trim(user_grads, nbyz)\n",
    "                elif attack_type == 'none':\n",
    "                    pass\n",
    "                else:\n",
    "                    if attack_type == 'NDSS21':\n",
    "                        distance_bound = np.mean(np.mean(good_distance_rage[-1:], 0))\n",
    "                        model_re = torch.mean(good_current_grads, dim=0)\n",
    "                        if dev_type == 'unit_vec':\n",
    "                            deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                        elif dev_type == 'sign':\n",
    "                            deviation = torch.sign(model_re)\n",
    "                        elif dev_type == 'std':\n",
    "                            deviation = torch.std(good_current_grads, 0)\n",
    "                        # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_rage[-1]))) / torch.norm(deviation))\n",
    "                        noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                    else:\n",
    "                        noise = torch.zeros(hvp.shape).to(device)\n",
    "                    for m in range(nbyz):\n",
    "                        # distance_bound = np.random.choice(np.mean(good_distance_rage[-1:], 0))\n",
    "                        # noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                        user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "            agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "            if distance is not None and e > (start_detection_epoch - window_size):\n",
    "                malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "            if malicious_scores.shape[0] >= window_size+1:\n",
    "                if e == window_size + 1: print('shape of malicious scores: ', malicious_scores.shape)\n",
    "                if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                    print('Stop at iteration:', e)\n",
    "                    print('e %d | val_loss %.3f val acc %.3f | best val_acc %.3f' % (e, val_loss, val_acc, best_val_acc))\n",
    "                    acc, fpr, fnr, auc = detection(\n",
    "                        np.sum(malicious_scores[-window_size:], axis=0), nbyz, len(user_grads))\n",
    "                    results[n_mal]['acc'].append(acc)\n",
    "                    results[n_mal]['fpr'].append(fpr)\n",
    "                    results[n_mal]['fnr'].append(fnr)\n",
    "                    results[n_mal]['auc'].append(auc)\n",
    "                    break\n",
    "\n",
    "            if e > (start_detection_epoch - window_size):\n",
    "                weight_record.append(weight - last_weight)\n",
    "                grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "            if (len(weight_record) > window_size):\n",
    "                del weight_record[0]\n",
    "                del grad_record[0]\n",
    "\n",
    "            last_weight = weight\n",
    "            last_grad = agg_grads\n",
    "            old_grad_list = user_grads\n",
    "            good_old_grads = good_current_grads\n",
    "\n",
    "            del user_grads\n",
    "            start_idx=0\n",
    "            cnn_optimizer.zero_grad()\n",
    "            model_grads=[]\n",
    "            for i, param in enumerate(fed_model.parameters()):\n",
    "                param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "                start_idx=start_idx+len(param.data.view(-1))\n",
    "                param_=param_.cuda()\n",
    "                model_grads.append(param_)\n",
    "            cnn_optimizer.step(model_grads)\n",
    "            val_loss, val_acc = test(global_test_data.reshape(-1,1,28,28), global_test_label.long(), fed_model, criterion, use_cuda, batch_size=50)\n",
    "            is_best = best_val_acc < val_acc\n",
    "            best_val_acc = max(best_val_acc, val_acc)\n",
    "    #         if e%20==0 or e==nepochs-1:\n",
    "    #             print('e %d | val_loss %.3f val acc %.3f | best val_acc %.3f' % (e, val_loss, val_acc, best_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6a319e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nmal 5 | fpr 0.523750 fnr 0.100000 auc 0.688125\n",
      "nmal 10 | fpr 0.565000 fnr 0.600000 auc 0.417500\n",
      "nmal 15 | fpr 0.540000 fnr 0.300000 auc 0.580000\n",
      "nmal 20 | fpr 0.620000 fnr 0.300000 auc 0.540000\n"
     ]
    }
   ],
   "source": [
    "for nmal in [5, 10, 15, 20]:\n",
    "    print('nmal %d | fpr %f fnr %f auc %f' % (nmal,\n",
    "                                              np.array(results[nmal]['fpr']).mean(),\n",
    "                                              np.array(results[nmal]['fnr']).mean(),\n",
    "                                              np.array(results[nmal]['auc']).mean(),\n",
    "                                             ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
