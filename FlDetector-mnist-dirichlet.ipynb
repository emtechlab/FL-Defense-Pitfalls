{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for MNIST with Dirichlet distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a52c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_data = torch.utils.data.DataLoader(trainset, batch_size=60000, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_data = torch.utils.data.DataLoader(testset, batch_size=5000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "            \n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_train_data(trainset, num_workers=100, bias=0.5):\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        \n",
    "        if not len(each_worker_data[selected_worker]):\n",
    "            each_worker_data[selected_worker] = x[None, :]\n",
    "        else:\n",
    "            each_worker_data[selected_worker]= torch.concat((each_worker_data[selected_worker], x[None, :]))\n",
    "        \n",
    "        each_worker_label[selected_worker].append(y)\n",
    "    \n",
    "    return each_worker_data, each_worker_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f36f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_data_dirichlet(trainset, num_workers, alpha=1, force=False):\n",
    "    per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=alpha, force=force)\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        for idx in per_participant_list[worker_idx]:\n",
    "            each_worker_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_data[worker_idx] = torch.stack(each_worker_data[worker_idx])\n",
    "        each_worker_label[worker_idx] = torch.Tensor(each_worker_label[worker_idx]).long()\n",
    "    return each_worker_data, each_worker_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "585ee015",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating participant indices for alpha 0.5\n"
     ]
    }
   ],
   "source": [
    "num_workers = 100\n",
    "distribution='dirichlet'\n",
    "if distribution=='bias':\n",
    "    each_worker_data, each_worker_label = get_client_train_data(trainset, num_workers=100, bias=0.5)\n",
    "elif distribution == 'dirichlet':\n",
    "    alpha = .5\n",
    "    force = True\n",
    "    each_worker_data, each_worker_label = get_client_data_dirichlet(trainset, num_workers, alpha=alpha, force=force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934d4b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mnist'\n",
    "bias = 0.1\n",
    "net = 'cnn'\n",
    "batch_size = 32\n",
    "# lr = 0.0002\n",
    "# lr = 1e-3\n",
    "lr = 0.01\n",
    "nworkers = 100\n",
    "nepochs = 100\n",
    "gpu = 3\n",
    "seed = 41\n",
    "nbyz = 84\n",
    "byz_type = 'full_trim'\n",
    "aggregation = 'trim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84b05be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbfgs(S_k_list, Y_k_list, v):\n",
    "    curr_S_k = torch.stack(S_k_list).T\n",
    "    curr_Y_k = torch.stack(Y_k_list).T\n",
    "    S_k_time_Y_k = np.dot(curr_S_k.T.cpu().numpy(), curr_Y_k.cpu().numpy())\n",
    "    S_k_time_S_k = np.dot(curr_S_k.T.cpu().numpy(), curr_S_k.cpu().numpy())\n",
    "    R_k = np.triu(S_k_time_Y_k)\n",
    "    L_k = S_k_time_Y_k - R_k\n",
    "    sigma_k = np.dot(Y_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()) / (np.dot(S_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()))\n",
    "    D_k_diag = np.diag(S_k_time_Y_k)\n",
    "    upper_mat = np.concatenate((sigma_k * S_k_time_S_k, L_k), axis=1)\n",
    "    lower_mat = np.concatenate((L_k.T, -np.diag(D_k_diag)), axis=1)\n",
    "    mat = np.concatenate((upper_mat, lower_mat), axis=0)\n",
    "    mat_inv = np.linalg.inv(mat)\n",
    "\n",
    "    approx_prod = sigma_k * v.cpu().numpy()\n",
    "    approx_prod = approx_prod.T\n",
    "    p_mat = np.concatenate((np.dot(curr_S_k.T.cpu().numpy(), sigma_k * v.unsqueeze(0).T.cpu().numpy()), np.dot(curr_Y_k.T.cpu().numpy(), v.unsqueeze(0).T.cpu().numpy())), axis=0)\n",
    "    approx_prod -= np.dot(np.dot(np.concatenate((sigma_k * curr_S_k.cpu().numpy(), curr_Y_k.cpu().numpy()), axis=1), mat_inv), p_mat)\n",
    "\n",
    "    return approx_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(20):\n",
    "        # apply attack to compromised worker devices with randomness\n",
    "        ##random_12 = 1. + np.random.uniform(size=vi_shape)\n",
    "        ##random_12 = torch.Tensor(random_12).float().cuda()\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc5c00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_mean(old_gradients, user_grads, b=0, hvp=None):\n",
    "#     print(type(old_gradients))\n",
    "#     print(type(user_grads))\n",
    "#     print(type(hvp))\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        print(type(pred_grad))\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "#             pred_grad.append(old_gradients[i].cpu().numpy() + hvp)\n",
    "            pred_grad[i] += hvp\n",
    "        \n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(old_gradients - user_grads, dim = 1).cpu().numpy()\n",
    "        auc1 = roc_auc_score(pred, distance)\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        auc2 = roc_auc_score(pred, distance)\n",
    "        print(\"Detection AUC: %0.4f; Detection AUC: %0.4f\" % (auc1, auc2))\n",
    "        \n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    agg_grads = torch.mean(user_grads,dim=0)\n",
    "    \n",
    "    return agg_grads, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcb12fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trimmed_mean(old_gradients, user_grads, b=0, hvp=None):\n",
    "# #     print(type(old_gradients))\n",
    "# #     print(type(user_grads))\n",
    "# #     print(type(hvp))\n",
    "#     if hvp is not None:\n",
    "#         hvp = torch.from_numpy(hvp).to(device)\n",
    "#         pred_grad = copy.deepcopy(old_gradients)\n",
    "#         print(type(pred_grad))\n",
    "#         distance = []\n",
    "#         for i in range(len(old_gradients)):\n",
    "#             pred_grad[i] += hvp\n",
    "#         pred = np.zeros(100)\n",
    "#         pred[:b] = 1\n",
    "#         distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "#         auc2 = roc_auc_score(pred, distance)\n",
    "#         print(\"Detection AUC: %0.4f; Detection AUC: %0.4f\" % (auc1, auc2))\n",
    "        \n",
    "#         distance = distance / np.sum(distance)\n",
    "#     else:\n",
    "#         distance = None\n",
    "    \n",
    "#     agg_grads = tr_mean(user_grads, 20)\n",
    "    \n",
    "#     return agg_grads, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72fe92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimmed_mean(old_gradients, user_grads, b=0, hvp=None):\n",
    "#     print(type(old_gradients))\n",
    "#     print(type(user_grads))\n",
    "#     print(type(hvp))\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "#         print(type(pred_grad))\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "#             pred_grad.append(old_gradients[i].cpu().numpy() + hvp)\n",
    "            pred_grad[i] += hvp\n",
    "        \n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(old_gradients - user_grads, dim = 1).cpu().numpy()\n",
    "#         auc1 = roc_auc_score(pred, distance)\n",
    "        auc1 = 0\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        auc2 = roc_auc_score(pred, distance)\n",
    "        print(\"Detection AUC: %0.4f; Detection AUC: %0.4f\" % (auc1, auc2))\n",
    "        \n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    agg_grads = tr_mean(user_grads, 20)\n",
    "    \n",
    "    return agg_grads, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67adc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(score, nobyz):\n",
    "    estimator = KMeans(n_clusters=2)\n",
    "    estimator.fit(score.reshape(-1, 1))\n",
    "    label_pred = estimator.labels_\n",
    "    if np.mean(score[label_pred==0])<np.mean(score[label_pred==1]):\n",
    "        #0 is the label of malicious clients\n",
    "        label_pred = 1 - label_pred\n",
    "    real_label=np.ones(100)\n",
    "    real_label[:nobyz]=0\n",
    "    acc=len(label_pred[label_pred==real_label])/100\n",
    "    recall=1-np.sum(label_pred[:nobyz])/nobyz\n",
    "    fpr=1-np.sum(label_pred[nobyz:])/(100-nobyz)\n",
    "    fnr=np.sum(label_pred[:nobyz])/nobyz\n",
    "    print(\"acc %0.4f; recall %0.4f; fpr %0.4f; fnr %0.4f;\" % (acc, recall, fpr, fnr))\n",
    "    print(silhouette_score(score.reshape(-1, 1), label_pred))\n",
    "\n",
    "def detection1(score, nobyz):\n",
    "    nrefs = 10\n",
    "    ks = range(1, 8)\n",
    "    gaps = np.zeros(len(ks))\n",
    "    gapDiff = np.zeros(len(ks) - 1)\n",
    "    sdk = np.zeros(len(ks))\n",
    "    min = np.min(score)\n",
    "    max = np.max(score)\n",
    "    score = (score - min)/(max-min)\n",
    "    for i, k in enumerate(ks):\n",
    "        estimator = KMeans(n_clusters=k)\n",
    "        estimator.fit(score.reshape(-1, 1))\n",
    "        label_pred = estimator.labels_\n",
    "        center = estimator.cluster_centers_\n",
    "        Wk = np.sum([np.square(score[m]-center[label_pred[m]]) for m in range(len(score))])\n",
    "        WkRef = np.zeros(nrefs)\n",
    "        for j in range(nrefs):\n",
    "            rand = np.random.uniform(0, 1, len(score))\n",
    "            estimator = KMeans(n_clusters=k)\n",
    "            estimator.fit(rand.reshape(-1, 1))\n",
    "            label_pred = estimator.labels_\n",
    "            center = estimator.cluster_centers_\n",
    "            WkRef[j] = np.sum([np.square(rand[m]-center[label_pred[m]]) for m in range(len(rand))])\n",
    "        gaps[i] = np.log(np.mean(WkRef)) - np.log(Wk)\n",
    "        sdk[i] = np.sqrt((1.0 + nrefs) / nrefs) * np.std(np.log(WkRef))\n",
    "\n",
    "        if i > 0:\n",
    "            gapDiff[i - 1] = gaps[i - 1] - gaps[i] + sdk[i]\n",
    "    #print(gapDiff)\n",
    "    for i in range(len(gapDiff)):\n",
    "        if gapDiff[i] >= 0:\n",
    "            select_k = i+1\n",
    "            break\n",
    "    if select_k == 1:\n",
    "        print('No attack detected!')\n",
    "        return 0\n",
    "    else:\n",
    "        print('Attack Detected!')\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72ced92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = nworkers\n",
    "lr = lr\n",
    "epochs = nepochs\n",
    "grad_list = []\n",
    "old_grad_list = []\n",
    "weight_record = []\n",
    "grad_record = []\n",
    "train_acc_list = []\n",
    "distance1 = []\n",
    "distance2 = []\n",
    "auc_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dea71d3",
   "metadata": {},
   "source": [
    "# Adaptive attack against FLDetector based on simple noise addition\n",
    "\n",
    "## Method: I added noise in the *little-is-enough attack fashion*\n",
    "\n",
    "## Observation: FLdetector does not detect the attack and the model does not train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a33ec087",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0781\n",
      "1 0.131\n",
      "2 0.1294\n",
      "3 0.1245\n",
      "4 0.1201\n",
      "5 0.1183\n",
      "6 0.1185\n",
      "7 0.1277\n",
      "8 0.1417\n",
      "9 0.1661\n",
      "10 0.1706\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.0000\n",
      "==> 11 (1, 100)\n",
      "11 0.1305\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 0.8875; Detection AUC: 0.0000\n",
      "==> 12 (2, 100)\n",
      "12 0.1025\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.0000\n",
      "==> 13 (3, 100)\n",
      "13 0.1009\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.9875\n",
      "==> 14 (4, 100)\n",
      "14 0.1815\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.0000\n",
      "==> 15 (5, 100)\n",
      "15 0.1009\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.1250\n",
      "==> 16 (6, 100)\n",
      "16 0.1028\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.0000\n",
      "==> 17 (7, 100)\n",
      "17 0.1009\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.0000\n",
      "==> 18 (8, 100)\n",
      "18 0.1009\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 0.4375; Detection AUC: 0.4375\n",
      "==> 19 (9, 100)\n",
      "19 0.1028\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.0000\n",
      "==> 20 (10, 100)\n",
      "performing detection at epoch 20\n",
      "Attack Detected!\n",
      "Stop at iteration: 20\n",
      "acc 0.0000; recall 0.0000; fpr 1.0000; fnr 1.0000;\n",
      "0.9524335694452621\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "weight_record = []\n",
    "grad_record = []\n",
    "test_grads = []\n",
    "malicious_scores = np.zeros((1, num_workers))\n",
    "attack_type = 'LIE'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = cnn().to(device)\n",
    "lr = 0.15\n",
    "start_detection_epoch = 10\n",
    "window_size = 10\n",
    "assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "for e in range(100):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr*(0.96**e))\n",
    "    user_grads = []\n",
    "\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, each_worker_label[i].cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "    tmp = []\n",
    "    for param in net.parameters():\n",
    "        tmp = param.data.view(-1) if not len(tmp) else torch.cat((tmp, param.data.view(-1)))\n",
    "    weight = tmp    \n",
    "\n",
    "    if (e > start_detection_epoch):\n",
    "        hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "        hvp = np.squeeze(hvp)\n",
    "    else:\n",
    "        hvp = None\n",
    "\n",
    "    if(e <= start_detection_epoch):\n",
    "        user_grads = full_trim(user_grads, 20)\n",
    "    elif e >= start_detection_epoch:\n",
    "        if attack_type == 'LIE':\n",
    "            print(\"LIE\")\n",
    "            z = 0.2\n",
    "            noise_avg = torch.mean(user_grads[:20], dim=0)\n",
    "            noise_std = torch.std(user_grads[:20], dim=0)\n",
    "            noise = noise_avg + z * noise_std\n",
    "        elif attack_type == 'NDSS21':\n",
    "            pass\n",
    "        else:\n",
    "            noise = torch.zeros(hvp.shape).to(device)\n",
    "        for m in range(20):\n",
    "            user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "#             user_grads[m] = torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "    agg_grads, distance = simple_mean(old_grad_list, user_grads, 20, hvp)\n",
    "#     agg_grads, distance = trimmed_mean(old_grad_list, user_grads, 20, hvp)\n",
    "    \n",
    "    if distance is not None and e > (start_detection_epoch - window_size):\n",
    "        print('==>', e, malicious_scores.shape)\n",
    "        malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "    if malicious_scores.shape[0] >= 11:\n",
    "        print('performing detection at epoch %d' % e)\n",
    "        if detection1(np.sum(malicious_scores[-10:], axis=0), 20):\n",
    "            print('Stop at iteration:', e)\n",
    "            detection(np.sum(malicious_scores[-10:], axis=0), 20)\n",
    "            break\n",
    "\n",
    "#     agg_grads = tr_mean(user_grads, 20)\n",
    "#     agg_grads=torch.median(user_grads,dim=0)[0]\n",
    "#     agg_grads=torch.mean(user_grads,dim=0)\n",
    "\n",
    "    if e > (start_detection_epoch - window_size):\n",
    "        weight_record.append(weight - last_weight)\n",
    "        grad_record.append(agg_grads - last_grad)\n",
    "    \n",
    "    if (len(weight_record) > 10):\n",
    "        del weight_record[0]\n",
    "        del grad_record[0]\n",
    "    \n",
    "    last_weight = weight\n",
    "    last_grad = agg_grads\n",
    "    old_grad_list = user_grads\n",
    "  \n",
    "    del user_grads\n",
    "    \n",
    "    start_idx=0\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "\n",
    "    model_grads=[]\n",
    "\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_data):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(e,correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc193f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10230177 0.10230177 0.10230177 0.10230177 0.10230177 0.10230177\n",
      " 0.10230177 0.10230177 0.10230177 0.10230177 0.10230177 0.10230177\n",
      " 0.10230177 0.10230177 0.10230177 0.10230177 0.10230177 0.10230177\n",
      " 0.10230177 0.10230177 0.12268023 0.09107435 0.0904362  0.12058906\n",
      " 0.09304538 0.09544147 0.09099482 0.10089649 0.08338294 0.10277701\n",
      " 0.10962427 0.098361   0.1174554  0.10538476 0.08873345 0.08587732\n",
      " 0.12129823 0.09377966 0.09805892 0.0900425  0.11687774 0.08813501\n",
      " 0.09805897 0.09579742 0.10005758 0.08456191 0.09155605 0.0872863\n",
      " 0.12900027 0.07977492 0.10466884 0.08922078 0.09769056 0.09269846\n",
      " 0.09178972 0.08882053 0.09436927 0.09118059 0.09165551 0.08413379\n",
      " 0.10515576 0.13925978 0.09470196 0.11183365 0.08872796 0.10218131\n",
      " 0.08600956 0.13773219 0.08853835 0.10910166 0.19727379 0.09124112\n",
      " 0.10438507 0.08669383 0.09617093 0.09831951 0.09251176 0.08511794\n",
      " 0.08690292 0.09194242 0.08892102 0.10986411 0.09598063 0.08636983\n",
      " 0.09267481 0.08669738 0.08972959 0.13679126 0.09577854 0.09539023\n",
      " 0.09664694 0.08973664 0.09750391 0.0880986  0.08777319 0.08693693\n",
      " 0.08798716 0.0856467  0.10180355 0.16256445]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(malicious_scores[3:13], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20c888ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06754897 0.06754897 0.06754897 0.06754897 0.06754897 0.06754897\n",
      " 0.06754897 0.06754897 0.06754897 0.06754897 0.06754897 0.06754897\n",
      " 0.06754897 0.06754897 0.06754897 0.06754897 0.06754897 0.06754897\n",
      " 0.06754897 0.06754897 0.11583674 0.1061734  0.10541703 0.11530078\n",
      " 0.10209913 0.10928    0.10556562 0.11058844 0.10044197 0.10715503\n",
      " 0.11391703 0.10973143 0.11228188 0.10602802 0.10419031 0.10232675\n",
      " 0.12208815 0.10758785 0.10897822 0.10309705 0.11492164 0.10489739\n",
      " 0.11273732 0.10740756 0.10448257 0.10129105 0.10774605 0.10504647\n",
      " 0.11946426 0.09804636 0.11199597 0.10233566 0.1077155  0.10564686\n",
      " 0.10346032 0.10353452 0.10674702 0.10488126 0.1059127  0.10110537\n",
      " 0.11332872 0.1230536  0.10263512 0.11062256 0.10143249 0.10865935\n",
      " 0.10396482 0.12381874 0.10215213 0.11194872 0.15680232 0.10222957\n",
      " 0.10729898 0.10065074 0.10894018 0.10914024 0.10731661 0.10158781\n",
      " 0.1032871  0.10579673 0.10319049 0.11049344 0.10732267 0.10372362\n",
      " 0.10706483 0.102365   0.1065529  0.12281318 0.10555815 0.1062698\n",
      " 0.10417917 0.10558218 0.11030034 0.10070927 0.1006251  0.09910868\n",
      " 0.10435435 0.10371689 0.10561165 0.13735164]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(malicious_scores[-10:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adab81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
