{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for CIFAR10 with Fang distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e300ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu036/4396557/ipykernel_1706373/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "import copy\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77aa30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_models import *\n",
    "from cifar10_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "\n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fang_train_data(trainset, num_workers=100, bias=0.5, force=False):\n",
    "    dist_file = 'fang_nworkers%d_bias%.1f.pkl' % (num_workers, bias)\n",
    "    if not force and os.path.exists(dist_file):\n",
    "        print('Loading fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "        return pickle.load(open(dist_file, 'rb'))\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    per_participant_list = defaultdict(list)\n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        per_participant_list[selected_worker].extend([i])\n",
    "    \n",
    "    print('Saving fang distribution for num_workers %d and bias %.1f to memory' % (num_workers, bias))\n",
    "    pickle.dump(per_participant_list, open(dist_file, 'wb'))\n",
    "    return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799ffb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_federated_data(trainset, num_workers, distribution='fang', param=1, force=False):\n",
    "    if distribution == 'fang':\n",
    "        per_participant_list = get_fang_train_data(trainset, num_workers, bias=param, force=force)\n",
    "    elif distribution == 'dirichlet':\n",
    "        per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=param, force=force)\n",
    "\n",
    "    each_worker_idx = [[] for _ in range(num_workers)] \n",
    "    each_worker_te_idx = [[] for _ in range(num_workers)]\n",
    "    np.random.seed(0)\n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(5*w_len/6)\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        each_worker_idx[worker_idx] = w_indices[tr_idx]\n",
    "        each_worker_te_idx[worker_idx] = w_indices[te_idx]\n",
    "    global_test_idx = np.concatenate(each_worker_te_idx)\n",
    "    return each_worker_idx, each_worker_te_idx, global_test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2590db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(dataset, indices, batch_size=32, shuffle=False):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acbb06d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/home/vshejwalkar_umass_edu/data/'\n",
    "# load the train dataset\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_train)\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_train)\n",
    "\n",
    "te_cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_test)\n",
    "te_cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.sum(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(trainloader, model, model_received, criterion, optimizer, pgd=False, eps=2):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size()[0])\n",
    "        top1.update(prec1.item()/100.0, inputs.size()[0])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if pgd:\n",
    "            curr_model = list(model.parameters())\n",
    "            curr_model_vec = parameters_to_vector(curr_model)\n",
    "            if torch.norm(curr_model_vec - model_received) > eps:\n",
    "                curr_model_vec = eps*(curr_model_vec - model_received)/torch.norm(curr_model_vec - model_received) + model_received\n",
    "                vector_to_parameters(curr_model_vec, curr_model)\n",
    "        \n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def test(testloader, model, criterion):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(testloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        top1.update(prec1/100.0, inputs.size()[0])\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8b7ed",
   "metadata": {},
   "source": [
    "# resnet-BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5204c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e25e2ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fang distribution for num_workers 100 and bias 0.5 from memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 100, 10038)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "batch_size = 32\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = False\n",
    "\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=100, distribution=distribution, param=param, force=force)\n",
    "\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    train_loaders.append((pos, get_train(all_data, indices, batch_size)))\n",
    "test_loaders = []\n",
    "for pos, indices in enumerate(each_worker_te_idx):\n",
    "    test_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "len(train_loaders), len(test_loaders), len(global_test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "963baed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loaders[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ab30849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62611dc3",
   "metadata": {},
   "source": [
    "# Mean with FedNOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed6f34",
   "metadata": {},
   "source": [
    "# Attack Resnet20BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fc6c121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data len:  49962\n",
      "client_data_len shape:  (100,)\n",
      "tau_per_client shape:  (100,)\n",
      "client_ws shape:  (100,)\n",
      "len tau_per_client 100 client_data_len 100\n",
      "len client_ws 100 renormalized_client_ws 100\n",
      "e 0 benign_norm 0.000 val loss 2.391 val acc 0.101 best val_acc 0.101\n",
      "e 10 benign_norm 0.000 val loss 1.812 val acc 0.301 best val_acc 0.301\n",
      "e 20 benign_norm 0.000 val loss 1.608 val acc 0.398 best val_acc 0.398\n",
      "e 30 benign_norm 0.000 val loss 1.527 val acc 0.439 best val_acc 0.441\n",
      "e 40 benign_norm 0.000 val loss 1.525 val acc 0.451 best val_acc 0.457\n",
      "e 50 benign_norm 0.000 val loss 1.582 val acc 0.436 best val_acc 0.467\n",
      "e 60 benign_norm 0.000 val loss 1.464 val acc 0.462 best val_acc 0.469\n",
      "e 70 benign_norm 0.000 val loss 1.390 val acc 0.493 best val_acc 0.493\n",
      "e 80 benign_norm 0.000 val loss 1.406 val acc 0.501 best val_acc 0.507\n",
      "e 90 benign_norm 0.000 val loss 1.397 val acc 0.501 best val_acc 0.512\n",
      "e 100 benign_norm 0.000 val loss 1.368 val acc 0.507 best val_acc 0.527\n",
      "e 110 benign_norm 0.000 val loss 1.443 val acc 0.490 best val_acc 0.528\n",
      "e 120 benign_norm 0.000 val loss 1.383 val acc 0.512 best val_acc 0.528\n",
      "e 130 benign_norm 0.000 val loss 1.259 val acc 0.555 best val_acc 0.555\n",
      "e 140 benign_norm 0.000 val loss 1.215 val acc 0.576 best val_acc 0.576\n",
      "e 160 benign_norm 0.000 val loss 1.280 val acc 0.556 best val_acc 0.577\n",
      "e 170 benign_norm 0.000 val loss 1.301 val acc 0.551 best val_acc 0.577\n",
      "e 180 benign_norm 0.000 val loss 1.419 val acc 0.529 best val_acc 0.577\n",
      "e 190 benign_norm 0.000 val loss 1.370 val acc 0.533 best val_acc 0.577\n",
      "e 199 benign_norm 0.000 val loss 1.509 val acc 0.495 best val_acc 0.577\n",
      "e 200 benign_norm 0.000 val loss 1.534 val acc 0.491 best val_acc 0.577\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 32\n",
    "num_workers = 100\n",
    "local_lr = 0.1\n",
    "global_lr = 1\n",
    "nbyz = 20\n",
    "total_data_len = 0\n",
    "client_data_len = []\n",
    "tau_per_client = []\n",
    "\n",
    "for client_idx, worker_indices in enumerate(each_worker_idx[20-nbyz:]):\n",
    "    if client_idx < nbyz:\n",
    "        total_data_len += len(worker_indices)\n",
    "        client_data_len.append(len(worker_indices))\n",
    "        tau_per_client.append(len(train_loaders[client_idx][1]))\n",
    "    else:\n",
    "        total_data_len += len(worker_indices)\n",
    "        client_data_len.append(len(worker_indices))\n",
    "        tau_per_client.append(len(train_loaders[client_idx][1]))\n",
    "client_data_len = np.array(client_data_len)\n",
    "tau_per_client = np.array(tau_per_client)\n",
    "client_ws = client_data_len / total_data_len\n",
    "\n",
    "print('total data len: ', total_data_len)\n",
    "print('client_data_len shape: ', client_data_len.shape)\n",
    "print('tau_per_client shape: ', tau_per_client.shape)\n",
    "print('client_ws shape: ', client_ws.shape)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(20-nbyz, num_workers)\n",
    "    round_benign = round_clients[nbyz:]\n",
    "    round_malicious = round_clients[:nbyz]\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    \n",
    "    for client_idx in round_malicious:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[client_idx][1], model, model_received, criterion, optimizer)\n",
    "        params = []\n",
    "        for _, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "        update =  (params - model_received)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    for client_idx in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[client_idx][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for _, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    if epoch_num == 0:\n",
    "        renormalized_client_weights = client_ws * np.sum(client_ws * tau_per_client) / tau_per_client\n",
    "        print('len tau_per_client %d client_data_len %d' % (len(tau_per_client), len(client_data_len)))\n",
    "        print('len client_ws %d renormalized_client_ws %d' % (len(client_ws), len(renormalized_client_weights)))\n",
    "\n",
    "    user_updates *= torch.from_numpy(renormalized_client_weights)[:, None].to(device)\n",
    "    user_updates[:nbyz] = full_trim(user_updates[:nbyz], nbyz)\n",
    "    agg_update = tr_mean(user_updates, nbyz)\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * (.999**epoch_num) * agg_update\n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    \n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "    \n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "543e43d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data len:  49962\n",
      "client_data_len shape:  (100,)\n",
      "tau_per_client shape:  (100,)\n",
      "client_ws shape:  (100,)\n",
      "e 10 benign_norm 0.000 val loss 1.850 val acc 0.283 best val_acc 0.283\n",
      "e 20 benign_norm 0.000 val loss 1.653 val acc 0.388 best val_acc 0.392\n",
      "e 30 benign_norm 0.000 val loss 1.750 val acc 0.371 best val_acc 0.413\n",
      "e 40 benign_norm 0.000 val loss 1.797 val acc 0.350 best val_acc 0.413\n",
      "e 50 benign_norm 0.000 val loss 1.663 val acc 0.397 best val_acc 0.414\n",
      "e 60 benign_norm 0.000 val loss 1.643 val acc 0.443 best val_acc 0.455\n",
      "e 70 benign_norm 0.000 val loss 1.460 val acc 0.486 best val_acc 0.486\n",
      "e 80 benign_norm 0.000 val loss 1.343 val acc 0.529 best val_acc 0.529\n",
      "e 90 benign_norm 0.000 val loss 1.315 val acc 0.533 best val_acc 0.554\n",
      "e 100 benign_norm 0.000 val loss 1.267 val acc 0.555 best val_acc 0.560\n",
      "e 110 benign_norm 0.000 val loss 1.267 val acc 0.550 best val_acc 0.560\n",
      "e 120 benign_norm 0.000 val loss 1.210 val acc 0.567 best val_acc 0.567\n",
      "e 130 benign_norm 0.000 val loss 1.500 val acc 0.467 best val_acc 0.567\n",
      "e 140 benign_norm 0.000 val loss 1.363 val acc 0.537 best val_acc 0.567\n",
      "e 150 benign_norm 0.000 val loss 1.305 val acc 0.546 best val_acc 0.567\n",
      "e 160 benign_norm 0.000 val loss 1.327 val acc 0.553 best val_acc 0.567\n",
      "e 170 benign_norm 0.000 val loss 1.457 val acc 0.521 best val_acc 0.567\n",
      "e 180 benign_norm 0.000 val loss 1.374 val acc 0.544 best val_acc 0.567\n",
      "e 190 benign_norm 0.000 val loss 1.318 val acc 0.546 best val_acc 0.567\n",
      "e 199 benign_norm 0.000 val loss 1.392 val acc 0.539 best val_acc 0.567\n",
      "e 200 benign_norm 0.000 val loss 1.383 val acc 0.527 best val_acc 0.567\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 32\n",
    "num_workers = 100\n",
    "local_lr = 0.1\n",
    "global_lr = 1\n",
    "nbyz = 20\n",
    "total_data_len = 0\n",
    "client_data_len = []\n",
    "tau_per_client = []\n",
    "\n",
    "for client_idx, worker_indices in enumerate(each_worker_idx[20-nbyz:]):\n",
    "    if client_idx < nbyz:\n",
    "        total_data_len += len(worker_indices)\n",
    "        client_data_len.append(len(worker_indices))\n",
    "        tau_per_client.append(len(train_loaders[client_idx][1]))\n",
    "    else:\n",
    "        total_data_len += len(worker_indices)\n",
    "        client_data_len.append(len(worker_indices))\n",
    "        tau_per_client.append(len(train_loaders[client_idx][1]))\n",
    "client_data_len = np.array(client_data_len)\n",
    "tau_per_client = np.array(tau_per_client)\n",
    "client_ws = client_data_len / total_data_len\n",
    "\n",
    "print('total data len: ', total_data_len)\n",
    "print('client_data_len shape: ', client_data_len.shape)\n",
    "print('tau_per_client shape: ', tau_per_client.shape)\n",
    "print('client_ws shape: ', client_ws.shape)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(20-nbyz, num_workers)\n",
    "    round_benign = round_clients[nbyz:]\n",
    "    round_malicious = round_clients[:nbyz]\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    \n",
    "    for client_idx in round_malicious:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[client_idx][1], model, model_received, criterion, optimizer)\n",
    "        params = []\n",
    "        for _, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "        update =  (params - model_received)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    for client_idx in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[client_idx][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for _, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    if epoch_num == 0:\n",
    "        renormalized_client_weights = client_ws * np.sum(client_ws * tau_per_client) / tau_per_client\n",
    "        print('len tau_per_client %d client_data_len %d' % (len(tau_per_client), len(client_data_len)))\n",
    "        print('len client_ws %d renormalized_client_ws %d' % (len(client_ws), len(renormalized_client_weights)))\n",
    "\n",
    "    user_updates *= torch.from_numpy(renormalized_client_weights)[:, None].to(device)\n",
    "    user_updates[:nbyz] = full_trim(user_updates[:nbyz], nbyz)\n",
    "    agg_update = tr_mean(user_updates, nbyz)\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * (.999**epoch_num) * agg_update\n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    \n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "    \n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc0bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 32\n",
    "num_workers = 100\n",
    "local_lr = 0.1\n",
    "global_lr = 1\n",
    "nbyz = 20\n",
    "total_data_len = 0\n",
    "client_data_len = []\n",
    "tau_per_client = []\n",
    "\n",
    "for client_idx, worker_indices in enumerate(each_worker_idx[20-nbyz:]):\n",
    "    if client_idx < nbyz:\n",
    "        total_data_len += len(worker_indices)\n",
    "        client_data_len.append(len(worker_indices))\n",
    "        tau_per_client.append(len(train_loaders[client_idx][1]))\n",
    "    else:\n",
    "        total_data_len += len(worker_indices)\n",
    "        client_data_len.append(len(worker_indices))\n",
    "        tau_per_client.append(len(train_loaders[client_idx][1]))\n",
    "client_data_len = np.array(client_data_len)\n",
    "tau_per_client = np.array(tau_per_client)\n",
    "client_ws = client_data_len / total_data_len\n",
    "\n",
    "print('total data len: ', total_data_len)\n",
    "print('client_data_len shape: ', client_data_len.shape)\n",
    "print('tau_per_client shape: ', tau_per_client.shape)\n",
    "print('client_ws shape: ', client_ws.shape)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_global_acc = best_ema_acc = 0\n",
    "epoch_num = 0\n",
    "ema_decay = 0.95\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "ema_model_received = copy.deepcopy(model_received)\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(20-nbyz, num_workers)\n",
    "    round_benign = round_clients[nbyz:]\n",
    "    round_malicious = round_clients[:nbyz]\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    \n",
    "    for client_idx in round_malicious:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[client_idx][1], model, model_received, criterion, optimizer)\n",
    "        params = []\n",
    "        for _, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "        update =  (params - model_received)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    for client_idx in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[client_idx][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for _, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    if epoch_num == 0:\n",
    "        renormalized_client_weights = client_ws * np.sum(client_ws * tau_per_client) / tau_per_client\n",
    "        print('len tau_per_client %d client_data_len %d' % (len(tau_per_client), len(client_data_len)))\n",
    "        print('len client_ws %d renormalized_client_ws %d' % (len(client_ws), len(renormalized_client_weights)))\n",
    "\n",
    "    user_updates *= torch.from_numpy(renormalized_client_weights)[:, None].to(device)\n",
    "    user_updates[:nbyz] = full_trim(user_updates[:nbyz], nbyz)\n",
    "    agg_update = tr_mean(user_updates, nbyz)\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * (.999**epoch_num) * agg_update\n",
    "    round_ema_decay = min(ema_decay, (1+epoch_num)/(10+epoch_num))\n",
    "    ema_model_received = ema_model_received * round_ema_decay + model_received * (1 - round_ema_decay)\n",
    "    \n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for _, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "\n",
    "    ema_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for _, (name, _) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = ema_model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    ema_model.load_state_dict(state_dict)\n",
    "    \n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    ema_loss, ema_acc = test(cifar10_test_loader, ema_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    \n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    best_ema_acc = max(best_ema_acc, ema_acc)\n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "    \n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "579a03b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 benign_norm 782.413 val loss 2.310 val acc 0.098 best val_acc 0.098\n",
      "e 10 benign_norm 277.659 val loss 1.768 val acc 0.346 best val_acc 0.346\n",
      "e 20 benign_norm 277.721 val loss 1.667 val acc 0.409 best val_acc 0.413\n",
      "e 30 benign_norm 278.085 val loss 1.646 val acc 0.419 best val_acc 0.438\n",
      "e 40 benign_norm 278.773 val loss 1.695 val acc 0.429 best val_acc 0.451\n",
      "e 50 benign_norm 279.670 val loss 1.637 val acc 0.478 best val_acc 0.478\n",
      "e 60 benign_norm 281.176 val loss 1.695 val acc 0.474 best val_acc 0.483\n",
      "e 70 benign_norm 282.138 val loss 1.683 val acc 0.482 best val_acc 0.492\n",
      "e 80 benign_norm 283.360 val loss 1.581 val acc 0.516 best val_acc 0.516\n",
      "e 90 benign_norm 286.147 val loss 1.540 val acc 0.504 best val_acc 0.523\n",
      "e 100 benign_norm 291.872 val loss 1.792 val acc 0.498 best val_acc 0.533\n",
      "e 110 benign_norm 293.738 val loss 1.615 val acc 0.524 best val_acc 0.533\n",
      "e 120 benign_norm 297.880 val loss 1.694 val acc 0.519 best val_acc 0.533\n",
      "e 130 benign_norm 306.630 val loss 1.681 val acc 0.525 best val_acc 0.541\n",
      "e 140 benign_norm 307.089 val loss 1.799 val acc 0.501 best val_acc 0.541\n",
      "e 150 benign_norm 334.480 val loss 1.974 val acc 0.514 best val_acc 0.541\n",
      "e 160 benign_norm 319.865 val loss 1.550 val acc 0.516 best val_acc 0.541\n",
      "e 170 benign_norm 395.673 val loss 2.136 val acc 0.507 best val_acc 0.541\n",
      "e 180 benign_norm 337.501 val loss 1.762 val acc 0.501 best val_acc 0.541\n",
      "e 190 benign_norm 370.490 val loss 2.050 val acc 0.496 best val_acc 0.541\n",
      "e 199 benign_norm 346.419 val loss 2.020 val acc 0.485 best val_acc 0.541\n",
      "e 200 benign_norm 344.746 val loss 1.911 val acc 0.501 best val_acc 0.541\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m     client_loss, client_acc \u001b[38;5;241m=\u001b[39m test(test_loaders[i][\u001b[38;5;241m1\u001b[39m], fed_model, criterion)\n\u001b[1;32m     80\u001b[0m     final_accs_per_client\u001b[38;5;241m.\u001b[39mappend(client_acc)\n\u001b[1;32m     81\u001b[0m results \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mOrderedDict(\n\u001b[0;32m---> 82\u001b[0m     final_accs_per_client\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_accs_per_client\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     83\u001b[0m     accs_per_round\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(accs_per_round),\n\u001b[1;32m     84\u001b[0m     best_accs_per_round\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(best_accs_per_round),\n\u001b[1;32m     85\u001b[0m     loss_per_round\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(loss_per_round)\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump(results, \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(home_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFLDetector_plots_data/rq1_baseline_cifar10_fast_trmean_trim20_resnet20BN.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torch/_tensor.py:757\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.1\n",
    "global_lr = 1\n",
    "nbyz = 20\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    user_updates[:nbyz] = full_trim(user_updates[:nbyz], nbyz)\n",
    "    agg_update = tr_mean(user_updates, nbyz)\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    \n",
    "    best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "    accs_per_round.append(val_acc.cpu().item())\n",
    "    loss_per_round.append(val_loss.cpu().item())\n",
    "    \n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "    epoch_num+=1\n",
    "    \n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    client_loss, client_acc = test(test_loaders[i][1], fed_model, criterion)\n",
    "    final_accs_per_client.append(client_acc.cpu().item())\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    "    loss_per_round=np.array(loss_per_round)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_cifar10_fast_trmean_trim20_resnet20BN.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a42d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
