{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for Fashion MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821b1014",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu066/6674368/ipykernel_2914930/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a52c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "            \n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_train_data(trainset, num_workers=100, bias=0.5):\n",
    "    np.random.seed(42)\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        \n",
    "        if not len(each_worker_data[selected_worker]):\n",
    "            each_worker_data[selected_worker] = x[None, :]\n",
    "        else:\n",
    "            each_worker_data[selected_worker]= torch.concat((each_worker_data[selected_worker], x[None, :]))\n",
    "        \n",
    "        each_worker_label[selected_worker].append(y)\n",
    "    \n",
    "    each_worker_tr_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_tr_label = [[] for _ in range(num_workers)]\n",
    "    each_worker_val_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_val_label = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "\n",
    "    for i in range(num_workers):\n",
    "        w_len = len(each_worker_data[i])\n",
    "        len_tr = int(5 * w_len / 7)\n",
    "        len_val = int(1 * w_len / 7)\n",
    "        len_te = w_len - (len_tr + len_val)\n",
    "        # tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        # te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        w = np.arange(w_len)\n",
    "        np.random.shuffle(w)\n",
    "        tr_idx, val_idx, te_idx = w[:len_tr], w[len_tr : (len_tr + len_val)], w[-len_te:]\n",
    "        each_worker_tr_data[i] = each_worker_data[i][tr_idx]\n",
    "        each_worker_tr_label[i] = torch.Tensor(each_worker_label[i])[tr_idx]\n",
    "        each_worker_val_data[i] = each_worker_data[i][val_idx]\n",
    "        each_worker_val_label[i] = torch.Tensor(each_worker_label[i])[val_idx]\n",
    "        each_worker_te_data[i] = each_worker_data[i][te_idx]\n",
    "        each_worker_te_label[i] = torch.Tensor(each_worker_label[i])[te_idx]\n",
    "        \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    del each_worker_data, each_worker_label\n",
    "    return each_worker_tr_data, each_worker_tr_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f36f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_data_dirichlet(trainset, num_workers, alpha=1, force=False):\n",
    "    per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=alpha, force=force)\n",
    "    \n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(6*w_len/7)\n",
    "        len_te = w_len - len_tr\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        \n",
    "        for idx in w_indices[tr_idx]:\n",
    "            each_worker_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_data[worker_idx] = torch.stack(each_worker_data[worker_idx])\n",
    "        each_worker_label[worker_idx] = torch.Tensor(each_worker_label[worker_idx]).long()\n",
    "        \n",
    "        for idx in w_indices[te_idx]:\n",
    "            each_worker_te_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_te_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_te_data[worker_idx] = torch.stack(each_worker_te_data[worker_idx])\n",
    "        each_worker_te_label[worker_idx] = torch.Tensor(each_worker_te_label[worker_idx]).long()\n",
    "    \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    \n",
    "    return each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6b5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 30, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(30, 50, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(1250, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "#         print(x.shape)\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae94ee06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266060"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cnn()\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb5ef3",
   "metadata": {},
   "source": [
    "# Good FedAvg baseline Fashion MNIST + Fang distribution + 80 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24164616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(train_data, labels, model, optimizer, batch_size=20):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    len_t = (len(train_data) // batch_size)\n",
    "    if len(train_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    r=np.arange(len(train_data))\n",
    "    np.random.shuffle(r)\n",
    "    \n",
    "    train_data = train_data[r]\n",
    "    labels = labels[r]\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "\n",
    "        inputs = train_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "        targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "\n",
    "def test(test_data, labels, model, criterion, use_cuda, debug_='MEDIUM', batch_size=64):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    len_t = (len(test_data) // batch_size)\n",
    "    if len(test_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ind in range(len_t):\n",
    "            # measure data loading time\n",
    "            inputs = test_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "            targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092fa012",
   "metadata": {},
   "source": [
    "# Mean without any attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55f2a272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Processing Dir 0.1 run 0\n",
      "generating participant indices for alpha 0.1\n",
      "Dir 0.1 r 0 e 0 val loss 2.071 val acc 30.404 best val_acc 30.404\n",
      "Dir 0.1 r 0 e 10 val loss 0.595 val acc 77.448 best val_acc 77.836\n",
      "Dir 0.1 r 0 e 20 val loss 0.489 val acc 82.026 best val_acc 82.026\n",
      "Dir 0.1 r 0 e 30 val loss 0.447 val acc 83.808 best val_acc 83.808\n",
      "Dir 0.1 r 0 e 40 val loss 0.416 val acc 84.952 best val_acc 84.952\n",
      "Dir 0.1 r 0 e 49 val loss 0.397 val acc 85.599 best val_acc 85.599\n",
      "Dir 0.1 r 0 e 50 val loss 0.397 val acc 85.629 best val_acc 85.629\n",
      "===> Processing Dir 0.1 run 1\n",
      "generating participant indices for alpha 0.1\n",
      "Dir 0.1 r 1 e 0 val loss 2.033 val acc 37.609 best val_acc 37.609\n",
      "Dir 0.1 r 1 e 10 val loss 0.609 val acc 76.533 best val_acc 76.533\n",
      "Dir 0.1 r 1 e 20 val loss 0.497 val acc 82.006 best val_acc 82.006\n",
      "Dir 0.1 r 1 e 40 val loss 0.415 val acc 84.813 best val_acc 84.902\n",
      "Dir 0.1 r 1 e 49 val loss 0.393 val acc 85.500 best val_acc 85.500\n",
      "Dir 0.1 r 1 e 50 val loss 0.392 val acc 85.649 best val_acc 85.649\n",
      "===> Processing Dir 0.1 run 2\n",
      "generating participant indices for alpha 0.1\n",
      "Dir 0.1 r 2 e 0 val loss 2.082 val acc 41.133 best val_acc 41.133\n",
      "Dir 0.1 r 2 e 10 val loss 0.579 val acc 78.782 best val_acc 78.782\n",
      "Dir 0.1 r 2 e 20 val loss 0.488 val acc 82.594 best val_acc 82.594\n",
      "Dir 0.1 r 2 e 30 val loss 0.443 val acc 83.937 best val_acc 83.937\n",
      "Dir 0.1 r 2 e 40 val loss 0.418 val acc 84.544 best val_acc 84.753\n",
      "Dir 0.1 r 2 e 49 val loss 0.401 val acc 85.470 best val_acc 85.510\n",
      "Dir 0.1 r 2 e 50 val loss 0.396 val acc 85.719 best val_acc 85.719\n",
      "===> Processing Dir 0.2 run 0\n",
      "generating participant indices for alpha 0.2\n",
      "Dir 0.2 r 0 e 0 val loss 1.828 val acc 51.524 best val_acc 51.524\n",
      "Dir 0.2 r 0 e 10 val loss 0.520 val acc 81.225 best val_acc 81.225\n",
      "Dir 0.2 r 0 e 20 val loss 0.435 val acc 84.432 best val_acc 84.432\n",
      "Dir 0.2 r 0 e 30 val loss 0.392 val acc 86.056 best val_acc 86.056\n",
      "Dir 0.2 r 0 e 40 val loss 0.365 val acc 86.833 best val_acc 86.853\n",
      "Dir 0.2 r 0 e 49 val loss 0.349 val acc 87.550 best val_acc 87.550\n",
      "Dir 0.2 r 0 e 50 val loss 0.348 val acc 87.669 best val_acc 87.669\n",
      "===> Processing Dir 0.2 run 1\n",
      "generating participant indices for alpha 0.2\n",
      "Dir 0.2 r 1 e 0 val loss 1.840 val acc 42.978 best val_acc 42.978\n",
      "Dir 0.2 r 1 e 10 val loss 0.517 val acc 80.707 best val_acc 80.707\n",
      "Dir 0.2 r 1 e 20 val loss 0.436 val acc 84.124 best val_acc 84.124\n",
      "Dir 0.2 r 1 e 30 val loss 0.397 val acc 85.926 best val_acc 85.926\n",
      "Dir 0.2 r 1 e 40 val loss 0.372 val acc 86.534 best val_acc 86.534\n",
      "Dir 0.2 r 1 e 49 val loss 0.356 val acc 87.012 best val_acc 87.012\n",
      "Dir 0.2 r 1 e 50 val loss 0.356 val acc 86.813 best val_acc 87.012\n",
      "===> Processing Dir 0.2 run 2\n",
      "generating participant indices for alpha 0.2\n",
      "Dir 0.2 r 2 e 0 val loss 1.760 val acc 57.251 best val_acc 57.251\n",
      "Dir 0.2 r 2 e 10 val loss 0.514 val acc 80.916 best val_acc 80.916\n",
      "Dir 0.2 r 2 e 30 val loss 0.405 val acc 85.518 best val_acc 85.518\n",
      "Dir 0.2 r 2 e 40 val loss 0.384 val acc 86.076 best val_acc 86.125\n",
      "Dir 0.2 r 2 e 49 val loss 0.367 val acc 86.614 best val_acc 86.653\n",
      "Dir 0.2 r 2 e 50 val loss 0.366 val acc 86.773 best val_acc 86.773\n",
      "===> Processing Dir 0.3 run 0\n",
      "generating participant indices for alpha 0.3\n",
      "Dir 0.3 r 0 e 0 val loss 1.759 val acc 48.830 best val_acc 48.830\n",
      "Dir 0.3 r 0 e 10 val loss 0.502 val acc 81.576 best val_acc 81.576\n",
      "Dir 0.3 r 0 e 20 val loss 0.435 val acc 84.235 best val_acc 84.235\n",
      "Dir 0.3 r 0 e 30 val loss 0.396 val acc 85.838 best val_acc 85.838\n",
      "Dir 0.3 r 0 e 40 val loss 0.369 val acc 86.714 best val_acc 86.744\n",
      "Dir 0.3 r 0 e 49 val loss 0.351 val acc 87.362 best val_acc 87.362\n",
      "Dir 0.3 r 0 e 50 val loss 0.350 val acc 87.332 best val_acc 87.362\n",
      "===> Processing Dir 0.3 run 1\n",
      "generating participant indices for alpha 0.3\n",
      "Dir 0.3 r 1 e 0 val loss 1.728 val acc 56.598 best val_acc 56.598\n",
      "Dir 0.3 r 1 e 10 val loss 0.484 val acc 81.765 best val_acc 81.765\n",
      "Dir 0.3 r 1 e 20 val loss 0.409 val acc 85.081 best val_acc 85.081\n",
      "Dir 0.3 r 1 e 30 val loss 0.372 val acc 86.246 best val_acc 86.246\n",
      "Dir 0.3 r 1 e 40 val loss 0.349 val acc 87.193 best val_acc 87.193\n",
      "Dir 0.3 r 1 e 49 val loss 0.335 val acc 87.720 best val_acc 87.730\n",
      "Dir 0.3 r 1 e 50 val loss 0.333 val acc 87.790 best val_acc 87.790\n",
      "===> Processing Dir 0.3 run 2\n",
      "generating participant indices for alpha 0.3\n",
      "Dir 0.3 r 2 e 0 val loss 1.515 val acc 61.388 best val_acc 61.388\n",
      "Dir 0.3 r 2 e 20 val loss 0.421 val acc 84.593 best val_acc 84.593\n",
      "Dir 0.3 r 2 e 30 val loss 0.378 val acc 86.246 best val_acc 86.246\n",
      "Dir 0.3 r 2 e 40 val loss 0.350 val acc 87.282 best val_acc 87.282\n",
      "Dir 0.3 r 2 e 49 val loss 0.334 val acc 87.870 best val_acc 87.870\n",
      "Dir 0.3 r 2 e 50 val loss 0.333 val acc 87.920 best val_acc 87.920\n",
      "===> Processing Dir 0.4 run 0\n",
      "generating participant indices for alpha 0.4\n",
      "Dir 0.4 r 0 e 0 val loss 1.580 val acc 61.627 best val_acc 61.627\n",
      "Dir 0.4 r 0 e 10 val loss 0.501 val acc 81.068 best val_acc 81.068\n",
      "Dir 0.4 r 0 e 20 val loss 0.421 val acc 84.513 best val_acc 84.513\n",
      "Dir 0.4 r 0 e 30 val loss 0.376 val acc 86.057 best val_acc 86.057\n",
      "Dir 0.4 r 0 e 40 val loss 0.350 val acc 87.133 best val_acc 87.133\n",
      "Dir 0.4 r 0 e 49 val loss 0.330 val acc 87.750 best val_acc 87.750\n",
      "Dir 0.4 r 0 e 50 val loss 0.328 val acc 87.870 best val_acc 87.870\n",
      "===> Processing Dir 0.4 run 1\n",
      "generating participant indices for alpha 0.4\n",
      "Dir 0.4 r 1 e 0 val loss 1.553 val acc 57.604 best val_acc 57.604\n",
      "Dir 0.4 r 1 e 10 val loss 0.480 val acc 82.362 best val_acc 82.362\n",
      "Dir 0.4 r 1 e 20 val loss 0.408 val acc 84.972 best val_acc 84.972\n",
      "Dir 0.4 r 1 e 30 val loss 0.367 val acc 86.575 best val_acc 86.575\n",
      "Dir 0.4 r 1 e 40 val loss 0.340 val acc 87.451 best val_acc 87.451\n",
      "Dir 0.4 r 1 e 49 val loss 0.323 val acc 88.089 best val_acc 88.089\n",
      "Dir 0.4 r 1 e 50 val loss 0.321 val acc 88.328 best val_acc 88.328\n",
      "===> Processing Dir 0.4 run 2\n",
      "generating participant indices for alpha 0.4\n",
      "Dir 0.4 r 2 e 0 val loss 1.492 val acc 54.596 best val_acc 54.596\n",
      "Dir 0.4 r 2 e 10 val loss 0.485 val acc 82.014 best val_acc 82.014\n",
      "Dir 0.4 r 2 e 20 val loss 0.408 val acc 85.250 best val_acc 85.250\n",
      "Dir 0.4 r 2 e 30 val loss 0.368 val acc 86.555 best val_acc 86.555\n",
      "Dir 0.4 r 2 e 40 val loss 0.343 val acc 87.611 best val_acc 87.611\n",
      "Dir 0.4 r 2 e 49 val loss 0.328 val acc 87.920 best val_acc 88.009\n",
      "Dir 0.4 r 2 e 50 val loss 0.328 val acc 88.039 best val_acc 88.039\n",
      "===> Processing Dir 0.5 run 0\n",
      "generating participant indices for alpha 0.5\n",
      "Dir 0.5 r 0 e 0 val loss 1.407 val acc 64.629 best val_acc 64.629\n",
      "Dir 0.5 r 0 e 10 val loss 0.476 val acc 82.031 best val_acc 82.031\n",
      "Dir 0.5 r 0 e 20 val loss 0.405 val acc 85.306 best val_acc 85.306\n",
      "Dir 0.5 r 0 e 30 val loss 0.370 val acc 86.401 best val_acc 86.401\n",
      "Dir 0.5 r 0 e 40 val loss 0.348 val acc 86.999 best val_acc 86.999\n",
      "Dir 0.5 r 0 e 49 val loss 0.333 val acc 87.606 best val_acc 87.616\n",
      "Dir 0.5 r 0 e 50 val loss 0.332 val acc 87.755 best val_acc 87.755\n",
      "===> Processing Dir 0.5 run 1\n",
      "generating participant indices for alpha 0.5\n",
      "Dir 0.5 r 1 e 10 val loss 0.462 val acc 82.728 best val_acc 82.728\n",
      "Dir 0.5 r 1 e 20 val loss 0.395 val acc 85.684 best val_acc 85.684\n",
      "Dir 0.5 r 1 e 30 val loss 0.357 val acc 87.128 best val_acc 87.128\n",
      "Dir 0.5 r 1 e 40 val loss 0.334 val acc 87.845 best val_acc 87.845\n",
      "Dir 0.5 r 1 e 49 val loss 0.318 val acc 88.342 best val_acc 88.342\n",
      "Dir 0.5 r 1 e 50 val loss 0.317 val acc 88.323 best val_acc 88.342\n",
      "===> Processing Dir 0.5 run 2\n",
      "generating participant indices for alpha 0.5\n",
      "Dir 0.5 r 2 e 0 val loss 1.366 val acc 66.033 best val_acc 66.033\n",
      "Dir 0.5 r 2 e 10 val loss 0.476 val acc 82.210 best val_acc 82.210\n",
      "Dir 0.5 r 2 e 20 val loss 0.411 val acc 84.888 best val_acc 84.888\n",
      "Dir 0.5 r 2 e 30 val loss 0.375 val acc 86.172 best val_acc 86.172\n",
      "Dir 0.5 r 2 e 40 val loss 0.354 val acc 86.869 best val_acc 86.869\n",
      "Dir 0.5 r 2 e 49 val loss 0.338 val acc 87.377 best val_acc 87.377\n",
      "Dir 0.5 r 2 e 50 val loss 0.337 val acc 87.536 best val_acc 87.536\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "force=True\n",
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "distribution='dirichlet'\n",
    "params = [.1, .2, .3, .4, .5]\n",
    "\n",
    "for dir_p in params:\n",
    "    for run in range(3):\n",
    "        print('===> Processing Dir %.1f run %d' % (dir_p, run))\n",
    "        each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_data_dirichlet(all_data, num_workers, alpha=dir_p, force=force)\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = cnn().to(device)\n",
    "        fed_model.apply(init_weights)\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(nbyz, num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_updates=[]\n",
    "            benign_norm = 0\n",
    "\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(\n",
    "                        each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                        (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "            agg_update = torch.mean(user_updates, 0)\n",
    "            del user_updates\n",
    "            model_received = model_received + global_lr * agg_update\n",
    "            fed_model = cnn().to(device)\n",
    "            fed_model.apply(init_weights)\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "            if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "                print('Dir %.1f r %d e %d val loss %.3f val acc %.3f best val_acc %.3f'% (dir_p, run, epoch_num, val_loss, val_acc, best_global_acc))\n",
    "            epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a352634",
   "metadata": {},
   "source": [
    "# Trim attack on Dir + TrMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8027f053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Processing NDSS on Dir 10.0 run 0\n",
      "generating participant indices for alpha 10.0\n",
      "Trim on Dir 10.0 + TrMean | r 0 e 0 val loss 1.115 val acc 68.625 best val_acc 68.625\n",
      "Trim on Dir 10.0 + TrMean | r 0 e 10 val loss 0.524 val acc 80.145 best val_acc 80.145\n",
      "Trim on Dir 10.0 + TrMean | r 0 e 20 val loss 0.478 val acc 82.525 best val_acc 82.525\n",
      "Trim on Dir 10.0 + TrMean | r 0 e 30 val loss 0.453 val acc 83.740 best val_acc 83.740\n",
      "Trim on Dir 10.0 + TrMean | r 0 e 40 val loss 0.443 val acc 84.367 best val_acc 84.367\n",
      "Trim on Dir 10.0 + TrMean | r 0 e 49 val loss 0.431 val acc 84.985 best val_acc 84.985\n",
      "Trim on Dir 10.0 + TrMean | r 0 e 50 val loss 0.429 val acc 85.184 best val_acc 85.184\n",
      "===> Processing NDSS on Dir 10.0 run 1\n",
      "generating participant indices for alpha 10.0\n",
      "Trim on Dir 10.0 + TrMean | r 1 e 0 val loss 1.078 val acc 70.915 best val_acc 70.915\n",
      "Trim on Dir 10.0 + TrMean | r 1 e 10 val loss 0.516 val acc 80.215 best val_acc 80.215\n",
      "Trim on Dir 10.0 + TrMean | r 1 e 20 val loss 0.473 val acc 82.336 best val_acc 82.336\n",
      "Trim on Dir 10.0 + TrMean | r 1 e 30 val loss 0.445 val acc 83.620 best val_acc 83.620\n",
      "Trim on Dir 10.0 + TrMean | r 1 e 40 val loss 0.435 val acc 83.999 best val_acc 84.039\n",
      "Trim on Dir 10.0 + TrMean | r 1 e 49 val loss 0.422 val acc 84.736 best val_acc 84.736\n",
      "Trim on Dir 10.0 + TrMean | r 1 e 50 val loss 0.423 val acc 84.686 best val_acc 84.736\n",
      "===> Processing NDSS on Dir 10.0 run 2\n",
      "generating participant indices for alpha 10.0\n",
      "Trim on Dir 10.0 + TrMean | r 2 e 0 val loss 1.156 val acc 69.651 best val_acc 69.651\n",
      "Trim on Dir 10.0 + TrMean | r 2 e 10 val loss 0.522 val acc 80.444 best val_acc 80.444\n",
      "Trim on Dir 10.0 + TrMean | r 2 e 20 val loss 0.481 val acc 82.416 best val_acc 82.416\n",
      "Trim on Dir 10.0 + TrMean | r 2 e 30 val loss 0.459 val acc 83.391 best val_acc 83.391\n",
      "Trim on Dir 10.0 + TrMean | r 2 e 40 val loss 0.445 val acc 84.188 best val_acc 84.188\n",
      "Trim on Dir 10.0 + TrMean | r 2 e 49 val loss 0.436 val acc 84.636 best val_acc 84.636\n",
      "Trim on Dir 10.0 + TrMean | r 2 e 50 val loss 0.431 val acc 84.915 best val_acc 84.915\n",
      "===> Processing NDSS on Dir 0.5 run 0\n",
      "generating participant indices for alpha 0.5\n",
      "Trim on Dir 0.5 + TrMean | r 0 e 0 val loss 2.096 val acc 28.990 best val_acc 28.990\n",
      "Trim on Dir 0.5 + TrMean | r 0 e 10 val loss 0.604 val acc 77.531 best val_acc 77.531\n",
      "Trim on Dir 0.5 + TrMean | r 0 e 20 val loss 0.561 val acc 79.642 best val_acc 79.642\n",
      "Trim on Dir 0.5 + TrMean | r 0 e 30 val loss 0.544 val acc 80.438 best val_acc 80.438\n",
      "Trim on Dir 0.5 + TrMean | r 0 e 40 val loss 0.530 val acc 80.946 best val_acc 80.976\n",
      "Trim on Dir 0.5 + TrMean | r 0 e 49 val loss 0.521 val acc 81.274 best val_acc 81.304\n",
      "Trim on Dir 0.5 + TrMean | r 0 e 50 val loss 0.527 val acc 81.035 best val_acc 81.304\n",
      "===> Processing NDSS on Dir 0.5 run 1\n",
      "generating participant indices for alpha 0.5\n",
      "Trim on Dir 0.5 + TrMean | r 1 e 0 val loss 1.995 val acc 46.770 best val_acc 46.770\n",
      "Trim on Dir 0.5 + TrMean | r 1 e 10 val loss 0.599 val acc 77.949 best val_acc 77.949\n",
      "Trim on Dir 0.5 + TrMean | r 1 e 20 val loss 0.555 val acc 79.950 best val_acc 79.950\n",
      "Trim on Dir 0.5 + TrMean | r 1 e 30 val loss 0.534 val acc 80.826 best val_acc 80.936\n",
      "Trim on Dir 0.5 + TrMean | r 1 e 40 val loss 0.519 val acc 81.324 best val_acc 81.424\n",
      "Trim on Dir 0.5 + TrMean | r 1 e 49 val loss 0.508 val acc 81.812 best val_acc 81.842\n",
      "Trim on Dir 0.5 + TrMean | r 1 e 50 val loss 0.509 val acc 81.931 best val_acc 81.931\n",
      "===> Processing NDSS on Dir 0.5 run 2\n",
      "generating participant indices for alpha 0.5\n",
      "Trim on Dir 0.5 + TrMean | r 2 e 0 val loss 1.999 val acc 46.909 best val_acc 46.909\n",
      "Trim on Dir 0.5 + TrMean | r 2 e 10 val loss 0.623 val acc 76.894 best val_acc 76.894\n",
      "Trim on Dir 0.5 + TrMean | r 2 e 20 val loss 0.570 val acc 78.596 best val_acc 78.596\n",
      "Trim on Dir 0.5 + TrMean | r 2 e 30 val loss 0.535 val acc 80.229 best val_acc 80.229\n",
      "Trim on Dir 0.5 + TrMean | r 2 e 40 val loss 0.518 val acc 81.205 best val_acc 81.205\n",
      "Trim on Dir 0.5 + TrMean | r 2 e 49 val loss 0.504 val acc 81.573 best val_acc 81.593\n",
      "Trim on Dir 0.5 + TrMean | r 2 e 50 val loss 0.502 val acc 81.672 best val_acc 81.672\n",
      "===> Processing NDSS on Dir 0.4 run 0\n",
      "generating participant indices for alpha 0.4\n",
      "Trim on Dir 0.4 + TrMean | r 0 e 0 val loss 2.033 val acc 46.549 best val_acc 46.549\n",
      "Trim on Dir 0.4 + TrMean | r 0 e 10 val loss 0.646 val acc 76.198 best val_acc 76.198\n",
      "Trim on Dir 0.4 + TrMean | r 0 e 20 val loss 0.602 val acc 76.845 best val_acc 76.954\n",
      "Trim on Dir 0.4 + TrMean | r 0 e 30 val loss 0.582 val acc 77.841 best val_acc 77.841\n",
      "Trim on Dir 0.4 + TrMean | r 0 e 40 val loss 0.582 val acc 77.801 best val_acc 79.006\n",
      "Trim on Dir 0.4 + TrMean | r 0 e 49 val loss 0.575 val acc 77.343 best val_acc 79.006\n",
      "Trim on Dir 0.4 + TrMean | r 0 e 50 val loss 0.575 val acc 77.672 best val_acc 79.006\n",
      "===> Processing NDSS on Dir 0.4 run 1\n",
      "generating participant indices for alpha 0.4\n",
      "Trim on Dir 0.4 + TrMean | r 1 e 0 val loss 2.089 val acc 32.547 best val_acc 32.547\n",
      "Trim on Dir 0.4 + TrMean | r 1 e 10 val loss 0.662 val acc 76.437 best val_acc 76.437\n",
      "Trim on Dir 0.4 + TrMean | r 1 e 20 val loss 0.603 val acc 77.911 best val_acc 77.911\n",
      "Trim on Dir 0.4 + TrMean | r 1 e 30 val loss 0.582 val acc 78.130 best val_acc 78.389\n",
      "Trim on Dir 0.4 + TrMean | r 1 e 40 val loss 0.559 val acc 78.717 best val_acc 78.887\n",
      "Trim on Dir 0.4 + TrMean | r 1 e 49 val loss 0.554 val acc 78.608 best val_acc 78.887\n",
      "Trim on Dir 0.4 + TrMean | r 1 e 50 val loss 0.552 val acc 78.837 best val_acc 78.887\n",
      "===> Processing NDSS on Dir 0.4 run 2\n",
      "generating participant indices for alpha 0.4\n",
      "Trim on Dir 0.4 + TrMean | r 2 e 0 val loss 2.102 val acc 38.134 best val_acc 38.134\n",
      "Trim on Dir 0.4 + TrMean | r 2 e 10 val loss 0.631 val acc 77.333 best val_acc 77.333\n",
      "Trim on Dir 0.4 + TrMean | r 2 e 20 val loss 0.569 val acc 79.404 best val_acc 79.404\n",
      "Trim on Dir 0.4 + TrMean | r 2 e 30 val loss 0.555 val acc 80.311 best val_acc 80.460\n",
      "Trim on Dir 0.4 + TrMean | r 2 e 40 val loss 0.534 val acc 81.028 best val_acc 81.167\n",
      "Trim on Dir 0.4 + TrMean | r 2 e 49 val loss 0.528 val acc 81.366 best val_acc 81.366\n",
      "Trim on Dir 0.4 + TrMean | r 2 e 50 val loss 0.526 val acc 81.366 best val_acc 81.366\n",
      "===> Processing NDSS on Dir 0.3 run 0\n",
      "generating participant indices for alpha 0.3\n",
      "Trim on Dir 0.3 + TrMean | r 0 e 0 val loss 2.187 val acc 22.627 best val_acc 22.627\n",
      "Trim on Dir 0.3 + TrMean | r 0 e 10 val loss 0.728 val acc 73.927 best val_acc 73.927\n",
      "Trim on Dir 0.3 + TrMean | r 0 e 20 val loss 0.659 val acc 76.188 best val_acc 76.188\n",
      "Trim on Dir 0.3 + TrMean | r 0 e 30 val loss 0.648 val acc 75.988 best val_acc 76.706\n",
      "Trim on Dir 0.3 + TrMean | r 0 e 40 val loss 0.635 val acc 76.387 best val_acc 76.706\n",
      "Trim on Dir 0.3 + TrMean | r 0 e 49 val loss 0.631 val acc 76.496 best val_acc 76.706\n",
      "Trim on Dir 0.3 + TrMean | r 0 e 50 val loss 0.640 val acc 76.287 best val_acc 76.706\n",
      "===> Processing NDSS on Dir 0.3 run 1\n",
      "generating participant indices for alpha 0.3\n",
      "Trim on Dir 0.3 + TrMean | r 1 e 0 val loss 2.093 val acc 37.197 best val_acc 37.197\n",
      "Trim on Dir 0.3 + TrMean | r 1 e 10 val loss 0.743 val acc 73.011 best val_acc 73.011\n",
      "Trim on Dir 0.3 + TrMean | r 1 e 20 val loss 0.676 val acc 74.913 best val_acc 74.913\n",
      "Trim on Dir 0.3 + TrMean | r 1 e 30 val loss 0.665 val acc 75.530 best val_acc 75.530\n",
      "Trim on Dir 0.3 + TrMean | r 1 e 40 val loss 0.673 val acc 75.152 best val_acc 75.530\n",
      "Trim on Dir 0.3 + TrMean | r 1 e 49 val loss 0.675 val acc 75.042 best val_acc 75.530\n",
      "Trim on Dir 0.3 + TrMean | r 1 e 50 val loss 0.696 val acc 74.614 best val_acc 75.530\n",
      "===> Processing NDSS on Dir 0.3 run 2\n",
      "generating participant indices for alpha 0.3\n",
      "Trim on Dir 0.3 + TrMean | r 2 e 0 val loss 2.154 val acc 35.724 best val_acc 35.724\n",
      "Trim on Dir 0.3 + TrMean | r 2 e 10 val loss 0.748 val acc 71.557 best val_acc 71.557\n",
      "Trim on Dir 0.3 + TrMean | r 2 e 20 val loss 0.697 val acc 73.837 best val_acc 73.837\n",
      "Trim on Dir 0.3 + TrMean | r 2 e 30 val loss 0.695 val acc 73.379 best val_acc 73.837\n",
      "Trim on Dir 0.3 + TrMean | r 2 e 40 val loss 0.682 val acc 73.917 best val_acc 74.345\n",
      "Trim on Dir 0.3 + TrMean | r 2 e 49 val loss 0.673 val acc 73.877 best val_acc 75.092\n",
      "Trim on Dir 0.3 + TrMean | r 2 e 50 val loss 0.680 val acc 73.917 best val_acc 75.092\n",
      "===> Processing NDSS on Dir 0.2 run 0\n",
      "generating participant indices for alpha 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trim on Dir 0.2 + TrMean | r 0 e 0 val loss 2.170 val acc 31.763 best val_acc 31.763\n",
      "Trim on Dir 0.2 + TrMean | r 0 e 10 val loss 0.943 val acc 64.781 best val_acc 64.781\n",
      "Trim on Dir 0.2 + TrMean | r 0 e 20 val loss 0.802 val acc 71.135 best val_acc 71.135\n",
      "Trim on Dir 0.2 + TrMean | r 0 e 30 val loss 0.771 val acc 72.430 best val_acc 72.430\n",
      "Trim on Dir 0.2 + TrMean | r 0 e 40 val loss 0.757 val acc 73.018 best val_acc 73.566\n",
      "Trim on Dir 0.2 + TrMean | r 0 e 49 val loss 0.759 val acc 73.038 best val_acc 73.566\n",
      "Trim on Dir 0.2 + TrMean | r 0 e 50 val loss 0.764 val acc 72.699 best val_acc 73.566\n",
      "===> Processing NDSS on Dir 0.2 run 1\n",
      "generating participant indices for alpha 0.2\n",
      "Trim on Dir 0.2 + TrMean | r 1 e 0 val loss 2.194 val acc 25.608 best val_acc 25.608\n",
      "Trim on Dir 0.2 + TrMean | r 1 e 10 val loss 0.998 val acc 63.147 best val_acc 63.147\n",
      "Trim on Dir 0.2 + TrMean | r 1 e 20 val loss 0.846 val acc 68.327 best val_acc 68.327\n",
      "Trim on Dir 0.2 + TrMean | r 1 e 30 val loss 0.831 val acc 68.546 best val_acc 68.825\n",
      "Trim on Dir 0.2 + TrMean | r 1 e 40 val loss 0.807 val acc 69.343 best val_acc 69.890\n",
      "Trim on Dir 0.2 + TrMean | r 1 e 49 val loss 0.795 val acc 69.880 best val_acc 69.890\n",
      "Trim on Dir 0.2 + TrMean | r 1 e 50 val loss 0.804 val acc 69.402 best val_acc 69.890\n",
      "===> Processing NDSS on Dir 0.2 run 2\n",
      "generating participant indices for alpha 0.2\n",
      "Trim on Dir 0.2 + TrMean | r 2 e 0 val loss 2.282 val acc 15.199 best val_acc 15.199\n",
      "Trim on Dir 0.2 + TrMean | r 2 e 10 val loss 1.009 val acc 62.122 best val_acc 62.122\n",
      "Trim on Dir 0.2 + TrMean | r 2 e 20 val loss 0.854 val acc 68.755 best val_acc 68.755\n",
      "Trim on Dir 0.2 + TrMean | r 2 e 30 val loss 0.839 val acc 69.392 best val_acc 69.532\n",
      "Trim on Dir 0.2 + TrMean | r 2 e 40 val loss 0.814 val acc 70.757 best val_acc 70.757\n",
      "Trim on Dir 0.2 + TrMean | r 2 e 49 val loss 0.788 val acc 72.022 best val_acc 72.022\n",
      "Trim on Dir 0.2 + TrMean | r 2 e 50 val loss 0.794 val acc 72.062 best val_acc 72.062\n",
      "===> Processing NDSS on Dir 0.1 run 0\n",
      "generating participant indices for alpha 0.1\n",
      "Trim on Dir 0.1 + TrMean | r 0 e 0 val loss 2.319 val acc 3.055 best val_acc 3.055\n",
      "Trim on Dir 0.1 + TrMean | r 0 e 10 val loss 1.561 val acc 36.435 best val_acc 36.435\n",
      "Trim on Dir 0.1 + TrMean | r 0 e 20 val loss 1.464 val acc 42.058 best val_acc 42.078\n",
      "Trim on Dir 0.1 + TrMean | r 0 e 30 val loss 1.309 val acc 49.244 best val_acc 49.244\n",
      "Trim on Dir 0.1 + TrMean | r 0 e 40 val loss 1.233 val acc 53.414 best val_acc 53.752\n",
      "Trim on Dir 0.1 + TrMean | r 0 e 49 val loss 1.216 val acc 54.041 best val_acc 54.807\n",
      "Trim on Dir 0.1 + TrMean | r 0 e 50 val loss 1.213 val acc 53.901 best val_acc 54.807\n",
      "===> Processing NDSS on Dir 0.1 run 1\n",
      "generating participant indices for alpha 0.1\n",
      "Trim on Dir 0.1 + TrMean | r 1 e 0 val loss 2.346 val acc 3.991 best val_acc 3.991\n",
      "Trim on Dir 0.1 + TrMean | r 1 e 10 val loss 1.661 val acc 39.859 best val_acc 40.068\n",
      "Trim on Dir 0.1 + TrMean | r 1 e 20 val loss 1.460 val acc 45.133 best val_acc 45.133\n",
      "Trim on Dir 0.1 + TrMean | r 1 e 30 val loss 1.304 val acc 49.821 best val_acc 49.821\n",
      "Trim on Dir 0.1 + TrMean | r 1 e 40 val loss 1.278 val acc 54.080 best val_acc 54.469\n",
      "Trim on Dir 0.1 + TrMean | r 1 e 49 val loss 1.252 val acc 55.404 best val_acc 55.404\n",
      "Trim on Dir 0.1 + TrMean | r 1 e 50 val loss 1.298 val acc 53.035 best val_acc 55.404\n",
      "===> Processing NDSS on Dir 0.1 run 2\n",
      "generating participant indices for alpha 0.1\n",
      "Trim on Dir 0.1 + TrMean | r 2 e 0 val loss 2.302 val acc 14.043 best val_acc 14.043\n",
      "Trim on Dir 0.1 + TrMean | r 2 e 10 val loss 1.631 val acc 35.281 best val_acc 35.281\n",
      "Trim on Dir 0.1 + TrMean | r 2 e 20 val loss 1.547 val acc 36.873 best val_acc 36.873\n",
      "Trim on Dir 0.1 + TrMean | r 2 e 30 val loss 1.387 val acc 47.412 best val_acc 47.412\n",
      "Trim on Dir 0.1 + TrMean | r 2 e 40 val loss 1.274 val acc 54.279 best val_acc 54.508\n",
      "Trim on Dir 0.1 + TrMean | r 2 e 49 val loss 1.253 val acc 56.479 best val_acc 56.479\n",
      "Trim on Dir 0.1 + TrMean | r 2 e 50 val loss 1.240 val acc 56.180 best val_acc 56.479\n",
      "======= Results for Trim attack on Dir -0.6  =====\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "tensor(-0.6187, device='cuda:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fcj_p \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m======= Results for Trim attack on Dir \u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[38;5;124m  =====\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m fcj_p)\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfcj_p\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: tensor(-0.6187, device='cuda:0')"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "fast_ndss = True\n",
    "params = [.1, .2, .3, .4, .5, 10]\n",
    "\n",
    "results = {}\n",
    "for dir_p in np.array(params)[::-1]:\n",
    "    results[dir_p] = []\n",
    "    for run in range(3):\n",
    "        print('===> Processing NDSS on Dir %.1f run %d' % (dir_p, run))\n",
    "        each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_data_dirichlet(all_data, num_workers, alpha=dir_p, force=force)\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = cnn().to(device)\n",
    "        fed_model.apply(init_weights)\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_updates=[]\n",
    "            benign_norm = 0\n",
    "\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(\n",
    "                        each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                        (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "            user_updates = full_trim(user_updates, nbyz)\n",
    "            agg_update = tr_mean(user_updates, nbyz)\n",
    "\n",
    "            del user_updates\n",
    "            model_received = model_received + global_lr * agg_update\n",
    "            fed_model = cnn().to(device)\n",
    "            fed_model.apply(init_weights)\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "            if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "                print('Trim on Dir %.1f + TrMean | r %d e %d val loss %.3f val acc %.3f best val_acc %.3f'% (dir_p, run, epoch_num, val_loss, val_acc, best_global_acc))\n",
    "            epoch_num+=1\n",
    "        results[dir_p].append(best_global_acc)\n",
    "\n",
    "for fcj_p in params:\n",
    "    print('======= Results for Trim attack on Dir %.1f  =====' % fcj_p)\n",
    "    print(results[fcj_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "357b9e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10.0: [85.18371005211648, 84.73563677331882, 84.91486608423014],\n",
       " 0.5: [81.30413141093959, 81.9313090930595, 81.67247386987438],\n",
       " 0.4: [79.00607508224458, 78.88656508999746, 81.36639776990633],\n",
       " 0.3: [76.70550740438324, 75.53032565565654, 75.09212228945795],\n",
       " 0.2: [73.56573705179282, 69.89043822269515, 72.06175298804781],\n",
       " 0.1: [54.80692675159236, 55.40406050955414, 56.47890127388535]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c32b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec12f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
