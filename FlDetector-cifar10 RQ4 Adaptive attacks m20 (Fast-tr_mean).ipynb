{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for CIFAR10 with Fang/Dirichlet distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e300ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu105/4484626/ipykernel_2194263/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "import copy\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77aa30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_models import *\n",
    "from cifar10_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "\n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fang_train_data(trainset, num_workers=100, bias=0.5, force=False):\n",
    "    dist_file = 'fang_nworkers%d_bias%.1f.pkl' % (num_workers, bias)\n",
    "    if not force and os.path.exists(dist_file):\n",
    "        print('Loading fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "        return pickle.load(open(dist_file, 'rb'))\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    per_participant_list = defaultdict(list)\n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        per_participant_list[selected_worker].extend([i])\n",
    "    \n",
    "    print('Saving fang distribution for num_workers %d and bias %.1f to memory' % (num_workers, bias))\n",
    "    pickle.dump(per_participant_list, open(dist_file, 'wb'))\n",
    "    return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799ffb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_federated_data(trainset, num_workers, distribution='fang', param=1, force=False):\n",
    "    if distribution == 'fang':\n",
    "        per_participant_list = get_fang_train_data(trainset, num_workers, bias=param, force=force)\n",
    "    elif distribution == 'dirichlet':\n",
    "        per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=param, force=force)\n",
    "\n",
    "    each_worker_idx = [[] for _ in range(num_workers)] \n",
    "    each_worker_te_idx = [[] for _ in range(num_workers)]\n",
    "    np.random.seed(0)\n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(5*w_len/6)\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        each_worker_idx[worker_idx] = w_indices[tr_idx]\n",
    "        each_worker_te_idx[worker_idx] = w_indices[te_idx]\n",
    "    global_test_idx = np.concatenate(each_worker_te_idx)\n",
    "    return each_worker_idx, each_worker_te_idx, global_test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2590db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(dataset, indices, batch_size=32, shuffle=False):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acbb06d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/home/vshejwalkar_umass_edu/data/'\n",
    "# load the train dataset\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_train)\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_train)\n",
    "\n",
    "te_cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_test)\n",
    "te_cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(trainloader, model, model_received, criterion, optimizer, pgd=False, eps=2):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size()[0])\n",
    "        top1.update(prec1.item()/100.0, inputs.size()[0])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if pgd:\n",
    "            curr_model = list(model.parameters())\n",
    "            curr_model_vec = parameters_to_vector(curr_model)\n",
    "            if torch.norm(curr_model_vec - model_received) > eps:\n",
    "                curr_model_vec = eps*(curr_model_vec - model_received)/torch.norm(curr_model_vec - model_received) + model_received\n",
    "                vector_to_parameters(curr_model_vec, curr_model)\n",
    "        \n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def test(testloader, model, criterion):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(testloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        top1.update(prec1/100.0, inputs.size()[0])\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f843efb",
   "metadata": {},
   "source": [
    "# resnet-BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bb2ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0bf58",
   "metadata": {},
   "source": [
    "# FLDetector utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b2fe2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def fldetector(old_gradients, user_grads, b=0, hvp=None, agr='tr_mean'):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    if agr == 'average':\n",
    "        agg_grads = torch.mean(user_grads, 0)\n",
    "    elif agr == 'median':\n",
    "        agg_grads = torch.median(user_grads, 0)[0]\n",
    "    elif agr == 'tr_mean':\n",
    "        agg_grads = tr_mean(user_grads, b)\n",
    "    return agg_grads, distance\n",
    "\n",
    "def lbfgs(S_k_list, Y_k_list, v):\n",
    "    curr_S_k = torch.stack(S_k_list).T\n",
    "    curr_Y_k = torch.stack(Y_k_list).T\n",
    "    S_k_time_Y_k = np.dot(curr_S_k.T.cpu().numpy(), curr_Y_k.cpu().numpy())\n",
    "    S_k_time_S_k = np.dot(curr_S_k.T.cpu().numpy(), curr_S_k.cpu().numpy())\n",
    "    R_k = np.triu(S_k_time_Y_k)\n",
    "    L_k = S_k_time_Y_k - R_k\n",
    "    sigma_k = np.dot(Y_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()) / (np.dot(S_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()))\n",
    "    D_k_diag = np.diag(S_k_time_Y_k)\n",
    "    upper_mat = np.concatenate((sigma_k * S_k_time_S_k, L_k), axis=1)\n",
    "    lower_mat = np.concatenate((L_k.T, -np.diag(D_k_diag)), axis=1)\n",
    "    mat = np.concatenate((upper_mat, lower_mat), axis=0)\n",
    "    mat_inv = np.linalg.inv(mat)\n",
    "\n",
    "    approx_prod = sigma_k * v.cpu().numpy()\n",
    "    approx_prod = approx_prod.T\n",
    "    p_mat = np.concatenate((np.dot(curr_S_k.T.cpu().numpy(), sigma_k * v.unsqueeze(0).T.cpu().numpy()), np.dot(curr_Y_k.T.cpu().numpy(), v.unsqueeze(0).T.cpu().numpy())), axis=0)\n",
    "    approx_prod -= np.dot(np.dot(np.concatenate((sigma_k * curr_S_k.cpu().numpy(), curr_Y_k.cpu().numpy()), axis=1), mat_inv), p_mat)\n",
    "\n",
    "    return approx_prod\n",
    "\n",
    "def detection(score, nobyz, nworkers):\n",
    "    estimator = KMeans(n_clusters=2)\n",
    "    estimator.fit(score.reshape(-1, 1))\n",
    "    label_pred = estimator.labels_\n",
    "    if np.mean(score[label_pred==0])<np.mean(score[label_pred==1]):\n",
    "        #0 is the label of malicious clients\n",
    "        label_pred = 1 - label_pred\n",
    "    real_label=np.ones(nworkers)\n",
    "    real_label[:nobyz]=0\n",
    "    acc=len(label_pred[label_pred==real_label])/nworkers\n",
    "    recall=1-np.sum(label_pred[:nobyz])/nobyz\n",
    "    fpr=1-np.sum(label_pred[nobyz:])/(nworkers-nobyz)\n",
    "    fnr=np.sum(label_pred[:nobyz])/nobyz\n",
    "    auc = roc_auc_score(real_label, label_pred)\n",
    "    print(\"acc %0.4f; recall %0.4f; fpr %0.4f; fnr %0.4f; auc %.4f\" % (acc, recall, fpr, fnr, auc))\n",
    "    return acc, fpr, fnr, auc\n",
    "    # print(silhouette_score(score.reshape(-1, 1), label_pred))\n",
    "\n",
    "def detection1(score, nobyz):\n",
    "    nrefs = 10\n",
    "    ks = range(1, 8)\n",
    "    gaps = np.zeros(len(ks))\n",
    "    gapDiff = np.zeros(len(ks) - 1)\n",
    "    sdk = np.zeros(len(ks))\n",
    "    min = np.min(score)\n",
    "    max = np.max(score)\n",
    "    score = (score - min)/(max-min)\n",
    "    for i, k in enumerate(ks):\n",
    "        estimator = KMeans(n_clusters=k)\n",
    "        estimator.fit(score.reshape(-1, 1))\n",
    "        label_pred = estimator.labels_\n",
    "        center = estimator.cluster_centers_\n",
    "        Wk = np.sum([np.square(score[m]-center[label_pred[m]]) for m in range(len(score))])\n",
    "        WkRef = np.zeros(nrefs)\n",
    "        for j in range(nrefs):\n",
    "            rand = np.random.uniform(0, 1, len(score))\n",
    "            estimator = KMeans(n_clusters=k)\n",
    "            estimator.fit(rand.reshape(-1, 1))\n",
    "            label_pred = estimator.labels_\n",
    "            center = estimator.cluster_centers_\n",
    "            WkRef[j] = np.sum([np.square(rand[m]-center[label_pred[m]]) for m in range(len(rand))])\n",
    "        gaps[i] = np.log(np.mean(WkRef)) - np.log(Wk)\n",
    "        sdk[i] = np.sqrt((1.0 + nrefs) / nrefs) * np.std(np.log(WkRef))\n",
    "\n",
    "        if i > 0:\n",
    "            gapDiff[i - 1] = gaps[i - 1] - gaps[i] + sdk[i]\n",
    "    #print(gapDiff)\n",
    "    for i in range(len(gapDiff)):\n",
    "        if gapDiff[i] >= 0:\n",
    "            select_k = i+1\n",
    "            break\n",
    "    if select_k == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        # print('Attack Detected!')\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e25e2ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fang distribution for num_workers 100 and bias 0.5 from memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 100, 10038)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = False\n",
    "\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=100, distribution=distribution, param=param, force=force)\n",
    "\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    train_loaders.append((pos, get_train(all_data, indices, batch_size)))\n",
    "test_loaders = []\n",
    "for pos, indices in enumerate(each_worker_te_idx):\n",
    "    test_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "len(train_loaders), len(test_loaders), len(global_test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62611dc3",
   "metadata": {},
   "source": [
    "# Mean under no attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c68ca6ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 benign_norm 1359.627 val loss 2.304 val acc 0.100 best val_acc 0.100\n",
      "e 1 benign_norm 647.379 val loss 2.312 val acc 0.100 best val_acc 0.100\n",
      "e 2 benign_norm 281.629 val loss 2.377 val acc 0.105 best val_acc 0.105\n",
      "e 3 benign_norm 278.466 val loss 2.259 val acc 0.126 best val_acc 0.126\n",
      "e 4 benign_norm 277.227 val loss 2.078 val acc 0.216 best val_acc 0.216\n",
      "e 5 benign_norm 276.699 val loss 1.925 val acc 0.277 best val_acc 0.277\n",
      "e 6 benign_norm 276.419 val loss 1.836 val acc 0.304 best val_acc 0.304\n",
      "e 7 benign_norm 276.116 val loss 1.772 val acc 0.329 best val_acc 0.329\n",
      "e 8 benign_norm 275.852 val loss 1.702 val acc 0.369 best val_acc 0.369\n",
      "e 9 benign_norm 275.644 val loss 1.633 val acc 0.398 best val_acc 0.398\n",
      "e 10 benign_norm 275.504 val loss 1.585 val acc 0.417 best val_acc 0.417\n",
      "e 11 benign_norm 275.477 val loss 1.549 val acc 0.427 best val_acc 0.427\n",
      "e 12 benign_norm 275.416 val loss 1.494 val acc 0.448 best val_acc 0.448\n",
      "e 13 benign_norm 275.439 val loss 1.472 val acc 0.456 best val_acc 0.456\n",
      "e 14 benign_norm 275.461 val loss 1.439 val acc 0.468 best val_acc 0.468\n",
      "e 15 benign_norm 275.496 val loss 1.404 val acc 0.481 best val_acc 0.481\n",
      "e 16 benign_norm 275.448 val loss 1.374 val acc 0.499 best val_acc 0.499\n",
      "e 17 benign_norm 275.520 val loss 1.345 val acc 0.504 best val_acc 0.504\n",
      "e 18 benign_norm 275.505 val loss 1.317 val acc 0.515 best val_acc 0.515\n",
      "e 19 benign_norm 275.569 val loss 1.290 val acc 0.523 best val_acc 0.523\n",
      "e 20 benign_norm 275.484 val loss 1.257 val acc 0.534 best val_acc 0.534\n",
      "e 21 benign_norm 275.507 val loss 1.238 val acc 0.541 best val_acc 0.541\n",
      "e 22 benign_norm 275.648 val loss 1.218 val acc 0.548 best val_acc 0.548\n",
      "e 23 benign_norm 275.602 val loss 1.191 val acc 0.556 best val_acc 0.556\n",
      "e 24 benign_norm 275.648 val loss 1.175 val acc 0.565 best val_acc 0.565\n",
      "e 25 benign_norm 275.611 val loss 1.149 val acc 0.575 best val_acc 0.575\n",
      "e 26 benign_norm 275.680 val loss 1.133 val acc 0.578 best val_acc 0.578\n",
      "e 27 benign_norm 275.628 val loss 1.108 val acc 0.591 best val_acc 0.591\n",
      "e 28 benign_norm 275.623 val loss 1.089 val acc 0.597 best val_acc 0.597\n",
      "e 29 benign_norm 275.704 val loss 1.074 val acc 0.605 best val_acc 0.605\n",
      "e 30 benign_norm 275.659 val loss 1.054 val acc 0.608 best val_acc 0.608\n",
      "e 31 benign_norm 275.711 val loss 1.046 val acc 0.617 best val_acc 0.617\n",
      "e 32 benign_norm 275.717 val loss 1.024 val acc 0.622 best val_acc 0.622\n",
      "e 33 benign_norm 275.728 val loss 1.008 val acc 0.627 best val_acc 0.627\n",
      "e 34 benign_norm 275.757 val loss 0.992 val acc 0.635 best val_acc 0.635\n",
      "e 35 benign_norm 275.718 val loss 0.981 val acc 0.639 best val_acc 0.639\n",
      "e 36 benign_norm 275.778 val loss 0.963 val acc 0.647 best val_acc 0.647\n",
      "e 37 benign_norm 275.768 val loss 0.947 val acc 0.653 best val_acc 0.653\n",
      "e 38 benign_norm 275.787 val loss 0.932 val acc 0.659 best val_acc 0.659\n",
      "e 39 benign_norm 275.803 val loss 0.920 val acc 0.664 best val_acc 0.664\n",
      "e 40 benign_norm 275.821 val loss 0.910 val acc 0.666 best val_acc 0.666\n",
      "e 41 benign_norm 275.898 val loss 0.897 val acc 0.673 best val_acc 0.673\n",
      "e 42 benign_norm 275.822 val loss 0.884 val acc 0.676 best val_acc 0.676\n",
      "e 43 benign_norm 275.934 val loss 0.875 val acc 0.679 best val_acc 0.679\n",
      "e 44 benign_norm 275.894 val loss 0.860 val acc 0.687 best val_acc 0.687\n",
      "e 45 benign_norm 276.012 val loss 0.853 val acc 0.688 best val_acc 0.688\n",
      "e 46 benign_norm 275.987 val loss 0.840 val acc 0.695 best val_acc 0.695\n",
      "e 47 benign_norm 275.956 val loss 0.833 val acc 0.696 best val_acc 0.696\n",
      "e 48 benign_norm 276.015 val loss 0.817 val acc 0.704 best val_acc 0.704\n",
      "e 49 benign_norm 276.081 val loss 0.812 val acc 0.706 best val_acc 0.706\n",
      "e 50 benign_norm 276.074 val loss 0.801 val acc 0.712 best val_acc 0.712\n",
      "e 51 benign_norm 276.207 val loss 0.791 val acc 0.719 best val_acc 0.719\n",
      "e 52 benign_norm 276.131 val loss 0.787 val acc 0.717 best val_acc 0.719\n",
      "e 53 benign_norm 276.165 val loss 0.776 val acc 0.723 best val_acc 0.723\n",
      "e 54 benign_norm 276.630 val loss 0.770 val acc 0.724 best val_acc 0.724\n",
      "e 55 benign_norm 276.224 val loss 0.758 val acc 0.729 best val_acc 0.729\n",
      "e 56 benign_norm 276.255 val loss 0.749 val acc 0.731 best val_acc 0.731\n",
      "e 57 benign_norm 276.191 val loss 0.746 val acc 0.735 best val_acc 0.735\n",
      "e 58 benign_norm 276.244 val loss 0.741 val acc 0.733 best val_acc 0.735\n",
      "e 59 benign_norm 276.307 val loss 0.737 val acc 0.736 best val_acc 0.736\n",
      "e 60 benign_norm 276.322 val loss 0.723 val acc 0.743 best val_acc 0.743\n",
      "e 61 benign_norm 276.444 val loss 0.724 val acc 0.742 best val_acc 0.743\n",
      "e 62 benign_norm 276.370 val loss 0.710 val acc 0.746 best val_acc 0.746\n",
      "e 63 benign_norm 276.477 val loss 0.712 val acc 0.746 best val_acc 0.746\n",
      "e 64 benign_norm 276.426 val loss 0.704 val acc 0.748 best val_acc 0.748\n",
      "e 65 benign_norm 276.450 val loss 0.693 val acc 0.753 best val_acc 0.753\n",
      "e 66 benign_norm 276.623 val loss 0.692 val acc 0.754 best val_acc 0.754\n",
      "e 67 benign_norm 276.446 val loss 0.686 val acc 0.759 best val_acc 0.759\n",
      "e 68 benign_norm 276.513 val loss 0.686 val acc 0.758 best val_acc 0.759\n",
      "e 69 benign_norm 276.659 val loss 0.675 val acc 0.762 best val_acc 0.762\n",
      "e 70 benign_norm 276.636 val loss 0.668 val acc 0.762 best val_acc 0.762\n",
      "e 71 benign_norm 276.607 val loss 0.668 val acc 0.762 best val_acc 0.762\n",
      "e 72 benign_norm 276.710 val loss 0.663 val acc 0.764 best val_acc 0.764\n",
      "e 73 benign_norm 276.735 val loss 0.662 val acc 0.766 best val_acc 0.766\n",
      "e 74 benign_norm 276.698 val loss 0.652 val acc 0.770 best val_acc 0.770\n",
      "e 75 benign_norm 276.735 val loss 0.647 val acc 0.771 best val_acc 0.771\n",
      "e 76 benign_norm 276.668 val loss 0.642 val acc 0.773 best val_acc 0.773\n",
      "e 77 benign_norm 276.821 val loss 0.642 val acc 0.773 best val_acc 0.773\n",
      "e 78 benign_norm 276.792 val loss 0.635 val acc 0.776 best val_acc 0.776\n",
      "e 79 benign_norm 276.883 val loss 0.636 val acc 0.776 best val_acc 0.776\n",
      "e 80 benign_norm 276.801 val loss 0.626 val acc 0.780 best val_acc 0.780\n",
      "e 81 benign_norm 276.937 val loss 0.631 val acc 0.779 best val_acc 0.780\n",
      "e 82 benign_norm 276.925 val loss 0.626 val acc 0.779 best val_acc 0.780\n",
      "e 83 benign_norm 276.875 val loss 0.613 val acc 0.785 best val_acc 0.785\n",
      "e 84 benign_norm 276.953 val loss 0.621 val acc 0.782 best val_acc 0.785\n",
      "e 85 benign_norm 276.962 val loss 0.612 val acc 0.788 best val_acc 0.788\n",
      "e 86 benign_norm 276.923 val loss 0.613 val acc 0.787 best val_acc 0.788\n",
      "e 87 benign_norm 277.093 val loss 0.610 val acc 0.786 best val_acc 0.788\n",
      "e 88 benign_norm 277.006 val loss 0.604 val acc 0.789 best val_acc 0.789\n",
      "e 89 benign_norm 277.235 val loss 0.602 val acc 0.791 best val_acc 0.791\n",
      "e 90 benign_norm 277.004 val loss 0.597 val acc 0.791 best val_acc 0.791\n",
      "e 91 benign_norm 277.228 val loss 0.597 val acc 0.794 best val_acc 0.794\n",
      "e 92 benign_norm 277.067 val loss 0.591 val acc 0.796 best val_acc 0.796\n",
      "e 93 benign_norm 277.167 val loss 0.585 val acc 0.799 best val_acc 0.799\n",
      "e 94 benign_norm 277.200 val loss 0.589 val acc 0.796 best val_acc 0.799\n",
      "e 95 benign_norm 277.340 val loss 0.584 val acc 0.797 best val_acc 0.799\n",
      "e 96 benign_norm 277.200 val loss 0.582 val acc 0.799 best val_acc 0.799\n",
      "e 97 benign_norm 277.185 val loss 0.577 val acc 0.800 best val_acc 0.800\n",
      "e 98 benign_norm 277.351 val loss 0.576 val acc 0.800 best val_acc 0.800\n",
      "e 99 benign_norm 277.220 val loss 0.575 val acc 0.802 best val_acc 0.802\n",
      "e 100 benign_norm 277.554 val loss 0.576 val acc 0.802 best val_acc 0.802\n",
      "e 101 benign_norm 277.316 val loss 0.571 val acc 0.805 best val_acc 0.805\n",
      "e 102 benign_norm 277.474 val loss 0.573 val acc 0.802 best val_acc 0.805\n",
      "e 103 benign_norm 277.354 val loss 0.567 val acc 0.806 best val_acc 0.806\n",
      "e 104 benign_norm 277.342 val loss 0.566 val acc 0.805 best val_acc 0.806\n",
      "e 105 benign_norm 277.406 val loss 0.561 val acc 0.806 best val_acc 0.806\n",
      "e 106 benign_norm 277.464 val loss 0.563 val acc 0.807 best val_acc 0.807\n",
      "e 107 benign_norm 277.332 val loss 0.556 val acc 0.808 best val_acc 0.808\n",
      "e 108 benign_norm 277.596 val loss 0.556 val acc 0.809 best val_acc 0.809\n",
      "e 109 benign_norm 277.417 val loss 0.555 val acc 0.809 best val_acc 0.809\n",
      "e 110 benign_norm 277.500 val loss 0.554 val acc 0.810 best val_acc 0.810\n",
      "e 111 benign_norm 277.571 val loss 0.554 val acc 0.808 best val_acc 0.810\n",
      "e 113 benign_norm 277.525 val loss 0.542 val acc 0.812 best val_acc 0.812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 114 benign_norm 277.663 val loss 0.548 val acc 0.810 best val_acc 0.812\n",
      "e 115 benign_norm 277.560 val loss 0.550 val acc 0.811 best val_acc 0.812\n",
      "e 116 benign_norm 277.522 val loss 0.542 val acc 0.812 best val_acc 0.812\n",
      "e 117 benign_norm 277.673 val loss 0.537 val acc 0.815 best val_acc 0.815\n",
      "e 118 benign_norm 277.734 val loss 0.537 val acc 0.814 best val_acc 0.815\n",
      "e 119 benign_norm 277.839 val loss 0.538 val acc 0.813 best val_acc 0.815\n",
      "e 120 benign_norm 277.763 val loss 0.533 val acc 0.815 best val_acc 0.815\n",
      "e 121 benign_norm 277.674 val loss 0.535 val acc 0.816 best val_acc 0.816\n",
      "e 122 benign_norm 277.660 val loss 0.528 val acc 0.818 best val_acc 0.818\n",
      "e 123 benign_norm 277.740 val loss 0.535 val acc 0.815 best val_acc 0.818\n",
      "e 124 benign_norm 277.841 val loss 0.525 val acc 0.818 best val_acc 0.818\n",
      "e 125 benign_norm 277.799 val loss 0.526 val acc 0.817 best val_acc 0.818\n",
      "e 126 benign_norm 277.846 val loss 0.525 val acc 0.820 best val_acc 0.820\n",
      "e 127 benign_norm 277.690 val loss 0.529 val acc 0.816 best val_acc 0.820\n",
      "e 128 benign_norm 277.736 val loss 0.523 val acc 0.820 best val_acc 0.820\n",
      "e 129 benign_norm 277.766 val loss 0.520 val acc 0.821 best val_acc 0.821\n",
      "e 130 benign_norm 277.729 val loss 0.525 val acc 0.817 best val_acc 0.821\n",
      "e 131 benign_norm 277.957 val loss 0.520 val acc 0.821 best val_acc 0.821\n",
      "e 132 benign_norm 277.837 val loss 0.522 val acc 0.821 best val_acc 0.821\n",
      "e 133 benign_norm 277.913 val loss 0.514 val acc 0.824 best val_acc 0.824\n",
      "e 134 benign_norm 278.284 val loss 0.518 val acc 0.821 best val_acc 0.824\n",
      "e 135 benign_norm 277.950 val loss 0.520 val acc 0.821 best val_acc 0.824\n",
      "e 136 benign_norm 277.960 val loss 0.518 val acc 0.821 best val_acc 0.824\n",
      "e 137 benign_norm 278.042 val loss 0.515 val acc 0.823 best val_acc 0.824\n",
      "e 138 benign_norm 278.017 val loss 0.515 val acc 0.822 best val_acc 0.824\n",
      "e 139 benign_norm 278.046 val loss 0.513 val acc 0.823 best val_acc 0.824\n",
      "e 140 benign_norm 277.926 val loss 0.510 val acc 0.826 best val_acc 0.826\n",
      "e 141 benign_norm 277.795 val loss 0.508 val acc 0.825 best val_acc 0.826\n",
      "e 142 benign_norm 277.992 val loss 0.508 val acc 0.826 best val_acc 0.826\n",
      "e 143 benign_norm 278.118 val loss 0.511 val acc 0.826 best val_acc 0.826\n",
      "e 144 benign_norm 278.046 val loss 0.508 val acc 0.825 best val_acc 0.826\n",
      "e 145 benign_norm 278.060 val loss 0.505 val acc 0.827 best val_acc 0.827\n",
      "e 146 benign_norm 278.093 val loss 0.504 val acc 0.828 best val_acc 0.828\n",
      "e 147 benign_norm 278.090 val loss 0.503 val acc 0.830 best val_acc 0.830\n",
      "e 148 benign_norm 278.166 val loss 0.501 val acc 0.829 best val_acc 0.830\n",
      "e 149 benign_norm 277.997 val loss 0.506 val acc 0.828 best val_acc 0.830\n",
      "e 150 benign_norm 277.986 val loss 0.501 val acc 0.829 best val_acc 0.830\n",
      "e 151 benign_norm 278.065 val loss 0.498 val acc 0.830 best val_acc 0.830\n",
      "e 152 benign_norm 278.196 val loss 0.505 val acc 0.828 best val_acc 0.830\n",
      "e 153 benign_norm 278.137 val loss 0.501 val acc 0.828 best val_acc 0.830\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m benign_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m round_benign:\n\u001b[0;32m---> 48\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m local_lr\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m0.999\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mepoch_num), momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(local_epochs):\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictiter:\n\u001b[1;32m    296\u001b[0m         key \u001b[38;5;241m=\u001b[39m deepcopy(key, memo)\n\u001b[0;32m--> 297\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m         y[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictiter:\n\u001b[1;32m    296\u001b[0m         key \u001b[38;5;241m=\u001b[39m deepcopy(key, memo)\n\u001b[0;32m--> 297\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m         y[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:177\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n\u001b[1;32m    176\u001b[0m     memo[d] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m--> 177\u001b[0m     \u001b[43m_keep_alive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Make sure x lives at least as long as d\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/copy.py:254\u001b[0m, in \u001b[0;36m_keep_alive\u001b[0;34m(x, memo)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m\"\"\"Keeps a reference to the object x in the memo.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03mBecause we remember objects by their id, we have\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03mthe memo itself...\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     memo[\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m]\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# aha, this is the first one :-)\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     memo[\u001b[38;5;28mid\u001b[39m(memo)]\u001b[38;5;241m=\u001b[39m[x]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.1\n",
    "global_lr = 1\n",
    "nbyz = 0\n",
    "agr = 'average'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "# FLDetector initializations\n",
    "weight_record = []\n",
    "grad_record = []\n",
    "test_grads = []\n",
    "old_grad_list = []\n",
    "malicious_scores = np.zeros((1, num_workers))\n",
    "start_detection_epoch = 10\n",
    "window_size = 10\n",
    "assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "# Adaptive attack initializers\n",
    "good_distance_rage = np.zeros((1, nbyz))\n",
    "attack_type = 'none'\n",
    "dev_type = 'std'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_grads=[]\n",
    "    benign_norm = 0\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "    weight = model_received\n",
    "\n",
    "    if (epoch_num > start_detection_epoch):\n",
    "        hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "        hvp = np.squeeze(hvp)\n",
    "    else:\n",
    "        hvp = None\n",
    "\n",
    "    good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "    if hvp is not None:\n",
    "        pred_grad = copy.deepcopy(good_old_grads)\n",
    "        distance = []\n",
    "        for i in range(len(good_old_grads)):\n",
    "            pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "        good_distance_rage = np.concatenate(\n",
    "            (good_distance_rage, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "        \n",
    "    if attack_type != 'none' and (epoch_num < start_detection_epoch):\n",
    "        user_grads = full_trim(user_grads, nbyz)\n",
    "        # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "    elif epoch_num > start_detection_epoch:\n",
    "        if attack_type == 'full_trim':\n",
    "            user_grads = full_trim(user_grads, nbyz)\n",
    "        elif attack_type == 'none':\n",
    "            pass\n",
    "        else:\n",
    "            if attack_type == 'LIE':\n",
    "                print(\"LIE\")\n",
    "                z = 0.1\n",
    "                noise_avg = torch.mean(user_grads[:nbyz], dim=0)\n",
    "                noise_std = torch.std(user_grads[:nbyz], dim=0)\n",
    "                noise = noise_avg + z * noise_std\n",
    "            elif attack_type == 'NDSS21':\n",
    "                distance_bound = np.random.choice(np.mean(good_distance_rage[-1:], 0))\n",
    "                model_re = torch.mean(good_current_grads, dim=0)\n",
    "                if dev_type == 'unit_vec':\n",
    "                    deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                elif dev_type == 'sign':\n",
    "                    deviation = torch.sign(model_re)\n",
    "                elif dev_type == 'std':\n",
    "                    deviation = torch.std(good_current_grads, 0)\n",
    "                # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_rage[-1]))) / torch.norm(deviation))\n",
    "                noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "            elif attack_type == 'mod_trim':\n",
    "                mal_grads= full_trim(user_grads[:nbyz], nbyz)\n",
    "                pass\n",
    "            else:\n",
    "                noise = torch.zeros(hvp.shape).to(device)\n",
    "            for m in range(nbyz):\n",
    "                user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "    agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "    \n",
    "    if distance is not None and epoch_num > (start_detection_epoch - window_size):\n",
    "        malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "    if malicious_scores.shape[0] >= window_size+1:\n",
    "        if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "            print('Stop at iteration:', epoch_num)\n",
    "            detection(np.sum(malicious_scores[-window_size:], axis=0), nbyz, num_workers)\n",
    "            break\n",
    "\n",
    "    if epoch_num > (start_detection_epoch - window_size):\n",
    "        weight_record.append(weight - last_weight)\n",
    "        grad_record.append(agg_grads - last_grad)\n",
    "    \n",
    "    if (len(weight_record) > window_size):\n",
    "        del weight_record[0]\n",
    "        del grad_record[0]\n",
    "    \n",
    "    last_weight = weight\n",
    "    last_grad = agg_grads\n",
    "    old_grad_list = user_grads\n",
    "    good_old_grads = good_current_grads\n",
    "    del user_grads\n",
    "\n",
    "    model_received = model_received + global_lr * agg_grads\n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    \n",
    "    best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "    accs_per_round.append(val_acc.cpu().item())\n",
    "    loss_per_round.append(val_loss.cpu().item())\n",
    "    \n",
    "    if epoch_num%1==0 or epoch_num==nepochs-1:\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "    epoch_num+=1\n",
    "\n",
    "# final_accs_per_client=[]\n",
    "# for i in range(num_workers):\n",
    "#     client_loss, client_acc = test(test_loaders[i][1], fed_model, criterion)\n",
    "#     final_accs_per_client.append(client_acc)\n",
    "# results = collections.OrderedDict(\n",
    "#     final_accs_per_client=np.array(final_accs_per_client),\n",
    "#     accs_per_round=np.array(accs_per_round),\n",
    "#     best_accs_per_round=np.array(best_accs_per_round),\n",
    "#     loss_per_round=np.array(loss_per_round)\n",
    "# )\n",
    "# pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_cifar10_fast_trmean_trim20.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f0e29",
   "metadata": {},
   "source": [
    "# TrMean-FLD + m=20% + Fang0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8659d0e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 11 benign_norm 275.926 val loss 1.944 val acc 0.272 best val_acc 0.272\n",
      "e 12 benign_norm 277.947 val loss 2.101 val acc 0.258 best val_acc 0.272\n",
      "e 13 benign_norm 280.003 val loss 1.974 val acc 0.278 best val_acc 0.278\n",
      "e 14 benign_norm 279.630 val loss 1.998 val acc 0.297 best val_acc 0.297\n",
      "e 15 benign_norm 279.360 val loss 2.104 val acc 0.278 best val_acc 0.297\n",
      "e 16 benign_norm 279.709 val loss 2.149 val acc 0.290 best val_acc 0.297\n",
      "e 17 benign_norm 279.903 val loss 2.373 val acc 0.269 best val_acc 0.297\n",
      "e 18 benign_norm 280.937 val loss 2.408 val acc 0.269 best val_acc 0.297\n",
      "e 19 benign_norm 280.945 val loss 2.294 val acc 0.284 best val_acc 0.297\n",
      "e 20 benign_norm 282.025 val loss 2.107 val acc 0.323 best val_acc 0.323\n",
      "e 21 benign_norm 283.073 val loss 2.012 val acc 0.330 best val_acc 0.330\n",
      "e 22 benign_norm 283.225 val loss 1.941 val acc 0.349 best val_acc 0.349\n",
      "e 23 benign_norm 284.876 val loss 1.870 val acc 0.372 best val_acc 0.372\n",
      "e 24 benign_norm 285.311 val loss 1.839 val acc 0.381 best val_acc 0.381\n",
      "e 25 benign_norm 287.395 val loss 1.916 val acc 0.370 best val_acc 0.381\n",
      "e 26 benign_norm 288.197 val loss 1.863 val acc 0.387 best val_acc 0.387\n",
      "e 27 benign_norm 290.098 val loss 1.689 val acc 0.424 best val_acc 0.424\n",
      "e 28 benign_norm 292.200 val loss 1.923 val acc 0.392 best val_acc 0.424\n",
      "e 29 benign_norm 294.460 val loss 1.593 val acc 0.445 best val_acc 0.445\n",
      "e 30 benign_norm 294.716 val loss 1.638 val acc 0.440 best val_acc 0.445\n",
      "e 31 benign_norm 299.728 val loss 1.620 val acc 0.445 best val_acc 0.445\n",
      "e 32 benign_norm 301.876 val loss 1.540 val acc 0.458 best val_acc 0.458\n",
      "e 33 benign_norm 304.467 val loss 1.562 val acc 0.456 best val_acc 0.458\n",
      "e 34 benign_norm 305.178 val loss 1.635 val acc 0.443 best val_acc 0.458\n",
      "e 35 benign_norm 312.477 val loss 1.517 val acc 0.471 best val_acc 0.471\n",
      "e 36 benign_norm 317.810 val loss 1.621 val acc 0.459 best val_acc 0.471\n",
      "e 37 benign_norm 321.608 val loss 1.657 val acc 0.448 best val_acc 0.471\n",
      "e 38 benign_norm 322.329 val loss 1.577 val acc 0.465 best val_acc 0.471\n",
      "e 39 benign_norm 328.295 val loss 1.744 val acc 0.439 best val_acc 0.471\n",
      "e 40 benign_norm 329.116 val loss 1.544 val acc 0.473 best val_acc 0.473\n",
      "e 41 benign_norm 337.061 val loss 1.431 val acc 0.496 best val_acc 0.496\n",
      "e 42 benign_norm 348.631 val loss 1.645 val acc 0.454 best val_acc 0.496\n",
      "e 43 benign_norm 345.544 val loss 1.502 val acc 0.480 best val_acc 0.496\n",
      "e 44 benign_norm 347.944 val loss 1.653 val acc 0.450 best val_acc 0.496\n",
      "e 45 benign_norm 353.358 val loss 1.517 val acc 0.475 best val_acc 0.496\n",
      "e 46 benign_norm 361.789 val loss 1.669 val acc 0.453 best val_acc 0.496\n",
      "e 47 benign_norm 366.549 val loss 1.596 val acc 0.474 best val_acc 0.496\n",
      "e 48 benign_norm 373.767 val loss 1.631 val acc 0.457 best val_acc 0.496\n",
      "e 49 benign_norm 393.033 val loss 1.560 val acc 0.469 best val_acc 0.496\n",
      "e 50 benign_norm 396.694 val loss 1.654 val acc 0.464 best val_acc 0.496\n",
      "e 51 benign_norm 415.622 val loss 1.534 val acc 0.470 best val_acc 0.496\n",
      "e 52 benign_norm 394.080 val loss 1.579 val acc 0.460 best val_acc 0.496\n",
      "e 53 benign_norm 418.095 val loss 1.597 val acc 0.463 best val_acc 0.496\n",
      "e 54 benign_norm 454.719 val loss 1.433 val acc 0.491 best val_acc 0.496\n",
      "e 55 benign_norm 442.078 val loss 1.744 val acc 0.449 best val_acc 0.496\n",
      "e 56 benign_norm 457.010 val loss 1.712 val acc 0.452 best val_acc 0.496\n",
      "e 57 benign_norm 459.544 val loss 1.584 val acc 0.457 best val_acc 0.496\n",
      "e 58 benign_norm 460.328 val loss 1.623 val acc 0.457 best val_acc 0.496\n",
      "e 59 benign_norm 500.056 val loss 1.533 val acc 0.476 best val_acc 0.496\n",
      "e 60 benign_norm 512.661 val loss 1.584 val acc 0.470 best val_acc 0.496\n",
      "e 61 benign_norm 513.835 val loss 1.672 val acc 0.454 best val_acc 0.496\n",
      "e 62 benign_norm 560.103 val loss 1.516 val acc 0.480 best val_acc 0.496\n",
      "e 63 benign_norm 571.464 val loss 1.597 val acc 0.469 best val_acc 0.496\n",
      "e 64 benign_norm 571.169 val loss 1.810 val acc 0.439 best val_acc 0.496\n",
      "e 65 benign_norm 584.436 val loss 1.478 val acc 0.493 best val_acc 0.496\n",
      "e 66 benign_norm 597.987 val loss 1.570 val acc 0.478 best val_acc 0.496\n",
      "e 67 benign_norm 605.625 val loss 1.515 val acc 0.480 best val_acc 0.496\n",
      "e 68 benign_norm 590.621 val loss 1.552 val acc 0.471 best val_acc 0.496\n",
      "e 69 benign_norm 611.080 val loss 1.507 val acc 0.483 best val_acc 0.496\n",
      "e 70 benign_norm 632.294 val loss 1.539 val acc 0.479 best val_acc 0.496\n",
      "e 71 benign_norm 680.171 val loss 1.450 val acc 0.490 best val_acc 0.496\n",
      "e 72 benign_norm 651.767 val loss 1.567 val acc 0.473 best val_acc 0.496\n",
      "e 73 benign_norm 710.649 val loss 1.584 val acc 0.476 best val_acc 0.496\n",
      "e 74 benign_norm 732.174 val loss 1.585 val acc 0.472 best val_acc 0.496\n",
      "e 75 benign_norm 767.146 val loss 1.567 val acc 0.469 best val_acc 0.496\n",
      "e 76 benign_norm 735.501 val loss 1.516 val acc 0.471 best val_acc 0.496\n",
      "e 77 benign_norm 705.419 val loss 1.565 val acc 0.472 best val_acc 0.496\n",
      "e 78 benign_norm 732.392 val loss 1.471 val acc 0.485 best val_acc 0.496\n",
      "e 79 benign_norm 817.991 val loss 1.445 val acc 0.493 best val_acc 0.496\n",
      "e 80 benign_norm 776.432 val loss 1.469 val acc 0.486 best val_acc 0.496\n",
      "e 81 benign_norm 840.803 val loss 1.403 val acc 0.498 best val_acc 0.498\n",
      "e 82 benign_norm 842.476 val loss 1.577 val acc 0.463 best val_acc 0.498\n",
      "e 83 benign_norm 898.517 val loss 1.513 val acc 0.480 best val_acc 0.498\n",
      "e 84 benign_norm 889.572 val loss 1.473 val acc 0.481 best val_acc 0.498\n",
      "e 85 benign_norm 961.986 val loss 1.483 val acc 0.488 best val_acc 0.498\n",
      "e 86 benign_norm 969.259 val loss 1.414 val acc 0.506 best val_acc 0.506\n",
      "e 87 benign_norm 1048.253 val loss 1.437 val acc 0.490 best val_acc 0.506\n",
      "e 88 benign_norm 961.852 val loss 1.551 val acc 0.478 best val_acc 0.506\n",
      "e 89 benign_norm 1070.721 val loss 1.396 val acc 0.513 best val_acc 0.513\n",
      "e 90 benign_norm 1073.927 val loss 1.407 val acc 0.490 best val_acc 0.513\n",
      "e 91 benign_norm 1051.288 val loss 1.399 val acc 0.501 best val_acc 0.513\n",
      "e 92 benign_norm 1064.811 val loss 1.339 val acc 0.519 best val_acc 0.519\n",
      "e 93 benign_norm 1124.537 val loss 1.365 val acc 0.510 best val_acc 0.519\n",
      "e 94 benign_norm 1298.953 val loss 1.421 val acc 0.492 best val_acc 0.519\n",
      "e 95 benign_norm 1308.894 val loss 1.444 val acc 0.494 best val_acc 0.519\n",
      "e 96 benign_norm 1193.672 val loss 1.514 val acc 0.466 best val_acc 0.519\n",
      "e 97 benign_norm 1269.030 val loss 1.415 val acc 0.499 best val_acc 0.519\n",
      "e 98 benign_norm 1307.288 val loss 1.342 val acc 0.540 best val_acc 0.540\n",
      "e 99 benign_norm 1166.193 val loss 1.472 val acc 0.477 best val_acc 0.540\n",
      "e 100 benign_norm 1379.511 val loss 1.282 val acc 0.539 best val_acc 0.540\n",
      "e 101 benign_norm 1255.723 val loss 1.491 val acc 0.483 best val_acc 0.540\n",
      "e 102 benign_norm 1219.354 val loss 1.352 val acc 0.521 best val_acc 0.540\n",
      "e 103 benign_norm 1339.397 val loss 1.400 val acc 0.519 best val_acc 0.540\n",
      "e 104 benign_norm 1501.679 val loss 1.324 val acc 0.528 best val_acc 0.540\n",
      "e 105 benign_norm 1358.495 val loss 1.302 val acc 0.538 best val_acc 0.540\n",
      "e 106 benign_norm 1296.638 val loss 1.396 val acc 0.515 best val_acc 0.540\n",
      "e 107 benign_norm 1452.960 val loss 1.324 val acc 0.529 best val_acc 0.540\n",
      "e 108 benign_norm 1498.969 val loss 1.395 val acc 0.512 best val_acc 0.540\n",
      "e 109 benign_norm 1555.156 val loss 1.340 val acc 0.523 best val_acc 0.540\n",
      "e 110 benign_norm 1571.469 val loss 1.334 val acc 0.529 best val_acc 0.540\n",
      "e 111 benign_norm 1544.780 val loss 1.413 val acc 0.498 best val_acc 0.540\n",
      "e 112 benign_norm 1708.061 val loss 1.314 val acc 0.536 best val_acc 0.540\n",
      "e 113 benign_norm 1602.617 val loss 1.331 val acc 0.534 best val_acc 0.540\n",
      "e 114 benign_norm 1768.761 val loss 1.255 val acc 0.555 best val_acc 0.555\n",
      "e 115 benign_norm 1753.501 val loss 1.258 val acc 0.555 best val_acc 0.555\n",
      "e 116 benign_norm 1601.939 val loss 1.396 val acc 0.522 best val_acc 0.555\n",
      "e 117 benign_norm 2003.109 val loss 1.269 val acc 0.554 best val_acc 0.555\n",
      "e 118 benign_norm 1822.547 val loss 1.333 val acc 0.543 best val_acc 0.555\n",
      "e 119 benign_norm 1844.243 val loss 1.235 val acc 0.560 best val_acc 0.560\n",
      "e 120 benign_norm 1999.796 val loss 1.413 val acc 0.495 best val_acc 0.560\n",
      "e 121 benign_norm 1934.074 val loss 1.319 val acc 0.539 best val_acc 0.560\n",
      "e 122 benign_norm 1893.732 val loss 1.269 val acc 0.558 best val_acc 0.560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 123 benign_norm 2488.839 val loss 1.258 val acc 0.555 best val_acc 0.560\n",
      "e 124 benign_norm 2167.619 val loss 1.296 val acc 0.544 best val_acc 0.560\n",
      "e 125 benign_norm 2246.414 val loss 1.328 val acc 0.524 best val_acc 0.560\n",
      "e 126 benign_norm 2017.557 val loss 1.337 val acc 0.535 best val_acc 0.560\n",
      "e 127 benign_norm 2466.261 val loss 1.273 val acc 0.545 best val_acc 0.560\n",
      "e 128 benign_norm 2185.093 val loss 1.249 val acc 0.559 best val_acc 0.560\n",
      "e 129 benign_norm 2132.378 val loss 1.283 val acc 0.556 best val_acc 0.560\n",
      "e 130 benign_norm 2908.978 val loss 1.439 val acc 0.484 best val_acc 0.560\n",
      "e 131 benign_norm 2432.674 val loss 1.317 val acc 0.543 best val_acc 0.560\n",
      "e 132 benign_norm 2299.885 val loss 1.247 val acc 0.564 best val_acc 0.564\n",
      "e 133 benign_norm 2200.564 val loss 1.214 val acc 0.575 best val_acc 0.575\n",
      "e 134 benign_norm 2749.518 val loss 1.304 val acc 0.540 best val_acc 0.575\n",
      "e 135 benign_norm 2521.189 val loss 1.300 val acc 0.547 best val_acc 0.575\n",
      "e 136 benign_norm 2533.591 val loss 1.342 val acc 0.535 best val_acc 0.575\n",
      "e 137 benign_norm 3530.248 val loss 1.337 val acc 0.524 best val_acc 0.575\n",
      "e 138 benign_norm 2695.912 val loss 1.209 val acc 0.574 best val_acc 0.575\n",
      "e 139 benign_norm 2657.668 val loss 1.361 val acc 0.518 best val_acc 0.575\n",
      "e 140 benign_norm 2720.454 val loss 1.269 val acc 0.547 best val_acc 0.575\n",
      "e 141 benign_norm 2646.259 val loss 1.290 val acc 0.554 best val_acc 0.575\n",
      "e 143 benign_norm 2683.883 val loss 1.288 val acc 0.557 best val_acc 0.575\n",
      "e 144 benign_norm 2876.006 val loss 1.454 val acc 0.495 best val_acc 0.575\n",
      "e 145 benign_norm 2588.948 val loss 1.273 val acc 0.556 best val_acc 0.575\n",
      "e 146 benign_norm 3386.204 val loss 1.436 val acc 0.506 best val_acc 0.575\n",
      "e 147 benign_norm 3855.649 val loss 1.380 val acc 0.521 best val_acc 0.575\n",
      "e 148 benign_norm 3022.617 val loss 1.247 val acc 0.566 best val_acc 0.575\n",
      "e 149 benign_norm 3174.052 val loss 1.334 val acc 0.541 best val_acc 0.575\n",
      "e 150 benign_norm 3192.533 val loss 1.360 val acc 0.526 best val_acc 0.575\n",
      "e 151 benign_norm 3370.717 val loss 1.274 val acc 0.552 best val_acc 0.575\n",
      "e 152 benign_norm 3735.298 val loss 1.320 val acc 0.541 best val_acc 0.575\n",
      "e 153 benign_norm 3385.301 val loss 1.323 val acc 0.539 best val_acc 0.575\n",
      "e 154 benign_norm 4428.313 val loss 1.387 val acc 0.517 best val_acc 0.575\n",
      "e 155 benign_norm 3528.118 val loss 1.237 val acc 0.565 best val_acc 0.575\n",
      "e 156 benign_norm 3646.688 val loss 1.377 val acc 0.526 best val_acc 0.575\n",
      "e 157 benign_norm 4248.193 val loss 1.420 val acc 0.506 best val_acc 0.575\n",
      "e 158 benign_norm 4868.038 val loss 1.424 val acc 0.505 best val_acc 0.575\n",
      "e 159 benign_norm 4846.774 val loss 1.252 val acc 0.558 best val_acc 0.575\n",
      "e 160 benign_norm 4076.580 val loss 1.266 val acc 0.547 best val_acc 0.575\n",
      "e 161 benign_norm 5295.352 val loss 1.334 val acc 0.528 best val_acc 0.575\n",
      "Attack Detected!\n",
      "Stop at iteration: 162\n",
      "acc 0.7800; recall 0.0000; fpr 0.0250; fnr 1.0000;\n",
      "0.9252696515789797\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.1\n",
    "global_lr = 1\n",
    "nbyz = 20\n",
    "agr = 'tr_mean'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "# FLDetector initializations\n",
    "weight_record = []\n",
    "grad_record = []\n",
    "test_grads = []\n",
    "old_grad_list = []\n",
    "malicious_scores = np.zeros((1, num_workers))\n",
    "start_detection_epoch = 10\n",
    "window_size = 10\n",
    "assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "# Adaptive attack initializers\n",
    "good_distance_range = np.zeros((1, nbyz))\n",
    "attack_type = 'NDSS21'\n",
    "dev_type = 'std'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_grads=[]\n",
    "    benign_norm = 0\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "    weight = model_received\n",
    "\n",
    "    if (epoch_num > start_detection_epoch):\n",
    "        hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "        hvp = np.squeeze(hvp)\n",
    "    else:\n",
    "        hvp = None\n",
    "\n",
    "    good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "    if hvp is not None:\n",
    "        pred_grad = copy.deepcopy(good_old_grads)\n",
    "        distance = []\n",
    "        for i in range(len(good_old_grads)):\n",
    "            pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "        good_distance_range = np.concatenate(\n",
    "            (good_distance_range, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "        \n",
    "    if attack_type != 'none' and (epoch_num < start_detection_epoch):\n",
    "        user_grads = full_trim(user_grads, nbyz)\n",
    "        # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "    elif epoch_num > start_detection_epoch:\n",
    "        if attack_type == 'full_trim':\n",
    "            user_grads = full_trim(user_grads, nbyz)\n",
    "        elif attack_type == 'none':\n",
    "            pass\n",
    "        else:\n",
    "            if attack_type == 'NDSS21':\n",
    "                distance_bound = np.random.choice(np.mean(good_distance_range[-1:], 0))\n",
    "                model_re = torch.mean(good_current_grads, dim=0)\n",
    "                if dev_type == 'unit_vec':\n",
    "                    deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                elif dev_type == 'sign':\n",
    "                    deviation = torch.sign(model_re)\n",
    "                elif dev_type == 'std':\n",
    "                    deviation = torch.std(good_current_grads, 0)\n",
    "                # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_range[-1]))) / torch.norm(deviation))\n",
    "                noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "            elif attack_type == 'mod_trim':\n",
    "                mal_grads= full_trim(user_grads[:nbyz], nbyz)\n",
    "                pass\n",
    "            else:\n",
    "                noise = torch.zeros(hvp.shape).to(device)\n",
    "            for m in range(nbyz):\n",
    "                user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "    agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "    \n",
    "    if distance is not None and epoch_num > (start_detection_epoch - window_size):\n",
    "        malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "    if malicious_scores.shape[0] >= window_size+1:\n",
    "        if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "            print('Stop at iteration:', epoch_num)\n",
    "            detection(np.sum(malicious_scores[-window_size:], axis=0), nbyz, len(user_grads))\n",
    "            break\n",
    "\n",
    "    if epoch_num > (start_detection_epoch - window_size):\n",
    "        weight_record.append(weight - last_weight)\n",
    "        grad_record.append(agg_grads - last_grad)\n",
    "    \n",
    "    if (len(weight_record) > window_size):\n",
    "        del weight_record[0]\n",
    "        del grad_record[0]\n",
    "    \n",
    "    last_weight = weight\n",
    "    last_grad = agg_grads\n",
    "    old_grad_list = user_grads\n",
    "    good_old_grads = good_current_grads\n",
    "    del user_grads\n",
    "\n",
    "    model_received = model_received + global_lr * agg_grads\n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    \n",
    "    best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "    accs_per_round.append(val_acc.cpu().item())\n",
    "    loss_per_round.append(val_loss.cpu().item())\n",
    "    \n",
    "    if epoch_num%1==0 or epoch_num==nepochs-1:\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bca7b3b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r 0 nmal 20 | e 0 benign_norm 1116.596 val loss 2.312 val acc 0.099 best val_acc 0.099\n",
      "r 0 nmal 20 | e 5 benign_norm 282.334 val loss 2.342 val acc 0.099 best val_acc 0.099\n",
      "r 0 nmal 20 | e 10 benign_norm 277.146 val loss 1.887 val acc 0.283 best val_acc 0.283\n",
      "r 0 nmal 20 | e 15 benign_norm 278.692 val loss 2.663 val acc 0.276 best val_acc 0.284\n",
      "r 0 nmal 20 | e 20 benign_norm 281.196 val loss 2.135 val acc 0.343 best val_acc 0.343\n",
      "r 0 nmal 20 | e 25 benign_norm 287.255 val loss 1.648 val acc 0.431 best val_acc 0.431\n",
      "r 0 nmal 20 | e 30 benign_norm 296.231 val loss 1.554 val acc 0.457 best val_acc 0.460\n",
      "r 0 nmal 20 | e 35 benign_norm 310.453 val loss 1.451 val acc 0.482 best val_acc 0.482\n",
      "r 0 nmal 20 | e 40 benign_norm 336.919 val loss 1.471 val acc 0.481 best val_acc 0.487\n",
      "r 0 nmal 20 | e 45 benign_norm 357.224 val loss 1.503 val acc 0.474 best val_acc 0.487\n",
      "r 0 nmal 20 | e 50 benign_norm 395.278 val loss 1.521 val acc 0.466 best val_acc 0.490\n",
      "r 0 nmal 20 | e 55 benign_norm 454.854 val loss 1.382 val acc 0.501 best val_acc 0.501\n",
      "r 0 nmal 20 | e 60 benign_norm 499.992 val loss 1.514 val acc 0.475 best val_acc 0.507\n",
      "r 0 nmal 20 | e 65 benign_norm 555.715 val loss 1.495 val acc 0.480 best val_acc 0.508\n",
      "r 0 nmal 20 | e 70 benign_norm 660.994 val loss 1.440 val acc 0.500 best val_acc 0.508\n",
      "r 0 nmal 20 | e 75 benign_norm 783.191 val loss 1.516 val acc 0.475 best val_acc 0.512\n",
      "r 0 nmal 20 | e 80 benign_norm 897.786 val loss 1.341 val acc 0.516 best val_acc 0.516\n",
      "r 0 nmal 20 | e 85 benign_norm 941.031 val loss 1.467 val acc 0.477 best val_acc 0.516\n",
      "r 0 nmal 20 | e 90 benign_norm 1054.228 val loss 1.466 val acc 0.478 best val_acc 0.516\n",
      "r 0 nmal 20 | e 95 benign_norm 1277.464 val loss 1.571 val acc 0.458 best val_acc 0.516\n",
      "r 0 nmal 20 | e 100 benign_norm 1372.568 val loss 1.418 val acc 0.503 best val_acc 0.516\n",
      "r 0 nmal 20 | e 105 benign_norm 1491.850 val loss 1.579 val acc 0.460 best val_acc 0.516\n",
      "r 0 nmal 20 | e 110 benign_norm 1795.176 val loss 1.428 val acc 0.495 best val_acc 0.516\n",
      "r 0 nmal 20 | e 115 benign_norm 2917.073 val loss 1.428 val acc 0.510 best val_acc 0.516\n",
      "!!!! Stop at iteration: 117\n",
      "r 0 nmal 20 | e 117 benign_norm 2117.979 val loss 1.779 val acc 0.442 best val_acc 0.516\n",
      "acc 0.7800; recall 0.0000; fpr 0.0250; fnr 1.0000; auc 0.4875\n",
      "r 1 nmal 20 | e 0 benign_norm 1051.635 val loss 2.315 val acc 0.099 best val_acc 0.099\n",
      "r 1 nmal 20 | e 5 benign_norm 285.245 val loss 2.324 val acc 0.121 best val_acc 0.121\n",
      "r 1 nmal 20 | e 10 benign_norm 280.357 val loss 2.058 val acc 0.208 best val_acc 0.208\n",
      "r 1 nmal 20 | e 15 benign_norm 277.773 val loss 1.878 val acc 0.343 best val_acc 0.348\n",
      "r 1 nmal 20 | e 20 benign_norm 279.958 val loss 1.745 val acc 0.391 best val_acc 0.391\n",
      "r 1 nmal 20 | e 25 benign_norm 284.815 val loss 1.781 val acc 0.398 best val_acc 0.401\n",
      "r 1 nmal 20 | e 30 benign_norm 293.323 val loss 1.762 val acc 0.412 best val_acc 0.424\n",
      "r 1 nmal 20 | e 35 benign_norm 303.196 val loss 1.704 val acc 0.432 best val_acc 0.439\n",
      "r 1 nmal 20 | e 40 benign_norm 318.091 val loss 1.882 val acc 0.385 best val_acc 0.439\n",
      "r 1 nmal 20 | e 45 benign_norm 353.763 val loss 1.655 val acc 0.438 best val_acc 0.439\n",
      "r 1 nmal 20 | e 50 benign_norm 381.483 val loss 1.701 val acc 0.434 best val_acc 0.451\n",
      "r 1 nmal 20 | e 55 benign_norm 437.804 val loss 1.714 val acc 0.442 best val_acc 0.457\n",
      "r 1 nmal 20 | e 60 benign_norm 487.185 val loss 1.618 val acc 0.466 best val_acc 0.466\n",
      "r 1 nmal 20 | e 65 benign_norm 554.016 val loss 1.761 val acc 0.445 best val_acc 0.481\n",
      "r 1 nmal 20 | e 70 benign_norm 655.877 val loss 1.939 val acc 0.436 best val_acc 0.481\n",
      "r 1 nmal 20 | e 75 benign_norm 697.091 val loss 1.735 val acc 0.465 best val_acc 0.481\n",
      "r 1 nmal 20 | e 80 benign_norm 786.399 val loss 1.830 val acc 0.456 best val_acc 0.499\n",
      "!!!! Stop at iteration: 85\n",
      "r 1 nmal 20 | e 85 benign_norm 867.339 val loss 1.882 val acc 0.455 best val_acc 0.499\n",
      "acc 0.8000; recall 1.0000; fpr 0.2500; fnr 0.0000; auc 0.8750\n",
      "r 2 nmal 20 | e 0 benign_norm 2661.370 val loss 2.318 val acc 0.099 best val_acc 0.099\n",
      "r 2 nmal 20 | e 5 benign_norm 286.620 val loss 2.320 val acc 0.106 best val_acc 0.106\n",
      "r 2 nmal 20 | e 10 benign_norm 279.323 val loss 1.957 val acc 0.237 best val_acc 0.237\n",
      "r 2 nmal 20 | e 15 benign_norm 278.070 val loss 1.856 val acc 0.348 best val_acc 0.361\n",
      "r 2 nmal 20 | e 20 benign_norm 280.070 val loss 2.209 val acc 0.301 best val_acc 0.361\n",
      "r 2 nmal 20 | e 25 benign_norm 285.391 val loss 1.827 val acc 0.384 best val_acc 0.384\n",
      "!!!! Stop at iteration: 29\n",
      "r 2 nmal 20 | e 29 benign_norm 291.720 val loss 1.858 val acc 0.386 best val_acc 0.386\n",
      "acc 0.9500; recall 1.0000; fpr 0.0625; fnr 0.0000; auc 0.9688\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_malicious = [20]\n",
    "n_runs = 3\n",
    "results={}\n",
    "for n_mal in n_malicious:\n",
    "    results[n_mal] = {'acc': [], 'fpr': [], 'fnr': [], 'auc': [], 'stop_e': []}\n",
    "    for run in range(n_runs):\n",
    "        torch.cuda.empty_cache()\n",
    "        nepochs=200\n",
    "        local_epochs = 2\n",
    "        batch_size = 16\n",
    "        num_workers = 100\n",
    "        local_lr = 0.1\n",
    "        global_lr = 1\n",
    "        agr = 'tr_mean'\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = resnet20().cuda()\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        best_accs_per_round = []\n",
    "        accs_per_round = []\n",
    "        loss_per_round = []\n",
    "        home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "        # FLDetector initializations\n",
    "        weight_record = []\n",
    "        grad_record = []\n",
    "        test_grads = []\n",
    "        old_grad_list = []\n",
    "        malicious_scores = np.zeros((1, num_workers-(20-nbyz)))\n",
    "        start_detection_epoch = 10\n",
    "        window_size = 10\n",
    "        assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "        # Adaptive attack initializers\n",
    "        good_distance_range = np.zeros((1, nbyz))\n",
    "        attack_type = 'NDSS21'\n",
    "        dev_type = 'std'\n",
    "        nbyz = n_mal\n",
    "        \n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(20-nbyz, num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_grads=[]\n",
    "            benign_norm = 0\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "            weight = model_received\n",
    "\n",
    "            if (epoch_num > start_detection_epoch):\n",
    "                hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "                hvp = np.squeeze(hvp)\n",
    "            else:\n",
    "                hvp = None\n",
    "\n",
    "            good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "            if hvp is not None:\n",
    "                pred_grad = copy.deepcopy(good_old_grads)\n",
    "                distance = []\n",
    "                for i in range(len(good_old_grads)):\n",
    "                    pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "                good_distance_range = np.concatenate(\n",
    "                    (good_distance_range, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "            if attack_type != 'none' and (epoch_num < start_detection_epoch):\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "                # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "            elif epoch_num > start_detection_epoch:\n",
    "                if attack_type == 'full_trim':\n",
    "                    user_grads = full_trim(user_grads, nbyz)\n",
    "                elif attack_type == 'none':\n",
    "                    pass\n",
    "                else:\n",
    "                    if attack_type == 'NDSS21':\n",
    "                        distance_bound = np.random.choice(np.mean(good_distance_range[-1:], 0))\n",
    "                        model_re = torch.mean(good_current_grads, dim=0)\n",
    "                        if dev_type == 'unit_vec':\n",
    "                            deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                        elif dev_type == 'sign':\n",
    "                            deviation = torch.sign(model_re)\n",
    "                        elif dev_type == 'std':\n",
    "                            deviation = torch.std(good_current_grads, 0)\n",
    "                        # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_range[-1]))) / torch.norm(deviation))\n",
    "                        noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                    elif attack_type == 'mod_trim':\n",
    "                        mal_grads= full_trim(user_grads[:nbyz], nbyz)\n",
    "                        pass\n",
    "                    else:\n",
    "                        noise = torch.zeros(hvp.shape).to(device)\n",
    "                    for m in range(nbyz):\n",
    "                        user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "            agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "            if distance is not None and epoch_num > (start_detection_epoch - window_size):\n",
    "                malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "            if malicious_scores.shape[0] >= window_size+1:\n",
    "                if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                    print('!!!! Stop at iteration:', epoch_num)\n",
    "                    print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "                    acc, fpr, fnr, auc = detection(\n",
    "                        np.sum(malicious_scores[-window_size:], axis=0), nbyz, len(user_grads))\n",
    "                    results[n_mal]['acc'].append(acc)\n",
    "                    results[n_mal]['fpr'].append(fpr)\n",
    "                    results[n_mal]['fnr'].append(fnr)\n",
    "                    results[n_mal]['auc'].append(auc)\n",
    "                    results[n_mal]['stop_e'].append(epoch_num)\n",
    "                    break\n",
    "                    break\n",
    "\n",
    "            if epoch_num > (start_detection_epoch - window_size):\n",
    "                weight_record.append(weight - last_weight)\n",
    "                grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "            if (len(weight_record) > window_size):\n",
    "                del weight_record[0]\n",
    "                del grad_record[0]\n",
    "\n",
    "            last_weight = weight\n",
    "            last_grad = agg_grads\n",
    "            old_grad_list = user_grads\n",
    "            good_old_grads = good_current_grads\n",
    "            del user_grads\n",
    "\n",
    "            model_received = model_received + global_lr * agg_grads\n",
    "            fed_model = resnet20().cuda()\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "            best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "            accs_per_round.append(val_acc.cpu().item())\n",
    "            loss_per_round.append(val_loss.cpu().item())\n",
    "\n",
    "            if epoch_num%5==0 or epoch_num==nepochs-1:\n",
    "                print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "            if math.isnan(val_loss) or val_loss > 100000:\n",
    "                print('val loss %f... exit'%val_loss)\n",
    "                break\n",
    "            epoch_num+=1\n",
    "\n",
    "        final_accs_per_client=[]\n",
    "        for i in range(num_workers):\n",
    "            client_loss, client_acc = test(test_loaders[i][1], fed_model, criterion)\n",
    "            final_accs_per_client.append(client_acc.cpu().item())\n",
    "        all_results = collections.OrderedDict(\n",
    "            final_accs_per_client=np.array(final_accs_per_client),\n",
    "            accs_per_round=np.array(accs_per_round),\n",
    "            best_accs_per_round=np.array(best_accs_per_round),\n",
    "            loss_per_round=np.array(loss_per_round),\n",
    "            results=results\n",
    "        )\n",
    "        pickle.dump(all_results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq4_adaptive_cifar10_fast_trmean_NDSS21_std_m%d_r%d.pkl' % (nbyz, run)), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a4141",
   "metadata": {},
   "source": [
    "# Load Dirichlet0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67429366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 10043)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "distribution='dirichlet'\n",
    "param = .1\n",
    "force = False\n",
    "\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=100, distribution=distribution, param=param, force=force)\n",
    "\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    train_loaders.append((pos, get_train(all_data, indices, batch_size)))\n",
    "test_loaders = []\n",
    "for pos, indices in enumerate(each_worker_te_idx):\n",
    "    test_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "len(train_loaders), len(test_loaders), len(global_test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d93c6",
   "metadata": {},
   "source": [
    "# Mean with no attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df1984c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 val loss 2.365 val acc 0.101 best val_acc 0.101\n",
      "e 10 val loss 2.090 val acc 0.228 best val_acc 0.255\n",
      "e 20 val loss 1.836 val acc 0.370 best val_acc 0.371\n",
      "e 30 val loss 1.699 val acc 0.445 best val_acc 0.445\n",
      "e 40 val loss 1.608 val acc 0.472 best val_acc 0.492\n",
      "e 50 val loss 1.498 val acc 0.521 best val_acc 0.570\n",
      "e 60 val loss 1.401 val acc 0.557 best val_acc 0.601\n",
      "e 70 val loss 1.292 val acc 0.585 best val_acc 0.609\n",
      "e 80 val loss 1.196 val acc 0.621 best val_acc 0.621\n",
      "e 90 val loss 1.137 val acc 0.650 best val_acc 0.650\n",
      "e 100 val loss 1.100 val acc 0.662 best val_acc 0.678\n",
      "e 110 val loss 1.053 val acc 0.677 best val_acc 0.678\n",
      "e 120 val loss 1.032 val acc 0.683 best val_acc 0.691\n",
      "e 130 val loss 1.023 val acc 0.678 best val_acc 0.694\n",
      "e 140 val loss 0.972 val acc 0.700 best val_acc 0.700\n",
      "e 150 val loss 0.911 val acc 0.719 best val_acc 0.719\n",
      "e 160 val loss 0.882 val acc 0.718 best val_acc 0.720\n",
      "e 170 val loss 0.874 val acc 0.718 best val_acc 0.720\n",
      "e 180 val loss 0.846 val acc 0.727 best val_acc 0.737\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 32\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    train_loaders.append((pos, get_train(all_data, indices, batch_size)))\n",
    "num_workers = 100\n",
    "local_lr = 0.1\n",
    "global_lr = 1\n",
    "agr = 'average'\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "nbyz = 0\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(20-nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_grads=[]\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "    agg_grads = torch.mean(user_grads, 0)\n",
    "    del user_grads\n",
    "    model_received = model_received + global_lr * agg_grads\n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    if is_best:\n",
    "        best_model = copy.deepcopy(fed_model)\n",
    "\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, val_loss, val_acc, best_global_acc))\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516c845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 benign_norm 1110.241 val loss 2.312 val acc 0.100 best val_acc 0.100\n",
      "e 10 benign_norm 279.361 val loss 1.987 val acc 0.255 best val_acc 0.255\n",
      "e 20 benign_norm 276.344 val loss 1.675 val acc 0.422 best val_acc 0.422\n",
      "e 30 benign_norm 276.152 val loss 1.487 val acc 0.495 best val_acc 0.496\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.1\n",
    "global_lr = 1\n",
    "nbyz = 0\n",
    "agr = 'average'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "# FLDetector initializations\n",
    "weight_record = []\n",
    "grad_record = []\n",
    "test_grads = []\n",
    "old_grad_list = []\n",
    "malicious_scores = np.zeros((1, num_workers))\n",
    "start_detection_epoch = 10\n",
    "window_size = 10\n",
    "assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "# Adaptive attack initializers\n",
    "good_distance_rage = np.zeros((1, nbyz))\n",
    "attack_type = 'none'\n",
    "dev_type = 'std'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_grads=[]\n",
    "    benign_norm = 0\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "    weight = model_received\n",
    "\n",
    "    if (epoch_num > start_detection_epoch):\n",
    "        hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "        hvp = np.squeeze(hvp)\n",
    "    else:\n",
    "        hvp = None\n",
    "\n",
    "    good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "    if hvp is not None:\n",
    "        pred_grad = copy.deepcopy(good_old_grads)\n",
    "        distance = []\n",
    "        for i in range(len(good_old_grads)):\n",
    "            pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "        good_distance_rage = np.concatenate(\n",
    "            (good_distance_rage, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "        \n",
    "    if attack_type != 'none' and (epoch_num < start_detection_epoch):\n",
    "        user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "    elif epoch_num > start_detection_epoch:\n",
    "        if attack_type == 'full_trim':\n",
    "            user_grads = full_trim(user_grads, nbyz)\n",
    "        elif attack_type == 'none':\n",
    "            pass\n",
    "        else:\n",
    "            if attack_type == 'LIE':\n",
    "                print(\"LIE\")\n",
    "                z = 0.1\n",
    "                noise_avg = torch.mean(user_grads[:nbyz], dim=0)\n",
    "                noise_std = torch.std(user_grads[:nbyz], dim=0)\n",
    "                noise = noise_avg + z * noise_std\n",
    "            elif attack_type == 'NDSS21':\n",
    "                distance_bound = np.random.choice(np.mean(good_distance_rage[-1:], 0))\n",
    "                model_re = torch.mean(good_current_grads, dim=0)\n",
    "                if dev_type == 'unit_vec':\n",
    "                    deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                elif dev_type == 'sign':\n",
    "                    deviation = torch.sign(model_re)\n",
    "                elif dev_type == 'std':\n",
    "                    deviation = torch.std(good_current_grads, 0)\n",
    "                # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_rage[-1]))) / torch.norm(deviation))\n",
    "                noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "            elif attack_type == 'mod_trim':\n",
    "                mal_grads= full_trim(user_grads[:nbyz], nbyz)\n",
    "                pass\n",
    "            else:\n",
    "                noise = torch.zeros(hvp.shape).to(device)\n",
    "            for m in range(nbyz):\n",
    "                user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "    agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "    \n",
    "    if distance is not None and epoch_num > (start_detection_epoch - window_size):\n",
    "        malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "    if malicious_scores.shape[0] >= window_size+1:\n",
    "        if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "            print('Stop at iteration:', epoch_num)\n",
    "            detection(np.sum(malicious_scores[-window_size:], axis=0), nbyz, num_workers)\n",
    "            break\n",
    "\n",
    "    if epoch_num > (start_detection_epoch - window_size):\n",
    "        weight_record.append(weight - last_weight)\n",
    "        grad_record.append(agg_grads - last_grad)\n",
    "    \n",
    "    if (len(weight_record) > window_size):\n",
    "        del weight_record[0]\n",
    "        del grad_record[0]\n",
    "    \n",
    "    last_weight = weight\n",
    "    last_grad = agg_grads\n",
    "    old_grad_list = user_grads\n",
    "    good_old_grads = good_current_grads\n",
    "    del user_grads\n",
    "\n",
    "    model_received = model_received + global_lr * agg_grads\n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    \n",
    "    best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "    accs_per_round.append(val_acc.cpu().item())\n",
    "    loss_per_round.append(val_loss.cpu().item())\n",
    "    \n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014dc668",
   "metadata": {},
   "source": [
    "# TrMean-FLD + m=20% + Dirichlet0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeeeff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011bf78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
