{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for FEMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f471eb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu083/4287636/ipykernel_72878/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "934d4b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'femnist'\n",
    "bias = 0.1\n",
    "net = 'cnn'\n",
    "batch_size = 32\n",
    "lr = 0.01\n",
    "num_workers = 300\n",
    "nepochs = 100\n",
    "gpu = 3\n",
    "seed = 41\n",
    "nbyz = 84\n",
    "byz_type = 'full_trim'\n",
    "aggregation = 'trim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ccb294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "user_tr_data = []\n",
    "user_tr_labels = []\n",
    "\n",
    "for i in range(34):\n",
    "    f = '/work/vshejwalkar_umass_edu/momin/leaf/data/femnist/data/train/all_data_%d_niid_0_keep_0_train_9.json'%i\n",
    "    with open(f, 'r') as myfile:\n",
    "        data=myfile.read()\n",
    "    obj = json.loads(data)\n",
    "    \n",
    "    for user in obj['users']:\n",
    "        user_tr_data.append(obj['user_data'][user]['x'])\n",
    "        user_tr_labels.append(obj['user_data'][user]['y'])\n",
    "\n",
    "user_te_data = []\n",
    "user_te_labels = []\n",
    "\n",
    "for i in range(34):\n",
    "    f = '/work/vshejwalkar_umass_edu/momin/leaf/data/femnist/data/test/all_data_%d_niid_0_keep_0_test_9.json'%i\n",
    "    with open(f, 'r') as myfile:\n",
    "        data=myfile.read()\n",
    "    obj = json.loads(data)\n",
    "    \n",
    "    for user in obj['users']:\n",
    "        user_te_data.append(obj['user_data'][user]['x'])\n",
    "        user_te_labels.append(obj['user_data'][user]['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4384e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_data = np.concatenate(user_te_data, 0)\n",
    "a = np.array(te_data)\n",
    "te_data = (a-np.mean(a,1)[:,None])/np.std(a,1)[:,None]\n",
    "te_labels = np.concatenate(user_te_labels)\n",
    "\n",
    "te_len = len(te_labels)\n",
    "if not os.path.exists('femnist_test_data_shuffler.pkl'):\n",
    "    a = np.arange(te_len)\n",
    "    np.random.shuffle(a)\n",
    "    pickle.dump(a, open('femnist_test_data_shuffler.pkl', 'wb'))\n",
    "else:\n",
    "    a = pickle.load(open('femnist_test_data_shuffler.pkl', 'rb'))\n",
    "te_data_tensor = torch.from_numpy(te_data[:(te_len//2)]).type(torch.FloatTensor)\n",
    "te_label_tensor = torch.from_numpy(te_labels[:(te_len//2)]).type(torch.LongTensor)\n",
    "\n",
    "val_data_tensor = torch.from_numpy(te_data[(te_len//2):]).type(torch.FloatTensor)\n",
    "val_label_tensor = torch.from_numpy(te_labels[(te_len//2):]).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc0bf424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user 1216 tr len 239 te len 27 \n",
      "user 978 tr len 203 te len 23 \n",
      "user 302 tr len 344 te len 39 \n",
      "user 368 tr len 305 te len 34 \n",
      "user 320 tr len 384 te len 43 \n",
      "user 1384 tr len 253 te len 29 \n",
      "user 138 tr len 333 te len 37 \n",
      "user 2416 tr len 153 te len 17 \n",
      "user 2686 tr len 165 te len 19 \n",
      "user 1410 tr len 321 te len 36 \n",
      "user 338 tr len 194 te len 22 \n",
      "user 2180 tr len 141 te len 16 \n",
      "user 2559 tr len 151 te len 17 \n",
      "user 2645 tr len 163 te len 19 \n",
      "user 2071 tr len 153 te len 18 \n",
      "user 490 tr len 235 te len 27 \n",
      "user 1130 tr len 69 te len 8 \n",
      "user 1107 tr len 272 te len 31 \n",
      "user 397 tr len 305 te len 34 \n",
      "user 951 tr len 275 te len 31 \n",
      "user 2299 tr len 148 te len 17 \n",
      "user 2808 tr len 145 te len 17 \n",
      "user 621 tr len 342 te len 38 \n",
      "user 2378 tr len 153 te len 17 \n",
      "user 2619 tr len 158 te len 18 \n",
      "user 3 tr len 228 te len 26 \n",
      "user 697 tr len 356 te len 40 \n",
      "user 2406 tr len 153 te len 18 \n",
      "user 2808 tr len 145 te len 17 \n",
      "user 542 tr len 372 te len 42 \n",
      "user 1437 tr len 242 te len 27 \n",
      "user 716 tr len 337 te len 38 \n",
      "user 1601 tr len 135 te len 15 \n",
      "user 1102 tr len 153 te len 18 \n",
      "user 2403 tr len 153 te len 17 \n",
      "user 1982 tr len 148 te len 17 \n",
      "user 774 tr len 303 te len 34 \n",
      "user 3268 tr len 83 te len 10 \n",
      "user 1736 tr len 151 te len 17 \n",
      "user 2306 tr len 160 te len 18 \n",
      "user 1361 tr len 166 te len 19 \n",
      "user 834 tr len 340 te len 38 \n",
      "user 775 tr len 316 te len 36 \n",
      "user 580 tr len 298 te len 34 \n",
      "user 3289 tr len 153 te len 18 \n",
      "user 2929 tr len 159 te len 18 \n",
      "user 576 tr len 265 te len 30 \n",
      "user 1232 tr len 162 te len 19 \n",
      "user 3254 tr len 161 te len 18 \n",
      "user 264 tr len 333 te len 37 \n",
      "user 1018 tr len 273 te len 31 \n",
      "user 1435 tr len 197 te len 22 \n",
      "user 1196 tr len 161 te len 18 \n",
      "user 1022 tr len 289 te len 33 \n",
      "user 2277 tr len 144 te len 16 \n",
      "user 3335 tr len 141 te len 16 \n",
      "user 2213 tr len 145 te len 17 \n",
      "user 2059 tr len 152 te len 17 \n",
      "user 2048 tr len 161 te len 18 \n",
      "user 1450 tr len 196 te len 22 \n",
      "user 132 tr len 267 te len 30 \n",
      "user 2026 tr len 143 te len 16 \n",
      "user 2160 tr len 138 te len 16 \n",
      "user 1781 tr len 127 te len 15 \n",
      "user 661 tr len 297 te len 34 \n",
      "user 1155 tr len 261 te len 30 \n",
      "user 1898 tr len 117 te len 13 \n",
      "user 79 tr len 291 te len 33 \n",
      "user 857 tr len 343 te len 39 \n",
      "user 1832 tr len 117 te len 13 \n",
      "user 737 tr len 341 te len 38 \n",
      "user 2267 tr len 151 te len 17 \n",
      "user 3268 tr len 83 te len 10 \n",
      "user 2737 tr len 157 te len 18 \n",
      "user 205 tr len 161 te len 18 \n",
      "user 3118 tr len 162 te len 18 \n",
      "user 1381 tr len 232 te len 26 \n",
      "user 2112 tr len 159 te len 18 \n",
      "user 1820 tr len 142 te len 16 \n",
      "user 1388 tr len 312 te len 35 \n",
      "user 25 tr len 363 te len 41 \n",
      "user 3088 tr len 158 te len 18 \n",
      "user 3383 tr len 159 te len 18 \n",
      "user 434 tr len 356 te len 40 \n",
      "user 2288 tr len 156 te len 18 \n",
      "user 1737 tr len 130 te len 15 \n",
      "user 1545 tr len 148 te len 17 \n",
      "user 1091 tr len 286 te len 32 \n",
      "user 1901 tr len 157 te len 18 \n",
      "user 3098 tr len 162 te len 18 \n",
      "user 1418 tr len 243 te len 28 \n",
      "user 2701 tr len 166 te len 19 \n",
      "user 2561 tr len 134 te len 15 \n",
      "user 1492 tr len 315 te len 36 \n",
      "user 198 tr len 324 te len 37 \n",
      "user 2397 tr len 144 te len 16 \n",
      "user 104 tr len 368 te len 41 \n",
      "user 1515 tr len 151 te len 17 \n",
      "user 3082 tr len 155 te len 18 \n",
      "user 2964 tr len 162 te len 18 \n",
      "user 645 tr len 272 te len 31 \n",
      "user 2691 tr len 162 te len 18 \n",
      "user 3255 tr len 162 te len 19 \n",
      "user 2326 tr len 142 te len 16 \n",
      "user 211 tr len 324 te len 36 \n",
      "user 2597 tr len 129 te len 15 \n",
      "user 1824 tr len 152 te len 17 \n",
      "user 2654 tr len 151 te len 17 \n",
      "user 527 tr len 212 te len 24 \n",
      "user 694 tr len 317 te len 36 \n",
      "user 2966 tr len 144 te len 16 \n",
      "user 3129 tr len 144 te len 16 \n",
      "user 1981 tr len 99 te len 11 \n",
      "user 530 tr len 333 te len 38 \n",
      "user 892 tr len 272 te len 31 \n",
      "user 2639 tr len 159 te len 18 \n",
      "user 2895 tr len 163 te len 19 \n",
      "user 2126 tr len 141 te len 16 \n",
      "user 2234 tr len 134 te len 15 \n",
      "user 369 tr len 344 te len 39 \n",
      "user 1401 tr len 313 te len 35 \n",
      "user 1416 tr len 264 te len 30 \n",
      "user 3015 tr len 163 te len 19 \n",
      "user 513 tr len 351 te len 39 \n",
      "user 3074 tr len 160 te len 18 \n",
      "user 263 tr len 191 te len 22 \n",
      "user 2560 tr len 145 te len 17 \n",
      "user 3118 tr len 162 te len 18 \n",
      "user 2583 tr len 127 te len 15 \n",
      "user 2915 tr len 161 te len 18 \n",
      "user 1058 tr len 288 te len 33 \n",
      "user 2374 tr len 142 te len 16 \n",
      "user 2368 tr len 136 te len 16 \n",
      "user 1424 tr len 186 te len 21 \n",
      "user 3123 tr len 154 te len 18 \n",
      "user 314 tr len 295 te len 33 \n",
      "user 1844 tr len 131 te len 15 \n",
      "user 1422 tr len 222 te len 25 \n",
      "user 3172 tr len 152 te len 17 \n",
      "user 847 tr len 366 te len 41 \n",
      "user 583 tr len 242 te len 27 \n",
      "user 510 tr len 257 te len 29 \n",
      "user 2527 tr len 155 te len 18 \n",
      "user 3382 tr len 144 te len 17 \n",
      "user 2411 tr len 153 te len 17 \n",
      "user 1746 tr len 138 te len 16 \n",
      "user 2707 tr len 157 te len 18 \n",
      "user 1108 tr len 114 te len 13 \n",
      "user 1813 tr len 126 te len 14 \n",
      "user 1430 tr len 181 te len 21 \n",
      "user 2530 tr len 158 te len 18 \n",
      "user 3216 tr len 159 te len 18 \n",
      "user 1184 tr len 309 te len 35 \n",
      "user 2080 tr len 97 te len 11 \n",
      "user 2737 tr len 157 te len 18 \n",
      "user 1792 tr len 124 te len 14 \n",
      "user 1366 tr len 254 te len 29 \n",
      "user 3399 tr len 163 te len 19 \n",
      "user 2970 tr len 162 te len 18 \n",
      "user 2780 tr len 155 te len 18 \n",
      "user 3390 tr len 162 te len 19 \n",
      "user 554 tr len 262 te len 30 \n",
      "user 2694 tr len 159 te len 18 \n",
      "user 2948 tr len 160 te len 18 \n",
      "user 2222 tr len 162 te len 19 \n",
      "user 88 tr len 273 te len 31 \n",
      "user 953 tr len 243 te len 28 \n",
      "user 2993 tr len 155 te len 18 \n",
      "user 2588 tr len 155 te len 18 \n",
      "user 549 tr len 366 te len 41 \n",
      "user 622 tr len 219 te len 25 \n",
      "user 1484 tr len 267 te len 30 \n",
      "user 737 tr len 341 te len 38 \n",
      "user 433 tr len 185 te len 21 \n",
      "user 1909 tr len 105 te len 12 \n",
      "user 565 tr len 277 te len 31 \n",
      "user 69 tr len 332 te len 37 \n",
      "user 2103 tr len 153 te len 17 \n",
      "user 117 tr len 253 te len 29 \n",
      "user 2483 tr len 152 te len 17 \n",
      "user 521 tr len 297 te len 34 \n",
      "user 1860 tr len 153 te len 18 \n",
      "user 3123 tr len 154 te len 18 \n",
      "user 815 tr len 276 te len 31 \n",
      "user 2352 tr len 134 te len 15 \n",
      "user 1426 tr len 237 te len 27 \n",
      "user 361 tr len 294 te len 33 \n",
      "user 1811 tr len 156 te len 18 \n",
      "user 2849 tr len 162 te len 18 \n",
      "user 1317 tr len 117 te len 14 \n",
      "user 188 tr len 294 te len 33 \n",
      "user 0 tr len 235 te len 27 \n",
      "user 2437 tr len 153 te len 18 \n",
      "user 5 tr len 360 te len 40 \n",
      "user 2454 tr len 162 te len 19 \n",
      "user 1419 tr len 144 te len 16 \n",
      "user 425 tr len 209 te len 24 \n",
      "user 1131 tr len 162 te len 18 \n",
      "user 1688 tr len 133 te len 15 \n",
      "user 2109 tr len 134 te len 15 \n",
      "user 3205 tr len 160 te len 18 \n",
      "user 368 tr len 305 te len 34 \n",
      "user 638 tr len 243 te len 27 \n",
      "user 1377 tr len 246 te len 28 \n",
      "user 963 tr len 359 te len 40 \n",
      "user 2657 tr len 162 te len 18 \n",
      "user 154 tr len 271 te len 31 \n",
      "user 2667 tr len 152 te len 17 \n",
      "user 2206 tr len 164 te len 19 \n",
      "user 2563 tr len 141 te len 16 \n",
      "user 2718 tr len 162 te len 19 \n",
      "user 2959 tr len 154 te len 18 \n",
      "user 1841 tr len 133 te len 15 \n",
      "user 2126 tr len 141 te len 16 \n",
      "user 2785 tr len 154 te len 18 \n",
      "user 1623 tr len 150 te len 17 \n",
      "user 2397 tr len 144 te len 16 \n",
      "user 2444 tr len 139 te len 16 \n",
      "user 2369 tr len 146 te len 17 \n",
      "user 838 tr len 243 te len 28 \n",
      "user 1503 tr len 144 te len 17 \n",
      "user 3122 tr len 164 te len 19 \n",
      "user 1762 tr len 119 te len 14 \n",
      "user 489 tr len 362 te len 41 \n",
      "user 2465 tr len 157 te len 18 \n",
      "user 964 tr len 331 te len 37 \n",
      "user 417 tr len 284 te len 32 \n",
      "user 1611 tr len 107 te len 12 \n",
      "user 242 tr len 342 te len 38 \n",
      "user 732 tr len 312 te len 35 \n",
      "user 205 tr len 161 te len 18 \n",
      "user 2726 tr len 163 te len 19 \n",
      "user 2468 tr len 161 te len 18 \n",
      "user 3017 tr len 159 te len 18 \n",
      "user 575 tr len 345 te len 39 \n",
      "user 1658 tr len 140 te len 16 \n",
      "user 2161 tr len 104 te len 12 \n",
      "user 3031 tr len 159 te len 18 \n",
      "user 857 tr len 343 te len 39 \n",
      "user 963 tr len 359 te len 40 \n",
      "user 7 tr len 343 te len 39 \n",
      "user 701 tr len 251 te len 28 \n",
      "user 3053 tr len 145 te len 17 \n",
      "user 2036 tr len 147 te len 17 \n",
      "user 2525 tr len 81 te len 10 \n",
      "user 2635 tr len 155 te len 18 \n",
      "user 733 tr len 313 te len 35 \n",
      "user 2114 tr len 121 te len 14 \n",
      "user 2669 tr len 163 te len 19 \n",
      "user 246 tr len 384 te len 43 \n",
      "user 1953 tr len 138 te len 16 \n",
      "user 1232 tr len 162 te len 19 \n",
      "user 1700 tr len 92 te len 11 \n",
      "user 457 tr len 353 te len 40 \n",
      "user 264 tr len 333 te len 37 \n",
      "user 3217 tr len 113 te len 13 \n",
      "user 109 tr len 287 te len 32 \n",
      "user 433 tr len 185 te len 21 \n",
      "user 2193 tr len 160 te len 18 \n",
      "user 1335 tr len 309 te len 35 \n",
      "user 588 tr len 256 te len 29 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user 3220 tr len 159 te len 18 \n",
      "user 3092 tr len 161 te len 18 \n",
      "user 3304 tr len 155 te len 18 \n",
      "user 2194 tr len 128 te len 15 \n",
      "user 21 tr len 282 te len 32 \n",
      "user 1229 tr len 255 te len 29 \n",
      "user 2674 tr len 140 te len 16 \n",
      "user 2688 tr len 161 te len 18 \n",
      "user 2908 tr len 157 te len 18 \n",
      "user 225 tr len 279 te len 31 \n",
      "user 1240 tr len 211 te len 24 \n",
      "user 3042 tr len 159 te len 18 \n",
      "user 616 tr len 302 te len 34 \n",
      "user 714 tr len 288 te len 33 \n",
      "user 3074 tr len 160 te len 18 \n",
      "user 174 tr len 375 te len 42 \n",
      "user 2595 tr len 147 te len 17 \n",
      "user 1255 tr len 259 te len 29 \n",
      "user 2257 tr len 158 te len 18 \n",
      "user 2463 tr len 159 te len 18 \n",
      "user 331 tr len 320 te len 36 \n",
      "user 599 tr len 325 te len 37 \n",
      "user 898 tr len 165 te len 19 \n",
      "user 1575 tr len 150 te len 17 \n",
      "user 1066 tr len 226 te len 26 \n",
      "user 81 tr len 299 te len 34 \n",
      "user 2370 tr len 146 te len 17 \n",
      "user 2063 tr len 104 te len 12 \n",
      "user 1184 tr len 309 te len 35 \n",
      "user 780 tr len 324 te len 36 \n",
      "user 1357 tr len 186 te len 21 \n",
      "user 3021 tr len 164 te len 19 \n",
      "user 2383 tr len 150 te len 17 \n",
      "user 1749 tr len 153 te len 17 \n",
      "user 1402 tr len 271 te len 31 \n",
      "user 552 tr len 260 te len 29 \n",
      "user 3074 tr len 160 te len 18 \n",
      "user 2615 tr len 149 te len 17 \n",
      "user 2392 tr len 150 te len 17 \n"
     ]
    }
   ],
   "source": [
    "each_worker_data=[]\n",
    "each_worker_label=[]\n",
    "\n",
    "each_worker_te_data=[]\n",
    "each_worker_te_label=[]\n",
    "\n",
    "if not os.path.exists('femnist_train_workers.pkl'):\n",
    "    worker_idx = np.random.choice (3400, num_workers)\n",
    "    pickle.dump(worker_idx, open('femnist_train_workers.pkl', 'wb'))\n",
    "else:\n",
    "    worker_idx = pickle.load(open('femnist_train_workers.pkl', 'rb'))\n",
    "    \n",
    "for i in worker_idx:\n",
    "    data = np.array(user_tr_data[i])\n",
    "    labels = np.array(user_tr_labels[i])\n",
    "    \n",
    "    te_data_ = np.array(user_te_data[i])\n",
    "    te_labels_ = np.array(user_te_labels[i])\n",
    "    \n",
    "    data = (data-np.mean(data,1)[:,None])/np.std(data,1)[:,None]\n",
    "    te_data_ = (te_data_-np.mean(te_data_,1)[:,None])/np.std(te_data_,1)[:,None]\n",
    "    \n",
    "    user_tr_data_tensor=torch.from_numpy(data).type(torch.FloatTensor)\n",
    "    user_tr_label_tensor=torch.from_numpy(labels).type(torch.LongTensor)\n",
    "    \n",
    "    user_te_data_tensor=torch.from_numpy(te_data_).type(torch.FloatTensor)\n",
    "    user_te_label_tensor=torch.from_numpy(te_labels_).type(torch.LongTensor)\n",
    "\n",
    "    each_worker_data.append(user_tr_data_tensor)\n",
    "    each_worker_label.append(user_tr_label_tensor)\n",
    "    \n",
    "    each_worker_te_data.append(user_te_data_tensor)\n",
    "    each_worker_te_label.append(user_te_label_tensor)\n",
    "    \n",
    "    print('user %d tr len %d te len %d ' % (i, len(user_tr_data_tensor) ,len(user_te_data_tensor)))\n",
    "\n",
    "global_te_data = torch.concat(each_worker_te_data)\n",
    "global_te_label = torch.concat(each_worker_te_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59440e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(train_data, labels, model, model_received, optimizer, batch_size=20):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    len_t = (len(train_data) // batch_size)\n",
    "    if len(train_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    r=np.arange(len(train_data))\n",
    "    np.random.shuffle(r)\n",
    "    \n",
    "    train_data = train_data[r]\n",
    "    labels = labels[r]\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "\n",
    "        inputs = train_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "        targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "\n",
    "def test(test_data, labels, model, criterion, use_cuda, debug_='MEDIUM', batch_size=64):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    len_t = (len(test_data) // batch_size)\n",
    "    if len(test_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ind in range(len_t):\n",
    "            # measure data loading time\n",
    "            inputs = test_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "            targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84b05be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbfgs(S_k_list, Y_k_list, v):\n",
    "    curr_S_k = torch.stack(S_k_list).T\n",
    "    curr_Y_k = torch.stack(Y_k_list).T\n",
    "    S_k_time_Y_k = np.dot(curr_S_k.T.cpu().numpy(), curr_Y_k.cpu().numpy())\n",
    "    S_k_time_S_k = np.dot(curr_S_k.T.cpu().numpy(), curr_S_k.cpu().numpy())\n",
    "    R_k = np.triu(S_k_time_Y_k)\n",
    "    L_k = S_k_time_Y_k - R_k\n",
    "    sigma_k = np.dot(Y_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()) / (np.dot(S_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()))\n",
    "    D_k_diag = np.diag(S_k_time_Y_k)\n",
    "    upper_mat = np.concatenate((sigma_k * S_k_time_S_k, L_k), axis=1)\n",
    "    lower_mat = np.concatenate((L_k.T, -np.diag(D_k_diag)), axis=1)\n",
    "    mat = np.concatenate((upper_mat, lower_mat), axis=0)\n",
    "    mat_inv = np.linalg.inv(mat)\n",
    "\n",
    "    approx_prod = sigma_k * v.cpu().numpy()\n",
    "    approx_prod = approx_prod.T\n",
    "    p_mat = np.concatenate((np.dot(curr_S_k.T.cpu().numpy(), sigma_k * v.unsqueeze(0).T.cpu().numpy()), np.dot(curr_Y_k.T.cpu().numpy(), v.unsqueeze(0).T.cpu().numpy())), axis=0)\n",
    "    approx_prod -= np.dot(np.dot(np.concatenate((sigma_k * curr_S_k.cpu().numpy(), curr_Y_k.cpu().numpy()), axis=1), mat_inv), p_mat)\n",
    "\n",
    "    return approx_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(20):\n",
    "        # apply attack to compromised worker devices with randomness\n",
    "        ##random_12 = 1. + np.random.uniform(size=vi_shape)\n",
    "        ##random_12 = torch.Tensor(random_12).float().cuda()\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc5c00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_mean(old_gradients, user_grads, b=0, hvp=None):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    agg_grads = torch.mean(user_grads,dim=0)\n",
    "    \n",
    "    return agg_grads, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcb12fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimmed_mean(old_gradients, user_grads, b=0, hvp=None):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    agg_grads = tr_mean(user_grads, 20)\n",
    "    \n",
    "    return agg_grads, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d37465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(old_gradients, user_grads, b=0, hvp=None):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    agg_grads = torch.median(user_grads, 0)[0]\n",
    "    \n",
    "    return agg_grads, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67adc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(score, nobyz):\n",
    "    estimator = KMeans(n_clusters=2)\n",
    "    estimator.fit(score.reshape(-1, 1))\n",
    "    label_pred = estimator.labels_\n",
    "    if np.mean(score[label_pred==0])<np.mean(score[label_pred==1]):\n",
    "        #0 is the label of malicious clients\n",
    "        label_pred = 1 - label_pred\n",
    "    real_label=np.ones(nworkers)\n",
    "    real_label[:nobyz]=0\n",
    "    acc=len(label_pred[label_pred==real_label])/nworkers\n",
    "    recall=1-np.sum(label_pred[:nobyz])/nobyz\n",
    "    fpr=1-np.sum(label_pred[nobyz:])/(nworkers-nobyz)\n",
    "    fnr=np.sum(label_pred[:nobyz])/nobyz\n",
    "    print(\"acc %0.4f; recall %0.4f; fpr %0.4f; fnr %0.4f;\" % (acc, recall, fpr, fnr))\n",
    "    print(silhouette_score(score.reshape(-1, 1), label_pred))\n",
    "\n",
    "def detection1(score, nobyz):\n",
    "    nrefs = 10\n",
    "    ks = range(1, 8)\n",
    "    gaps = np.zeros(len(ks))\n",
    "    gapDiff = np.zeros(len(ks) - 1)\n",
    "    sdk = np.zeros(len(ks))\n",
    "    min = np.min(score)\n",
    "    max = np.max(score)\n",
    "    score = (score - min)/(max-min)\n",
    "    for i, k in enumerate(ks):\n",
    "        estimator = KMeans(n_clusters=k)\n",
    "        estimator.fit(score.reshape(-1, 1))\n",
    "        label_pred = estimator.labels_\n",
    "        center = estimator.cluster_centers_\n",
    "        Wk = np.sum([np.square(score[m]-center[label_pred[m]]) for m in range(len(score))])\n",
    "        WkRef = np.zeros(nrefs)\n",
    "        for j in range(nrefs):\n",
    "            rand = np.random.uniform(0, 1, len(score))\n",
    "            estimator = KMeans(n_clusters=k)\n",
    "            estimator.fit(rand.reshape(-1, 1))\n",
    "            label_pred = estimator.labels_\n",
    "            center = estimator.cluster_centers_\n",
    "            WkRef[j] = np.sum([np.square(rand[m]-center[label_pred[m]]) for m in range(len(rand))])\n",
    "        gaps[i] = np.log(np.mean(WkRef)) - np.log(Wk)\n",
    "        sdk[i] = np.sqrt((1.0 + nrefs) / nrefs) * np.std(np.log(WkRef))\n",
    "\n",
    "        if i > 0:\n",
    "            gapDiff[i - 1] = gaps[i - 1] - gaps[i] + sdk[i]\n",
    "    #print(gapDiff)\n",
    "    for i in range(len(gapDiff)):\n",
    "        if gapDiff[i] >= 0:\n",
    "            select_k = i+1\n",
    "            break\n",
    "    if select_k == 1:\n",
    "        print('No attack detected!')\n",
    "        return 0\n",
    "    else:\n",
    "        print('Attack Detected!')\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72ced92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_list = []\n",
    "old_grad_list = []\n",
    "weight_record = []\n",
    "grad_record = []\n",
    "train_acc_list = []\n",
    "distance1 = []\n",
    "distance2 = []\n",
    "auc_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b089e",
   "metadata": {},
   "source": [
    "# Match bad FEMNIST baseline with 216 (72% of 300) clients (AGR = Mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2ba62",
   "metadata": {},
   "source": [
    "# Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d64ca3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 | val_loss 4.125 val acc 5.591 | best val_acc 5.59 test_acc 3.93\n",
      "e 10 | val_loss 3.833 val acc 13.329 | best val_acc 13.33 test_acc 6.93\n",
      "e 20 | val_loss 3.551 val acc 13.545 | best val_acc 13.54 test_acc 6.67\n",
      "e 30 | val_loss 3.423 val acc 20.538 | best val_acc 20.54 test_acc 9.71\n",
      "e 40 | val_loss 3.366 val acc 25.234 | best val_acc 25.23 test_acc 11.90\n",
      "e 50 | val_loss 3.316 val acc 28.164 | best val_acc 28.16 test_acc 13.96\n",
      "e 60 | val_loss 3.261 val acc 30.555 | best val_acc 30.55 test_acc 15.50\n",
      "e 70 | val_loss 3.198 val acc 33.015 | best val_acc 33.01 test_acc 16.85\n",
      "e 80 | val_loss 3.128 val acc 35.435 | best val_acc 35.44 test_acc 18.22\n",
      "e 90 | val_loss 3.051 val acc 37.821 | best val_acc 37.82 test_acc 19.49\n",
      "e 100 | val_loss 2.970 val acc 40.184 | best val_acc 40.18 test_acc 20.64\n",
      "e 110 | val_loss 2.885 val acc 42.299 | best val_acc 42.30 test_acc 21.92\n",
      "e 120 | val_loss 2.798 val acc 44.262 | best val_acc 44.26 test_acc 23.34\n",
      "e 130 | val_loss 2.712 val acc 45.847 | best val_acc 45.85 test_acc 24.60\n",
      "e 140 | val_loss 2.627 val acc 47.302 | best val_acc 47.30 test_acc 25.84\n",
      "e 150 | val_loss 2.544 val acc 48.537 | best val_acc 48.54 test_acc 27.36\n",
      "e 160 | val_loss 2.464 val acc 49.547 | best val_acc 49.55 test_acc 28.95\n",
      "e 170 | val_loss 2.388 val acc 50.413 | best val_acc 50.41 test_acc 30.20\n",
      "e 180 | val_loss 2.315 val acc 51.333 | best val_acc 51.33 test_acc 31.49\n",
      "e 190 | val_loss 2.246 val acc 52.143 | best val_acc 52.14 test_acc 32.81\n",
      "e 200 | val_loss 2.180 val acc 53.050 | best val_acc 53.05 test_acc 34.23\n",
      "e 210 | val_loss 2.118 val acc 53.793 | best val_acc 53.79 test_acc 35.61\n",
      "e 220 | val_loss 2.162 val acc 47.440 | best val_acc 54.11 test_acc 36.75\n",
      "e 230 | val_loss 2.146 val acc 46.117 | best val_acc 54.11 test_acc 36.75\n",
      "e 240 | val_loss 2.055 val acc 49.852 | best val_acc 54.11 test_acc 36.75\n",
      "e 250 | val_loss 2.096 val acc 47.290 | best val_acc 54.11 test_acc 36.75\n",
      "e 260 | val_loss 2.081 val acc 47.835 | best val_acc 54.11 test_acc 36.75\n",
      "e 270 | val_loss 1.963 val acc 51.220 | best val_acc 54.11 test_acc 36.75\n",
      "e 280 | val_loss 1.916 val acc 52.633 | best val_acc 54.11 test_acc 36.75\n",
      "e 290 | val_loss 1.877 val acc 53.501 | best val_acc 54.40 test_acc 42.08\n",
      "e 300 | val_loss 1.855 val acc 53.386 | best val_acc 54.69 test_acc 42.76\n",
      "e 310 | val_loss 1.870 val acc 51.613 | best val_acc 54.69 test_acc 42.76\n",
      "e 320 | val_loss 1.760 val acc 55.608 | best val_acc 55.61 test_acc 42.68\n",
      "e 330 | val_loss 1.711 val acc 56.959 | best val_acc 56.96 test_acc 43.73\n",
      "e 340 | val_loss 1.691 val acc 57.061 | best val_acc 57.23 test_acc 43.78\n",
      "e 350 | val_loss 1.721 val acc 56.176 | best val_acc 57.23 test_acc 43.78\n",
      "e 360 | val_loss 1.770 val acc 54.141 | best val_acc 57.23 test_acc 43.78\n",
      "e 370 | val_loss 1.652 val acc 58.329 | best val_acc 58.76 test_acc 47.84\n",
      "e 380 | val_loss 1.614 val acc 59.141 | best val_acc 60.21 test_acc 49.69\n",
      "e 390 | val_loss 1.624 val acc 58.346 | best val_acc 60.21 test_acc 49.69\n",
      "e 400 | val_loss 1.641 val acc 57.596 | best val_acc 60.21 test_acc 49.69\n",
      "e 410 | val_loss 1.559 val acc 60.512 | best val_acc 60.51 test_acc 50.49\n",
      "e 420 | val_loss 1.556 val acc 60.387 | best val_acc 60.99 test_acc 51.41\n",
      "e 430 | val_loss 1.581 val acc 59.016 | best val_acc 60.99 test_acc 51.41\n",
      "e 440 | val_loss 1.548 val acc 59.221 | best val_acc 61.31 test_acc 48.85\n",
      "e 450 | val_loss 1.488 val acc 61.149 | best val_acc 62.11 test_acc 51.42\n",
      "e 460 | val_loss 1.458 val acc 61.864 | best val_acc 62.58 test_acc 53.45\n",
      "e 470 | val_loss 1.468 val acc 61.067 | best val_acc 62.58 test_acc 53.45\n",
      "e 480 | val_loss 1.490 val acc 61.077 | best val_acc 62.58 test_acc 53.45\n",
      "e 490 | val_loss 1.430 val acc 62.704 | best val_acc 62.70 test_acc 54.03\n",
      "e 500 | val_loss 1.396 val acc 63.590 | best val_acc 63.59 test_acc 55.22\n",
      "e 510 | val_loss 1.367 val acc 64.432 | best val_acc 64.43 test_acc 56.06\n",
      "e 520 | val_loss 1.378 val acc 63.932 | best val_acc 64.57 test_acc 57.21\n",
      "e 530 | val_loss 1.434 val acc 62.099 | best val_acc 64.57 test_acc 57.21\n",
      "e 540 | val_loss 1.378 val acc 63.244 | best val_acc 65.36 test_acc 53.59\n",
      "e 550 | val_loss 1.364 val acc 63.655 | best val_acc 65.76 test_acc 54.14\n",
      "e 560 | val_loss 1.321 val acc 65.110 | best val_acc 66.43 test_acc 56.21\n",
      "e 570 | val_loss 1.320 val acc 65.157 | best val_acc 66.43 test_acc 56.21\n",
      "e 580 | val_loss 1.322 val acc 64.790 | best val_acc 66.43 test_acc 56.21\n",
      "e 590 | val_loss 1.249 val acc 66.883 | best val_acc 66.88 test_acc 56.98\n",
      "e 600 | val_loss 1.246 val acc 66.710 | best val_acc 66.88 test_acc 56.98\n",
      "e 610 | val_loss 1.222 val acc 67.288 | best val_acc 67.29 test_acc 57.94\n",
      "e 620 | val_loss 1.205 val acc 67.685 | best val_acc 67.69 test_acc 58.56\n",
      "e 630 | val_loss 1.196 val acc 67.923 | best val_acc 67.92 test_acc 59.12\n",
      "e 640 | val_loss 1.230 val acc 66.705 | best val_acc 67.92 test_acc 59.12\n",
      "e 650 | val_loss 1.212 val acc 67.395 | best val_acc 67.92 test_acc 59.12\n",
      "e 700 | val_loss 1.132 val acc 69.505 | best val_acc 69.51 test_acc 60.83\n",
      "e 710 | val_loss 1.114 val acc 69.828 | best val_acc 69.83 test_acc 60.60\n",
      "e 720 | val_loss 1.102 val acc 69.960 | best val_acc 70.04 test_acc 60.70\n",
      "e 730 | val_loss 1.143 val acc 67.793 | best val_acc 70.04 test_acc 60.70\n",
      "e 740 | val_loss 1.123 val acc 68.875 | best val_acc 70.04 test_acc 60.70\n",
      "e 750 | val_loss 1.083 val acc 70.291 | best val_acc 70.29 test_acc 62.92\n",
      "e 760 | val_loss 1.096 val acc 69.828 | best val_acc 70.29 test_acc 62.92\n",
      "e 770 | val_loss 1.062 val acc 70.823 | best val_acc 70.82 test_acc 62.40\n",
      "e 780 | val_loss 1.049 val acc 71.158 | best val_acc 71.16 test_acc 62.75\n",
      "e 790 | val_loss 1.091 val acc 69.508 | best val_acc 71.16 test_acc 62.75\n",
      "e 800 | val_loss 1.083 val acc 69.403 | best val_acc 71.16 test_acc 62.75\n",
      "e 810 | val_loss 1.063 val acc 69.868 | best val_acc 71.70 test_acc 64.22\n",
      "e 820 | val_loss 1.118 val acc 67.920 | best val_acc 71.73 test_acc 63.91\n",
      "e 830 | val_loss 1.068 val acc 70.048 | best val_acc 71.73 test_acc 63.91\n",
      "e 840 | val_loss 1.045 val acc 70.811 | best val_acc 72.01 test_acc 64.11\n",
      "e 850 | val_loss 1.042 val acc 70.703 | best val_acc 72.01 test_acc 64.15\n",
      "e 860 | val_loss 1.037 val acc 70.976 | best val_acc 72.01 test_acc 64.15\n",
      "e 870 | val_loss 1.014 val acc 71.583 | best val_acc 72.01 test_acc 64.15\n",
      "e 880 | val_loss 0.999 val acc 71.898 | best val_acc 72.01 test_acc 64.15\n",
      "e 890 | val_loss 1.021 val acc 71.196 | best val_acc 72.01 test_acc 64.15\n",
      "e 900 | val_loss 1.051 val acc 70.633 | best val_acc 72.01 test_acc 64.15\n",
      "e 910 | val_loss 1.044 val acc 70.781 | best val_acc 72.31 test_acc 63.73\n",
      "e 920 | val_loss 1.023 val acc 71.118 | best val_acc 72.91 test_acc 64.09\n",
      "e 930 | val_loss 1.009 val acc 71.193 | best val_acc 73.33 test_acc 64.51\n",
      "e 940 | val_loss 1.011 val acc 70.873 | best val_acc 73.37 test_acc 64.47\n",
      "e 950 | val_loss 1.012 val acc 70.811 | best val_acc 73.37 test_acc 64.47\n",
      "e 960 | val_loss 0.982 val acc 72.008 | best val_acc 73.37 test_acc 64.47\n",
      "e 970 | val_loss 0.964 val acc 72.781 | best val_acc 73.37 test_acc 64.47\n",
      "e 980 | val_loss 0.991 val acc 71.543 | best val_acc 73.37 test_acc 64.47\n",
      "e 990 | val_loss 0.956 val acc 72.428 | best val_acc 73.37 test_acc 64.47\n",
      "e 999 | val_loss 0.929 val acc 73.591 | best val_acc 73.59 test_acc 69.29\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = cnn().to(device)\n",
    "use_cuda = True\n",
    "lr = 0.01\n",
    "best_val_acc=0.0\n",
    "for e in range(1000):\n",
    "    # cnn_optimizer = SGD(net.parameters(), lr = lr*(0.995**e), weight_decay=1e-6)\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(nbyz, num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].reshape(-1, 1, 28, 28).cuda())\n",
    "        loss = criterion(output, each_worker_label[i].cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat(\n",
    "                (param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "#     agg_grads = tr_mean(user_grads, 20)\n",
    "#     agg_grads=torch.median(user_grads,dim=0)[0]\n",
    "    agg_grads=torch.mean(user_grads,dim=0)\n",
    "\n",
    "    del user_grads\n",
    "    \n",
    "    start_idx=0\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "\n",
    "    model_grads=[]\n",
    "\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    val_loss, val_acc = test(val_data_tensor.reshape(-1,1,28,28), val_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    is_best = best_val_acc < val_acc\n",
    "    best_val_acc = max(best_val_acc, val_acc)\n",
    "    if is_best:\n",
    "        _, best_te_acc = test(te_data_tensor.reshape(-1,1,28,28), te_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    if e%10==0 or e==999:\n",
    "        print('e %d | val_loss %.3f val acc %.3f | best val_acc %.2f test_acc %.2f' % (\n",
    "                e, val_loss, val_acc, best_val_acc, best_te_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8afd5c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 | val_loss 4.120 val acc 0.750 | best val_acc 0.75 test_acc 1.66\n",
      "e 10 | val_loss 4.026 val acc 4.933 | best val_acc 4.93 test_acc 2.73\n",
      "e 20 | val_loss 3.922 val acc 6.301 | best val_acc 6.30 test_acc 3.13\n",
      "e 30 | val_loss 3.797 val acc 7.314 | best val_acc 7.31 test_acc 3.40\n",
      "e 40 | val_loss 3.661 val acc 11.167 | best val_acc 11.17 test_acc 4.78\n",
      "e 50 | val_loss 3.546 val acc 16.945 | best val_acc 16.95 test_acc 8.18\n",
      "e 60 | val_loss 3.472 val acc 22.276 | best val_acc 22.28 test_acc 10.90\n",
      "e 70 | val_loss 3.430 val acc 26.301 | best val_acc 26.30 test_acc 12.94\n",
      "e 80 | val_loss 3.402 val acc 30.607 | best val_acc 30.61 test_acc 15.28\n",
      "e 90 | val_loss 3.378 val acc 32.900 | best val_acc 32.90 test_acc 16.57\n",
      "e 100 | val_loss 3.356 val acc 34.065 | best val_acc 34.07 test_acc 17.19\n",
      "e 110 | val_loss 3.333 val acc 34.920 | best val_acc 34.92 test_acc 17.73\n",
      "e 120 | val_loss 3.309 val acc 35.710 | best val_acc 35.71 test_acc 18.06\n",
      "e 130 | val_loss 3.283 val acc 36.330 | best val_acc 36.33 test_acc 18.35\n",
      "e 140 | val_loss 3.255 val acc 36.868 | best val_acc 36.87 test_acc 18.64\n",
      "e 150 | val_loss 3.224 val acc 37.423 | best val_acc 37.42 test_acc 18.89\n",
      "e 160 | val_loss 3.192 val acc 37.928 | best val_acc 37.93 test_acc 19.16\n",
      "e 170 | val_loss 3.157 val acc 38.571 | best val_acc 38.57 test_acc 19.47\n",
      "e 180 | val_loss 3.120 val acc 39.341 | best val_acc 39.34 test_acc 19.84\n",
      "e 190 | val_loss 3.081 val acc 40.209 | best val_acc 40.21 test_acc 20.22\n",
      "e 200 | val_loss 3.040 val acc 41.011 | best val_acc 41.01 test_acc 20.60\n",
      "e 210 | val_loss 2.997 val acc 42.084 | best val_acc 42.08 test_acc 21.01\n",
      "e 220 | val_loss 2.953 val acc 43.029 | best val_acc 43.03 test_acc 21.48\n",
      "e 230 | val_loss 2.908 val acc 43.934 | best val_acc 43.93 test_acc 21.95\n",
      "e 240 | val_loss 2.862 val acc 44.704 | best val_acc 44.70 test_acc 22.33\n",
      "e 250 | val_loss 2.816 val acc 45.507 | best val_acc 45.51 test_acc 22.82\n",
      "e 260 | val_loss 2.770 val acc 46.232 | best val_acc 46.23 test_acc 23.34\n",
      "e 270 | val_loss 2.725 val acc 46.932 | best val_acc 46.93 test_acc 23.94\n",
      "e 280 | val_loss 2.680 val acc 47.457 | best val_acc 47.46 test_acc 24.54\n",
      "e 290 | val_loss 2.636 val acc 48.017 | best val_acc 48.02 test_acc 25.21\n",
      "e 300 | val_loss 2.593 val acc 48.505 | best val_acc 48.50 test_acc 25.87\n",
      "e 310 | val_loss 2.551 val acc 48.970 | best val_acc 48.97 test_acc 26.51\n",
      "e 320 | val_loss 2.510 val acc 49.427 | best val_acc 49.43 test_acc 27.33\n",
      "e 330 | val_loss 2.471 val acc 49.910 | best val_acc 49.91 test_acc 28.08\n",
      "e 340 | val_loss 2.432 val acc 50.310 | best val_acc 50.31 test_acc 28.82\n",
      "e 350 | val_loss 2.395 val acc 50.643 | best val_acc 50.64 test_acc 29.64\n",
      "e 360 | val_loss 2.359 val acc 51.100 | best val_acc 51.10 test_acc 30.49\n",
      "e 370 | val_loss 2.324 val acc 51.538 | best val_acc 51.54 test_acc 31.25\n",
      "e 380 | val_loss 2.290 val acc 51.980 | best val_acc 51.98 test_acc 32.00\n",
      "e 390 | val_loss 2.257 val acc 52.333 | best val_acc 52.33 test_acc 32.75\n",
      "e 400 | val_loss 2.225 val acc 52.760 | best val_acc 52.76 test_acc 33.50\n",
      "e 410 | val_loss 2.194 val acc 53.135 | best val_acc 53.14 test_acc 34.27\n",
      "e 420 | val_loss 2.163 val acc 53.463 | best val_acc 53.46 test_acc 35.01\n",
      "e 430 | val_loss 2.134 val acc 53.891 | best val_acc 53.89 test_acc 35.90\n",
      "e 440 | val_loss 2.105 val acc 54.303 | best val_acc 54.30 test_acc 36.69\n",
      "e 450 | val_loss 2.077 val acc 54.686 | best val_acc 54.69 test_acc 37.43\n",
      "e 460 | val_loss 2.050 val acc 55.043 | best val_acc 55.04 test_acc 38.20\n",
      "e 470 | val_loss 2.025 val acc 55.368 | best val_acc 55.37 test_acc 38.88\n",
      "e 480 | val_loss 1.999 val acc 55.696 | best val_acc 55.70 test_acc 39.65\n",
      "e 490 | val_loss 1.975 val acc 56.056 | best val_acc 56.06 test_acc 40.35\n",
      "e 500 | val_loss 1.952 val acc 56.433 | best val_acc 56.43 test_acc 40.98\n",
      "e 510 | val_loss 1.929 val acc 56.774 | best val_acc 56.77 test_acc 41.61\n",
      "e 520 | val_loss 1.907 val acc 57.074 | best val_acc 57.07 test_acc 42.22\n",
      "e 530 | val_loss 1.885 val acc 57.306 | best val_acc 57.31 test_acc 42.79\n",
      "e 540 | val_loss 1.864 val acc 57.596 | best val_acc 57.60 test_acc 43.34\n",
      "e 550 | val_loss 1.844 val acc 57.886 | best val_acc 57.89 test_acc 43.79\n",
      "e 560 | val_loss 1.825 val acc 58.136 | best val_acc 58.14 test_acc 44.24\n",
      "e 570 | val_loss 1.806 val acc 58.364 | best val_acc 58.36 test_acc 44.69\n",
      "e 580 | val_loss 1.787 val acc 58.544 | best val_acc 58.54 test_acc 45.18\n",
      "e 590 | val_loss 1.770 val acc 58.766 | best val_acc 58.77 test_acc 45.64\n",
      "e 600 | val_loss 1.752 val acc 58.964 | best val_acc 58.96 test_acc 46.12\n",
      "e 610 | val_loss 1.736 val acc 59.196 | best val_acc 59.20 test_acc 46.53\n",
      "e 620 | val_loss 1.720 val acc 59.404 | best val_acc 59.40 test_acc 46.97\n",
      "e 630 | val_loss 1.704 val acc 59.616 | best val_acc 59.62 test_acc 47.38\n",
      "e 640 | val_loss 1.689 val acc 59.816 | best val_acc 59.82 test_acc 47.79\n",
      "e 650 | val_loss 1.676 val acc 60.054 | best val_acc 60.05 test_acc 48.16\n",
      "e 660 | val_loss 1.691 val acc 59.124 | best val_acc 60.05 test_acc 48.16\n",
      "e 670 | val_loss 1.692 val acc 58.919 | best val_acc 60.05 test_acc 48.16\n",
      "e 680 | val_loss 1.680 val acc 59.129 | best val_acc 60.05 test_acc 48.16\n",
      "e 690 | val_loss 1.668 val acc 59.351 | best val_acc 60.05 test_acc 48.16\n",
      "e 700 | val_loss 1.652 val acc 59.639 | best val_acc 60.05 test_acc 48.16\n",
      "e 710 | val_loss 1.636 val acc 59.994 | best val_acc 60.05 test_acc 48.16\n",
      "e 720 | val_loss 1.620 val acc 60.294 | best val_acc 60.29 test_acc 50.56\n",
      "e 730 | val_loss 1.606 val acc 60.487 | best val_acc 60.49 test_acc 50.88\n",
      "e 740 | val_loss 1.595 val acc 60.522 | best val_acc 60.55 test_acc 51.02\n",
      "e 750 | val_loss 1.587 val acc 60.532 | best val_acc 60.55 test_acc 51.02\n",
      "e 760 | val_loss 1.577 val acc 60.757 | best val_acc 60.76 test_acc 51.42\n",
      "e 770 | val_loss 1.565 val acc 61.057 | best val_acc 61.06 test_acc 51.74\n",
      "e 780 | val_loss 1.552 val acc 61.324 | best val_acc 61.32 test_acc 52.12\n",
      "e 790 | val_loss 1.538 val acc 61.667 | best val_acc 61.67 test_acc 52.49\n",
      "e 800 | val_loss 1.526 val acc 61.992 | best val_acc 61.99 test_acc 52.83\n",
      "e 810 | val_loss 1.514 val acc 62.269 | best val_acc 62.27 test_acc 53.13\n",
      "e 820 | val_loss 1.502 val acc 62.484 | best val_acc 62.48 test_acc 53.47\n",
      "e 830 | val_loss 1.492 val acc 62.664 | best val_acc 62.68 test_acc 53.71\n",
      "e 840 | val_loss 1.484 val acc 62.762 | best val_acc 62.79 test_acc 53.73\n",
      "e 850 | val_loss 1.482 val acc 62.689 | best val_acc 62.79 test_acc 53.73\n",
      "e 860 | val_loss 1.476 val acc 62.529 | best val_acc 62.79 test_acc 53.73\n",
      "e 870 | val_loss 1.468 val acc 62.544 | best val_acc 62.79 test_acc 53.73\n",
      "e 880 | val_loss 1.454 val acc 62.839 | best val_acc 62.84 test_acc 52.37\n",
      "e 890 | val_loss 1.435 val acc 63.525 | best val_acc 63.52 test_acc 53.21\n",
      "e 900 | val_loss 1.420 val acc 64.062 | best val_acc 64.06 test_acc 54.01\n",
      "e 910 | val_loss 1.409 val acc 64.320 | best val_acc 64.32 test_acc 54.85\n",
      "e 920 | val_loss 1.406 val acc 64.250 | best val_acc 64.32 test_acc 54.85\n",
      "e 930 | val_loss 1.428 val acc 63.385 | best val_acc 64.32 test_acc 54.85\n",
      "e 940 | val_loss 1.449 val acc 62.547 | best val_acc 64.32 test_acc 54.85\n",
      "e 950 | val_loss 1.421 val acc 63.375 | best val_acc 64.32 test_acc 54.85\n",
      "e 960 | val_loss 1.396 val acc 64.175 | best val_acc 64.32 test_acc 54.85\n",
      "e 970 | val_loss 1.380 val acc 64.595 | best val_acc 64.61 test_acc 55.48\n",
      "e 980 | val_loss 1.370 val acc 64.837 | best val_acc 64.84 test_acc 57.35\n",
      "e 990 | val_loss 1.362 val acc 64.995 | best val_acc 64.99 test_acc 57.58\n",
      "e 999 | val_loss 1.356 val acc 65.045 | best val_acc 65.06 test_acc 57.74\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = cnn().to(device)\n",
    "use_cuda = True\n",
    "lr = 0.005\n",
    "best_val_acc=0.0\n",
    "for e in range(1000):\n",
    "    # cnn_optimizer = SGD(net.parameters(), lr = lr*(0.995**e), weight_decay=1e-6)\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(nbyz, num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].reshape(-1, 1, 28, 28).cuda())\n",
    "        loss = criterion(output, each_worker_label[i].cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat(\n",
    "                (param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "#     agg_grads = tr_mean(user_grads, 20)\n",
    "#     agg_grads=torch.median(user_grads,dim=0)[0]\n",
    "    agg_grads=torch.mean(user_grads,dim=0)\n",
    "\n",
    "    del user_grads\n",
    "    \n",
    "    start_idx=0\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "\n",
    "    model_grads=[]\n",
    "\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    val_loss, val_acc = test(val_data_tensor.reshape(-1,1,28,28), val_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    is_best = best_val_acc < val_acc\n",
    "    best_val_acc = max(best_val_acc, val_acc)\n",
    "    if is_best:\n",
    "        _, best_te_acc = test(te_data_tensor.reshape(-1,1,28,28), te_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    if e%10==0 or e==999:\n",
    "        print('e %d | val_loss %.3f val acc %.3f | best val_acc %.2f test_acc %.2f' % (\n",
    "                e, val_loss, val_acc, best_val_acc, best_te_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa862ee",
   "metadata": {},
   "source": [
    "# Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40e3182a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 | val_loss 4.107 val acc 4.356 | best val_acc 4.36 test_acc 2.10\n",
      "e 10 | val_loss 3.972 val acc 6.684 | best val_acc 6.68 test_acc 3.39\n",
      "e 20 | val_loss 3.817 val acc 6.701 | best val_acc 6.70 test_acc 3.39\n",
      "e 30 | val_loss 3.648 val acc 6.724 | best val_acc 6.72 test_acc 3.39\n",
      "e 40 | val_loss 3.518 val acc 11.614 | best val_acc 11.61 test_acc 4.97\n",
      "e 50 | val_loss 3.449 val acc 21.768 | best val_acc 21.77 test_acc 10.73\n",
      "e 60 | val_loss 3.411 val acc 26.066 | best val_acc 26.07 test_acc 13.06\n",
      "e 70 | val_loss 3.381 val acc 28.884 | best val_acc 28.88 test_acc 14.54\n",
      "e 80 | val_loss 3.352 val acc 31.037 | best val_acc 31.04 test_acc 15.44\n",
      "e 90 | val_loss 3.322 val acc 32.682 | best val_acc 32.68 test_acc 16.22\n",
      "e 100 | val_loss 3.291 val acc 34.078 | best val_acc 34.08 test_acc 16.91\n",
      "e 110 | val_loss 3.258 val acc 35.448 | best val_acc 35.45 test_acc 17.54\n",
      "e 120 | val_loss 3.223 val acc 36.688 | best val_acc 36.69 test_acc 18.11\n",
      "e 130 | val_loss 3.186 val acc 37.686 | best val_acc 37.69 test_acc 18.58\n",
      "e 140 | val_loss 3.147 val acc 38.771 | best val_acc 38.77 test_acc 19.05\n",
      "e 150 | val_loss 3.105 val acc 39.606 | best val_acc 39.61 test_acc 19.61\n",
      "e 160 | val_loss 3.061 val acc 40.419 | best val_acc 40.42 test_acc 20.02\n",
      "e 170 | val_loss 3.016 val acc 41.356 | best val_acc 41.36 test_acc 20.48\n",
      "e 180 | val_loss 2.969 val acc 42.321 | best val_acc 42.32 test_acc 20.86\n",
      "e 190 | val_loss 2.921 val acc 43.086 | best val_acc 43.09 test_acc 21.27\n",
      "e 200 | val_loss 2.873 val acc 43.862 | best val_acc 43.86 test_acc 21.58\n",
      "e 210 | val_loss 2.825 val acc 44.654 | best val_acc 44.65 test_acc 21.91\n",
      "e 220 | val_loss 2.778 val acc 45.279 | best val_acc 45.28 test_acc 22.18\n",
      "e 230 | val_loss 2.732 val acc 45.824 | best val_acc 45.82 test_acc 22.49\n",
      "e 240 | val_loss 2.688 val acc 46.287 | best val_acc 46.29 test_acc 22.71\n",
      "e 250 | val_loss 2.645 val acc 46.690 | best val_acc 46.69 test_acc 22.89\n",
      "e 260 | val_loss 2.603 val acc 47.022 | best val_acc 47.02 test_acc 23.09\n",
      "e 270 | val_loss 2.563 val acc 47.367 | best val_acc 47.37 test_acc 23.30\n",
      "e 280 | val_loss 2.525 val acc 47.692 | best val_acc 47.69 test_acc 23.48\n",
      "e 290 | val_loss 2.488 val acc 47.937 | best val_acc 47.94 test_acc 23.64\n",
      "e 300 | val_loss 2.452 val acc 48.180 | best val_acc 48.18 test_acc 23.76\n",
      "e 310 | val_loss 2.418 val acc 48.417 | best val_acc 48.42 test_acc 23.90\n",
      "e 320 | val_loss 2.385 val acc 48.662 | best val_acc 48.66 test_acc 24.07\n",
      "e 330 | val_loss 2.352 val acc 48.922 | best val_acc 48.92 test_acc 24.28\n",
      "e 340 | val_loss 2.321 val acc 49.195 | best val_acc 49.19 test_acc 24.54\n",
      "e 350 | val_loss 2.290 val acc 49.500 | best val_acc 49.50 test_acc 24.82\n",
      "e 360 | val_loss 2.261 val acc 49.725 | best val_acc 49.72 test_acc 25.23\n",
      "e 370 | val_loss 2.232 val acc 50.013 | best val_acc 50.01 test_acc 25.55\n",
      "e 380 | val_loss 2.266 val acc 46.192 | best val_acc 50.05 test_acc 25.61\n",
      "e 390 | val_loss 2.398 val acc 42.301 | best val_acc 50.05 test_acc 25.61\n",
      "e 400 | val_loss 2.381 val acc 42.236 | best val_acc 50.05 test_acc 25.61\n",
      "e 410 | val_loss 2.311 val acc 42.226 | best val_acc 50.05 test_acc 25.61\n",
      "e 420 | val_loss 2.260 val acc 43.707 | best val_acc 50.05 test_acc 25.61\n",
      "e 430 | val_loss 2.312 val acc 43.331 | best val_acc 50.05 test_acc 25.61\n",
      "e 440 | val_loss 2.215 val acc 46.032 | best val_acc 50.05 test_acc 25.61\n",
      "e 450 | val_loss 2.225 val acc 44.454 | best val_acc 50.05 test_acc 25.61\n",
      "e 460 | val_loss 2.168 val acc 47.010 | best val_acc 50.05 test_acc 25.61\n",
      "e 470 | val_loss 2.276 val acc 43.914 | best val_acc 50.05 test_acc 25.61\n",
      "e 480 | val_loss 2.150 val acc 47.592 | best val_acc 50.05 test_acc 25.61\n",
      "e 490 | val_loss 2.138 val acc 47.630 | best val_acc 50.05 test_acc 25.61\n",
      "e 500 | val_loss 2.251 val acc 44.464 | best val_acc 50.05 test_acc 25.61\n",
      "e 510 | val_loss 2.189 val acc 45.012 | best val_acc 50.05 test_acc 25.61\n",
      "e 520 | val_loss 2.216 val acc 45.022 | best val_acc 50.05 test_acc 25.61\n",
      "e 530 | val_loss 2.187 val acc 45.694 | best val_acc 50.05 test_acc 25.61\n",
      "e 540 | val_loss 2.134 val acc 46.322 | best val_acc 50.05 test_acc 25.61\n",
      "e 550 | val_loss 2.125 val acc 46.104 | best val_acc 50.05 test_acc 25.61\n",
      "e 560 | val_loss 2.073 val acc 46.977 | best val_acc 50.05 test_acc 25.61\n",
      "e 570 | val_loss 2.092 val acc 46.920 | best val_acc 50.05 test_acc 25.61\n",
      "e 580 | val_loss 1.990 val acc 50.025 | best val_acc 50.05 test_acc 25.61\n",
      "e 590 | val_loss 2.049 val acc 48.170 | best val_acc 50.37 test_acc 31.03\n",
      "e 600 | val_loss 1.963 val acc 50.875 | best val_acc 50.88 test_acc 31.64\n",
      "e 610 | val_loss 2.017 val acc 49.367 | best val_acc 51.37 test_acc 32.11\n",
      "e 620 | val_loss 1.952 val acc 50.998 | best val_acc 51.93 test_acc 32.66\n",
      "e 630 | val_loss 2.027 val acc 47.000 | best val_acc 51.93 test_acc 32.66\n",
      "e 640 | val_loss 2.033 val acc 48.105 | best val_acc 51.93 test_acc 32.66\n",
      "e 650 | val_loss 1.903 val acc 52.125 | best val_acc 52.13 test_acc 33.95\n",
      "e 660 | val_loss 1.943 val acc 50.653 | best val_acc 52.13 test_acc 33.95\n",
      "e 670 | val_loss 1.935 val acc 50.478 | best val_acc 52.13 test_acc 33.95\n",
      "e 680 | val_loss 1.889 val acc 52.540 | best val_acc 52.54 test_acc 37.20\n",
      "e 690 | val_loss 1.907 val acc 51.530 | best val_acc 53.01 test_acc 38.34\n",
      "e 700 | val_loss 1.885 val acc 52.905 | best val_acc 53.26 test_acc 39.43\n",
      "e 710 | val_loss 1.932 val acc 50.753 | best val_acc 53.26 test_acc 39.43\n",
      "e 720 | val_loss 1.869 val acc 53.120 | best val_acc 53.26 test_acc 39.43\n",
      "e 730 | val_loss 1.866 val acc 52.805 | best val_acc 53.68 test_acc 41.20\n",
      "e 740 | val_loss 1.828 val acc 54.223 | best val_acc 54.22 test_acc 41.74\n",
      "e 750 | val_loss 1.835 val acc 53.646 | best val_acc 54.48 test_acc 42.10\n",
      "e 760 | val_loss 1.805 val acc 54.768 | best val_acc 54.77 test_acc 42.54\n",
      "e 770 | val_loss 1.817 val acc 53.983 | best val_acc 54.89 test_acc 42.77\n",
      "e 780 | val_loss 1.790 val acc 55.018 | best val_acc 55.02 test_acc 43.13\n",
      "e 790 | val_loss 1.802 val acc 54.248 | best val_acc 55.11 test_acc 43.30\n",
      "e 800 | val_loss 1.772 val acc 55.398 | best val_acc 55.40 test_acc 43.67\n",
      "e 810 | val_loss 1.780 val acc 54.793 | best val_acc 55.58 test_acc 43.92\n",
      "e 820 | val_loss 1.751 val acc 55.871 | best val_acc 55.87 test_acc 44.29\n",
      "e 830 | val_loss 1.758 val acc 55.301 | best val_acc 56.05 test_acc 44.47\n",
      "e 840 | val_loss 1.729 val acc 56.303 | best val_acc 56.30 test_acc 44.82\n",
      "e 850 | val_loss 1.741 val acc 55.683 | best val_acc 56.41 test_acc 45.06\n",
      "e 860 | val_loss 1.714 val acc 56.551 | best val_acc 56.55 test_acc 45.33\n",
      "e 870 | val_loss 1.727 val acc 55.881 | best val_acc 56.65 test_acc 40.16\n",
      "e 880 | val_loss 1.701 val acc 56.671 | best val_acc 56.73 test_acc 40.35\n",
      "e 890 | val_loss 1.716 val acc 55.798 | best val_acc 56.83 test_acc 40.61\n",
      "e 900 | val_loss 1.696 val acc 56.263 | best val_acc 56.83 test_acc 40.61\n",
      "e 910 | val_loss 1.748 val acc 53.456 | best val_acc 56.83 test_acc 40.61\n",
      "e 920 | val_loss 1.739 val acc 53.798 | best val_acc 56.83 test_acc 40.61\n",
      "e 930 | val_loss 1.648 val acc 56.859 | best val_acc 56.86 test_acc 41.24\n",
      "e 940 | val_loss 1.608 val acc 58.256 | best val_acc 58.26 test_acc 42.23\n",
      "e 950 | val_loss 1.602 val acc 58.339 | best val_acc 58.68 test_acc 42.64\n",
      "e 960 | val_loss 1.581 val acc 59.126 | best val_acc 59.13 test_acc 43.17\n",
      "e 970 | val_loss 1.590 val acc 58.621 | best val_acc 59.33 test_acc 43.38\n",
      "e 980 | val_loss 1.567 val acc 59.546 | best val_acc 59.55 test_acc 43.67\n",
      "e 990 | val_loss 1.579 val acc 58.824 | best val_acc 59.61 test_acc 43.88\n",
      "e 999 | val_loss 1.718 val acc 56.866 | best val_acc 59.67 test_acc 44.06\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = cnn().to(device)\n",
    "use_cuda = True\n",
    "lr = 0.005\n",
    "best_val_acc=0.0\n",
    "for e in range(1000):\n",
    "    # cnn_optimizer = SGD(net.parameters(), lr = lr*(0.995**e), weight_decay=1e-6)\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(nbyz, num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].reshape(-1, 1, 28, 28).cuda())\n",
    "        loss = criterion(output, each_worker_label[i].cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat(\n",
    "                (param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "    agg_grads=torch.median(user_grads,dim=0)[0]\n",
    "\n",
    "    del user_grads\n",
    "    \n",
    "    start_idx=0\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "\n",
    "    model_grads=[]\n",
    "\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    val_loss, val_acc = test(val_data_tensor.reshape(-1,1,28,28), val_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    is_best = best_val_acc < val_acc\n",
    "    best_val_acc = max(best_val_acc, val_acc)\n",
    "    if is_best:\n",
    "        _, best_te_acc = test(te_data_tensor.reshape(-1,1,28,28), te_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    if e%10==0 or e==999:\n",
    "        print('e %d | val_loss %.3f val acc %.3f | best val_acc %.2f test_acc %.2f' % (\n",
    "                e, val_loss, val_acc, best_val_acc, best_te_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85fce3d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 | val_loss 4.111 val acc 0.588 | best val_acc 0.59 test_acc 0.95\n",
      "e 10 | val_loss 3.927 val acc 13.340 | best val_acc 13.40 test_acc 6.69\n",
      "e 20 | val_loss 3.659 val acc 9.841 | best val_acc 13.40 test_acc 6.69\n",
      "e 30 | val_loss 3.467 val acc 15.697 | best val_acc 15.70 test_acc 7.47\n",
      "e 40 | val_loss 3.405 val acc 25.664 | best val_acc 25.66 test_acc 12.73\n",
      "e 50 | val_loss 3.360 val acc 31.115 | best val_acc 31.11 test_acc 15.39\n",
      "e 60 | val_loss 3.313 val acc 33.780 | best val_acc 33.78 test_acc 16.82\n",
      "e 70 | val_loss 3.260 val acc 35.930 | best val_acc 35.93 test_acc 17.84\n",
      "e 80 | val_loss 3.201 val acc 37.806 | best val_acc 37.81 test_acc 18.73\n",
      "e 90 | val_loss 3.135 val acc 39.616 | best val_acc 39.62 test_acc 19.60\n",
      "e 100 | val_loss 3.063 val acc 41.144 | best val_acc 41.14 test_acc 20.34\n",
      "e 110 | val_loss 2.987 val acc 42.494 | best val_acc 42.49 test_acc 20.99\n",
      "e 120 | val_loss 2.907 val acc 43.784 | best val_acc 43.78 test_acc 21.61\n",
      "e 130 | val_loss 2.826 val acc 44.869 | best val_acc 44.87 test_acc 22.12\n",
      "e 140 | val_loss 2.745 val acc 45.719 | best val_acc 45.72 test_acc 22.52\n",
      "e 150 | val_loss 2.667 val acc 46.632 | best val_acc 46.63 test_acc 22.91\n",
      "e 160 | val_loss 2.592 val acc 47.232 | best val_acc 47.23 test_acc 23.18\n",
      "e 170 | val_loss 2.522 val acc 47.810 | best val_acc 47.81 test_acc 23.47\n",
      "e 180 | val_loss 2.481 val acc 45.814 | best val_acc 48.06 test_acc 23.67\n",
      "e 190 | val_loss 2.616 val acc 37.143 | best val_acc 48.06 test_acc 23.67\n",
      "e 200 | val_loss 2.590 val acc 37.363 | best val_acc 48.06 test_acc 23.67\n",
      "e 210 | val_loss 2.439 val acc 41.309 | best val_acc 48.06 test_acc 23.67\n",
      "e 220 | val_loss 2.461 val acc 38.486 | best val_acc 48.06 test_acc 23.67\n",
      "e 230 | val_loss 2.389 val acc 40.534 | best val_acc 48.06 test_acc 23.67\n",
      "e 240 | val_loss 2.332 val acc 41.719 | best val_acc 48.06 test_acc 23.67\n",
      "e 250 | val_loss 2.285 val acc 42.626 | best val_acc 48.06 test_acc 23.67\n",
      "e 260 | val_loss 2.240 val acc 44.247 | best val_acc 48.06 test_acc 23.67\n",
      "e 270 | val_loss 2.198 val acc 45.992 | best val_acc 48.06 test_acc 23.67\n",
      "e 280 | val_loss 2.298 val acc 40.579 | best val_acc 48.06 test_acc 23.67\n",
      "e 290 | val_loss 2.156 val acc 45.257 | best val_acc 48.06 test_acc 23.67\n",
      "e 300 | val_loss 2.106 val acc 46.509 | best val_acc 48.06 test_acc 23.67\n",
      "e 310 | val_loss 2.065 val acc 47.800 | best val_acc 48.06 test_acc 23.67\n",
      "e 320 | val_loss 2.027 val acc 48.827 | best val_acc 48.89 test_acc 29.95\n",
      "e 330 | val_loss 1.994 val acc 49.562 | best val_acc 49.86 test_acc 31.53\n",
      "e 340 | val_loss 1.962 val acc 50.238 | best val_acc 50.81 test_acc 33.27\n",
      "e 350 | val_loss 1.933 val acc 51.018 | best val_acc 51.88 test_acc 35.50\n",
      "e 360 | val_loss 1.993 val acc 48.520 | best val_acc 52.21 test_acc 36.82\n",
      "e 370 | val_loss 1.943 val acc 49.200 | best val_acc 52.21 test_acc 36.82\n",
      "e 380 | val_loss 1.895 val acc 51.353 | best val_acc 52.90 test_acc 38.69\n",
      "e 390 | val_loss 1.893 val acc 51.853 | best val_acc 52.90 test_acc 38.69\n",
      "e 400 | val_loss 1.881 val acc 53.158 | best val_acc 54.20 test_acc 36.82\n",
      "e 410 | val_loss 1.811 val acc 54.423 | best val_acc 55.14 test_acc 37.53\n",
      "e 420 | val_loss 1.844 val acc 54.161 | best val_acc 55.70 test_acc 38.13\n",
      "e 430 | val_loss 1.766 val acc 55.578 | best val_acc 56.27 test_acc 39.05\n",
      "e 440 | val_loss 1.821 val acc 54.701 | best val_acc 56.56 test_acc 39.73\n",
      "e 450 | val_loss 1.738 val acc 56.053 | best val_acc 56.62 test_acc 39.93\n",
      "e 460 | val_loss 1.819 val acc 54.113 | best val_acc 56.62 test_acc 39.93\n",
      "e 470 | val_loss 1.789 val acc 55.183 | best val_acc 56.88 test_acc 43.45\n",
      "e 480 | val_loss 1.807 val acc 54.226 | best val_acc 56.88 test_acc 43.45\n",
      "e 490 | val_loss 1.688 val acc 57.171 | best val_acc 57.96 test_acc 46.52\n",
      "e 500 | val_loss 1.691 val acc 57.361 | best val_acc 59.37 test_acc 46.94\n",
      "e 510 | val_loss 1.624 val acc 59.111 | best val_acc 59.94 test_acc 47.46\n",
      "e 520 | val_loss 1.721 val acc 57.159 | best val_acc 59.94 test_acc 47.46\n",
      "e 530 | val_loss 1.686 val acc 58.291 | best val_acc 59.94 test_acc 47.46\n",
      "e 540 | val_loss 1.730 val acc 56.896 | best val_acc 59.94 test_acc 47.46\n",
      "e 550 | val_loss 1.607 val acc 59.879 | best val_acc 60.67 test_acc 43.82\n",
      "e 560 | val_loss 1.622 val acc 59.371 | best val_acc 60.93 test_acc 44.72\n",
      "e 570 | val_loss 1.587 val acc 59.874 | best val_acc 60.93 test_acc 44.72\n",
      "e 580 | val_loss 1.627 val acc 58.859 | best val_acc 61.06 test_acc 45.25\n",
      "e 590 | val_loss 1.551 val acc 60.794 | best val_acc 61.65 test_acc 46.09\n",
      "e 600 | val_loss 1.600 val acc 59.859 | best val_acc 62.01 test_acc 46.56\n",
      "e 610 | val_loss 1.535 val acc 61.824 | best val_acc 62.19 test_acc 46.92\n",
      "e 620 | val_loss 1.632 val acc 59.944 | best val_acc 62.19 test_acc 46.92\n",
      "e 630 | val_loss 1.578 val acc 59.954 | best val_acc 62.19 test_acc 46.92\n",
      "e 640 | val_loss 1.502 val acc 61.807 | best val_acc 62.19 test_acc 46.92\n",
      "e 650 | val_loss 1.468 val acc 62.454 | best val_acc 63.02 test_acc 52.60\n",
      "e 660 | val_loss 1.491 val acc 61.674 | best val_acc 63.02 test_acc 52.60\n",
      "e 670 | val_loss 1.448 val acc 62.717 | best val_acc 63.02 test_acc 52.60\n",
      "e 680 | val_loss 1.429 val acc 63.222 | best val_acc 63.22 test_acc 49.51\n",
      "e 690 | val_loss 1.424 val acc 63.355 | best val_acc 63.35 test_acc 49.72\n",
      "e 700 | val_loss 1.425 val acc 63.254 | best val_acc 63.35 test_acc 49.72\n",
      "e 710 | val_loss 1.438 val acc 62.817 | best val_acc 63.35 test_acc 49.72\n",
      "e 720 | val_loss 1.434 val acc 62.844 | best val_acc 63.35 test_acc 49.72\n",
      "e 730 | val_loss 1.412 val acc 63.552 | best val_acc 64.30 test_acc 53.54\n",
      "e 740 | val_loss 1.412 val acc 63.955 | best val_acc 64.69 test_acc 53.15\n",
      "e 750 | val_loss 1.636 val acc 59.349 | best val_acc 64.69 test_acc 53.15\n",
      "e 760 | val_loss 1.407 val acc 64.555 | best val_acc 64.69 test_acc 53.15\n",
      "e 770 | val_loss 1.375 val acc 64.995 | best val_acc 65.66 test_acc 52.38\n",
      "e 780 | val_loss 1.406 val acc 64.135 | best val_acc 65.66 test_acc 52.38\n",
      "e 790 | val_loss 1.391 val acc 64.482 | best val_acc 65.66 test_acc 52.38\n",
      "e 800 | val_loss 1.369 val acc 65.005 | best val_acc 65.74 test_acc 52.98\n",
      "e 810 | val_loss 1.366 val acc 65.107 | best val_acc 65.80 test_acc 53.14\n",
      "e 820 | val_loss 1.361 val acc 65.087 | best val_acc 65.88 test_acc 53.31\n",
      "e 830 | val_loss 1.349 val acc 65.335 | best val_acc 66.22 test_acc 53.56\n",
      "e 840 | val_loss 1.342 val acc 65.500 | best val_acc 66.60 test_acc 53.52\n",
      "e 850 | val_loss 1.341 val acc 65.412 | best val_acc 67.08 test_acc 53.19\n",
      "e 860 | val_loss 1.353 val acc 64.960 | best val_acc 67.46 test_acc 52.15\n",
      "e 870 | val_loss 1.408 val acc 62.227 | best val_acc 67.51 test_acc 51.82\n",
      "e 880 | val_loss 1.324 val acc 64.487 | best val_acc 67.56 test_acc 52.31\n",
      "e 890 | val_loss 1.288 val acc 65.535 | best val_acc 68.43 test_acc 53.37\n",
      "e 900 | val_loss 1.328 val acc 64.260 | best val_acc 68.43 test_acc 53.37\n",
      "e 910 | val_loss 1.305 val acc 64.990 | best val_acc 68.43 test_acc 53.37\n",
      "e 920 | val_loss 1.284 val acc 65.632 | best val_acc 68.86 test_acc 53.37\n",
      "e 930 | val_loss 1.295 val acc 65.332 | best val_acc 68.91 test_acc 53.31\n",
      "e 940 | val_loss 1.289 val acc 65.580 | best val_acc 68.96 test_acc 53.27\n",
      "e 950 | val_loss 1.276 val acc 65.962 | best val_acc 69.22 test_acc 53.71\n",
      "e 960 | val_loss 1.278 val acc 65.832 | best val_acc 69.32 test_acc 53.69\n",
      "e 970 | val_loss 1.271 val acc 65.900 | best val_acc 69.50 test_acc 53.84\n",
      "e 980 | val_loss 1.262 val acc 66.030 | best val_acc 69.74 test_acc 54.18\n",
      "e 990 | val_loss 1.257 val acc 66.025 | best val_acc 69.90 test_acc 54.32\n",
      "e 999 | val_loss 1.152 val acc 70.038 | best val_acc 70.04 test_acc 54.47\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = cnn().to(device)\n",
    "use_cuda = True\n",
    "lr = 0.01\n",
    "best_val_acc=0.0\n",
    "for e in range(1000):\n",
    "    # cnn_optimizer = SGD(net.parameters(), lr = lr*(0.995**e), weight_decay=1e-6)\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(nbyz, num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].reshape(-1, 1, 28, 28).cuda())\n",
    "        loss = criterion(output, each_worker_label[i].cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat(\n",
    "                (param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "    agg_grads=torch.median(user_grads,dim=0)[0]\n",
    "\n",
    "    del user_grads\n",
    "    \n",
    "    start_idx=0\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "\n",
    "    model_grads=[]\n",
    "\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    val_loss, val_acc = test(val_data_tensor.reshape(-1,1,28,28), val_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    is_best = best_val_acc < val_acc\n",
    "    best_val_acc = max(best_val_acc, val_acc)\n",
    "    if is_best:\n",
    "        _, best_te_acc = test(te_data_tensor.reshape(-1,1,28,28), te_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    if e%10==0 or e==999:\n",
    "        print('e %d | val_loss %.3f val acc %.3f | best val_acc %.2f test_acc %.2f' % (\n",
    "                e, val_loss, val_acc, best_val_acc, best_te_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8575b2bc",
   "metadata": {},
   "source": [
    "# Good FedAvg FEMNIST baseline with 216 (72% of 300) clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a66029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(train_data, labels, model, optimizer, batch_size=20):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    len_t = (len(train_data) // batch_size)\n",
    "    if len(train_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    r=np.arange(len(train_data))\n",
    "    np.random.shuffle(r)\n",
    "    \n",
    "    train_data = train_data[r]\n",
    "    labels = labels[r]\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "\n",
    "        inputs = train_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "        targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "\n",
    "def test(test_data, labels, model, criterion, use_cuda, debug_='MEDIUM', batch_size=64):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    len_t = (len(test_data) // batch_size)\n",
    "    if len(test_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ind in range(len_t):\n",
    "            # measure data loading time\n",
    "            inputs = test_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "            targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ecd246",
   "metadata": {},
   "source": [
    "# Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 3\n",
    "batch_size = 10\n",
    "num_workers = 300\n",
    "local_lr = 0.1\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 28\n",
    "byz_type = 'full_trim'\n",
    "aggregation = 'trim'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "resume=False\n",
    "round_nclients = num_workers\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "fed_model = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    \n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num))\n",
    "        # optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(\n",
    "                each_worker_data[i].reshape(-1, 1, 28, 28), torch.Tensor(each_worker_label[i]).long(),\n",
    "                model, optimizer, batch_size)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    agg_update = torch.mean(user_updates, 0)\n",
    "\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = cnn().to(device)\n",
    "    net.apply(init_weights)\n",
    "    \n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "\n",
    "    val_loss, val_acc = test(val_data_tensor.reshape(-1,1,28,28), val_label_tensor, fed_model, criterion, use_cuda, batch_size=50)\n",
    "    is_best = best_val_acc < val_acc\n",
    "    best_val_acc = max(best_val_acc, val_acc)\n",
    "    \n",
    "    if is_best:\n",
    "        _, best_te_acc = test(te_data_tensor.reshape(-1,1,28,28), te_label_tensor, fed_model, criterion, use_cuda, batch_size=50)\n",
    "    \n",
    "    if epoch_num%10==0 or epoch_num==200:\n",
    "        print('e %d | val_loss %.3f val acc %.3f | best val_acc %.2f test_acc %.2f' % (\n",
    "                e, val_loss, val_acc, best_val_acc, best_te_acc))\n",
    "\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ccb06",
   "metadata": {},
   "source": [
    "# Setup FEMNIST baseline for mean and median for the given model\n",
    "\n",
    "## This baseline is for FedSGD with full-batch gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a5735a",
   "metadata": {},
   "source": [
    "# For mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b197cb5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = cnn().to(device)\n",
    "use_cuda = True\n",
    "lr = 0.25\n",
    "best_val_acc=0.0\n",
    "for e in range(100):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr*(0.995**e), weight_decay=1e-6)\n",
    "    user_grads = []\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].reshape(-1, 1, 28, 28).cuda())\n",
    "        loss = criterion(output, each_worker_label[i].cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat(\n",
    "                (param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "#     agg_grads = tr_mean(user_grads, 20)\n",
    "#     agg_grads=torch.median(user_grads,dim=0)[0]\n",
    "    agg_grads=torch.mean(user_grads,dim=0)\n",
    "\n",
    "    del user_grads\n",
    "    \n",
    "    start_idx=0\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "\n",
    "    model_grads=[]\n",
    "\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    val_loss, val_acc = test(val_data_tensor.reshape(-1,1,28,28), val_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    is_best = best_val_acc < val_acc\n",
    "    best_val_acc = max(best_val_acc, val_acc)\n",
    "    if is_best:\n",
    "        _, best_te_acc = test(te_data_tensor.reshape(-1,1,28,28), te_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    print('e %d | val_loss %.3f val acc %.3f | best val_acc %.2f test_acc %.2f' % (\n",
    "        e, val_loss, val_acc, best_val_acc, best_te_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e2f1ff",
   "metadata": {},
   "source": [
    "# For median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda76a46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = cnn().to(device)\n",
    "use_cuda = True\n",
    "lr = 0.2\n",
    "best_val_acc=0.0\n",
    "for e in range(100):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr*(0.995**e), weight_decay=1e-6)\n",
    "    user_grads = []\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].reshape(-1, 1, 28, 28).cuda())\n",
    "        loss = criterion(output, each_worker_label[i].cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat(\n",
    "                (param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "#     agg_grads = tr_mean(user_grads, 20)\n",
    "    agg_grads=torch.median(user_grads,dim=0)[0]\n",
    "#     agg_grads=torch.mean(user_grads,dim=0)\n",
    "\n",
    "    del user_grads\n",
    "    \n",
    "    start_idx=0\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "\n",
    "    model_grads=[]\n",
    "\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    val_loss, val_acc = test(val_data_tensor.reshape(-1,1,28,28), val_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    is_best = best_val_acc < val_acc\n",
    "    best_val_acc = max(best_val_acc, val_acc)\n",
    "    if is_best:\n",
    "        _, best_te_acc = test(te_data_tensor.reshape(-1,1,28,28), te_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    print('e %d | val_loss %.3f val acc %.3f | best val_acc %.2f test_acc %.2f' % (\n",
    "        e, val_loss, val_acc, best_val_acc, best_te_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510f695",
   "metadata": {},
   "source": [
    "# FLDetector for FEMNIST + 300 clients + Cross-silo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010781b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "weight_record = []\n",
    "grad_record = []\n",
    "test_grads = []\n",
    "malicious_scores = np.zeros((1, num_workers))\n",
    "attack = False\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = cnn().to(device)\n",
    "lr = 0.25\n",
    "best_val_acc=0.0\n",
    "\n",
    "start_detection_epoch = 10\n",
    "window_size = 10\n",
    "assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "for e in range(100):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr*(0.995**e), weight_decay=1e-6)\n",
    "    user_grads = []\n",
    "\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].reshape(-1, 1, 28, 28).cuda())\n",
    "        loss = criterion(output, each_worker_label[i].cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "    tmp = []\n",
    "    for param in net.parameters():\n",
    "        tmp = param.data.view(-1) if not len(tmp) else torch.cat((tmp, param.data.view(-1)))\n",
    "    weight = tmp    \n",
    "\n",
    "    if (e > start_detection_epoch):\n",
    "        hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "        hvp = np.squeeze(hvp)\n",
    "    else:\n",
    "        hvp = None\n",
    "\n",
    "    if attack:\n",
    "        if(e>0):\n",
    "            user_grads = full_trim(user_grads, 20)\n",
    "        elif e>10:\n",
    "            for m in range(20):\n",
    "                user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device)\n",
    "\n",
    "    agg_grads, distance = simple_mean(old_grad_list, user_grads, nbyz, hvp)\n",
    "    \n",
    "    if distance is not None and e > (start_detection_epoch - window_size):\n",
    "        print('==>', e, malicious_scores.shape)\n",
    "        malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "    if malicious_scores.shape[0] >= 11:\n",
    "        print('performing detection at epoch %d' % e)\n",
    "        if detection1(np.sum(malicious_scores[-10:], axis=0), 20):\n",
    "            print('Stop at iteration:', e)\n",
    "            detection(np.sum(malicious_scores[-10:], axis=0), 20)\n",
    "            break\n",
    "\n",
    "#     agg_grads = tr_mean(user_grads, 20)\n",
    "#     agg_grads=torch.median(user_grads,dim=0)[0]\n",
    "#     agg_grads=torch.mean(user_grads,dim=0)\n",
    "\n",
    "    if e > (start_detection_epoch - window_size):\n",
    "        weight_record.append(weight - last_weight)\n",
    "        grad_record.append(agg_grads - last_grad)\n",
    "    \n",
    "    if (len(weight_record) > 10):\n",
    "        del weight_record[0]\n",
    "        del grad_record[0]\n",
    "    \n",
    "    last_weight = weight\n",
    "    last_grad = agg_grads\n",
    "    old_grad_list = user_grads\n",
    "  \n",
    "    del user_grads\n",
    "    \n",
    "    start_idx=0\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "\n",
    "    model_grads=[]\n",
    "\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    val_loss, val_acc = test(val_data_tensor.reshape(-1,1,28,28), val_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    is_best = best_val_acc < val_acc\n",
    "    best_val_acc = max(best_val_acc, val_acc)\n",
    "    if is_best:\n",
    "        _, best_te_acc = test(te_data_tensor.reshape(-1,1,28,28), te_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    print('e %d | val_loss %.3f val acc %.3f | best val_acc %.2f test_acc %.2f' % (\n",
    "        e, val_loss, val_acc, best_val_acc, best_te_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e170d",
   "metadata": {},
   "source": [
    "# FLdetector + FEMNIST + naive trim attack\n",
    "\n",
    "## Observation: FLdetector correctly identifies the malicious clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f780c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "weight_record = []\n",
    "grad_record = []\n",
    "test_grads = []\n",
    "malicious_scores = np.zeros((1, num_workers))\n",
    "attack = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = cnn().to(device)\n",
    "lr = 0.25\n",
    "best_val_acc=0.0\n",
    "\n",
    "start_detection_epoch = 10\n",
    "window_size = 10\n",
    "assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "for e in range(100):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr*(0.995**e), weight_decay=1e-6)\n",
    "    user_grads = []\n",
    "\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].reshape(-1, 1, 28, 28).cuda())\n",
    "        loss = criterion(output, each_worker_label[i].cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "    tmp = []\n",
    "    for param in net.parameters():\n",
    "        tmp = param.data.view(-1) if not len(tmp) else torch.cat((tmp, param.data.view(-1)))\n",
    "    weight = tmp    \n",
    "\n",
    "    if (e > start_detection_epoch):\n",
    "        hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "        hvp = np.squeeze(hvp)\n",
    "    else:\n",
    "        hvp = None\n",
    "\n",
    "    if attack:\n",
    "        if(e>0):\n",
    "            user_grads = full_trim(user_grads, 20)\n",
    "\n",
    "    agg_grads, distance = simple_mean(old_grad_list, user_grads, nbyz, hvp)\n",
    "    \n",
    "    if distance is not None and e > (start_detection_epoch - window_size):\n",
    "        print('==>', e, malicious_scores.shape)\n",
    "        malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "    if malicious_scores.shape[0] >= 11:\n",
    "        print('performing detection at epoch %d' % e)\n",
    "        if detection1(np.sum(malicious_scores[-10:], axis=0), 20):\n",
    "            print('Stop at iteration:', e)\n",
    "            detection(np.sum(malicious_scores[-10:], axis=0), 20)\n",
    "            break\n",
    "\n",
    "#     agg_grads = tr_mean(user_grads, 20)\n",
    "#     agg_grads=torch.median(user_grads,dim=0)[0]\n",
    "#     agg_grads=torch.mean(user_grads,dim=0)\n",
    "\n",
    "    if e > (start_detection_epoch - window_size):\n",
    "        weight_record.append(weight - last_weight)\n",
    "        grad_record.append(agg_grads - last_grad)\n",
    "    \n",
    "    if (len(weight_record) > 10):\n",
    "        del weight_record[0]\n",
    "        del grad_record[0]\n",
    "    \n",
    "    last_weight = weight\n",
    "    last_grad = agg_grads\n",
    "    old_grad_list = user_grads\n",
    "  \n",
    "    del user_grads\n",
    "    \n",
    "    start_idx=0\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "\n",
    "    model_grads=[]\n",
    "\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    val_loss, val_acc = test(val_data_tensor.reshape(-1,1,28,28), val_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    is_best = best_val_acc < val_acc\n",
    "    best_val_acc = max(best_val_acc, val_acc)\n",
    "    if is_best:\n",
    "        _, best_te_acc = test(te_data_tensor.reshape(-1,1,28,28), te_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    print('e %d | val_loss %.3f val acc %.3f | best val_acc %.2f test_acc %.2f' % (\n",
    "        e, val_loss, val_acc, best_val_acc, best_te_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ed1b8",
   "metadata": {},
   "source": [
    "# FLDetector + FEMNIST + hvp \n",
    "\n",
    "## Method: malicious clients compute updates that FLDetector would use for detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd4fb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "weight_record = []\n",
    "grad_record = []\n",
    "test_grads = []\n",
    "malicious_scores = np.zeros((1, num_workers))\n",
    "attack = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = cnn().to(device)\n",
    "lr = 0.25\n",
    "best_val_acc=0.0\n",
    "attack_type = 'none'\n",
    "use_cuda = True\n",
    "start_detection_epoch = 20\n",
    "window_size = 10\n",
    "assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "for e in range(100):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr*(0.995**e), weight_decay=1e-6)\n",
    "    user_grads = []\n",
    "\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].reshape(-1, 1, 28, 28).cuda())\n",
    "        loss = criterion(output, each_worker_label[i].cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "    tmp = []\n",
    "    for param in net.parameters():\n",
    "        tmp = param.data.view(-1) if not len(tmp) else torch.cat((tmp, param.data.view(-1)))\n",
    "    weight = tmp    \n",
    "\n",
    "    if (e > start_detection_epoch):\n",
    "        hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "        hvp = np.squeeze(hvp)\n",
    "    else:\n",
    "        hvp = None\n",
    "\n",
    "    if attack:\n",
    "        if(e<start_detection_epoch):\n",
    "            user_grads = full_trim(user_grads, nbyz)\n",
    "        elif e>start_detection_epoch:\n",
    "            for m in range(nbyz):\n",
    "                if attack_type == 'LIE':\n",
    "                    z = 0.2\n",
    "                    noise_avg = torch.mean(user_grads[:nbyz], dim=0)\n",
    "                    noise_std = torch.std(user_grads[:nbyz], dim=0)\n",
    "                    noise = noise_avg + z * noise_std\n",
    "                elif attack_type == 'NDSS21':\n",
    "                    pass\n",
    "                else:\n",
    "                    noise = torch.zeros(hvp.shape)\n",
    "                user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise.to(device)\n",
    "\n",
    "#     agg_grads, distance = simple_mean(old_grad_list, user_grads, nbyz, hvp)\n",
    "    agg_grads, distance = trimmed_mean(old_grad_list, user_grads, nbyz, hvp)\n",
    "    \n",
    "    if distance is not None and e > (start_detection_epoch - window_size):\n",
    "        print('==>', e, malicious_scores.shape)\n",
    "        malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "    if malicious_scores.shape[0] >= 11:\n",
    "        print('performing detection at epoch %d' % e)\n",
    "        if detection1(np.sum(malicious_scores[-10:], axis=0), 20):\n",
    "            print('Stop at iteration:', e)\n",
    "            detection(np.sum(malicious_scores[-10:], axis=0), 20)\n",
    "            break\n",
    "\n",
    "#     agg_grads = tr_mean(user_grads, 20)\n",
    "#     agg_grads=torch.median(user_grads,dim=0)[0]\n",
    "#     agg_grads=torch.mean(user_grads,dim=0)\n",
    "\n",
    "    if e > (start_detection_epoch - window_size):\n",
    "        weight_record.append(weight - last_weight)\n",
    "        grad_record.append(agg_grads - last_grad)\n",
    "    \n",
    "    if (len(weight_record) > 10):\n",
    "        del weight_record[0]\n",
    "        del grad_record[0]\n",
    "    \n",
    "    last_weight = weight\n",
    "    last_grad = agg_grads\n",
    "    old_grad_list = user_grads\n",
    "  \n",
    "    del user_grads\n",
    "    \n",
    "    start_idx=0\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "\n",
    "    model_grads=[]\n",
    "\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    val_loss, val_acc = test(val_data_tensor.reshape(-1,1,28,28), val_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    is_best = best_val_acc < val_acc\n",
    "    best_val_acc = max(best_val_acc, val_acc)\n",
    "    if is_best:\n",
    "        _, best_te_acc = test(te_data_tensor.reshape(-1,1,28,28), te_label_tensor, net, criterion, use_cuda, batch_size=50)\n",
    "    print('e %d | val_loss %.3f val acc %.3f | best val_acc %.2f test_acc %.2f' % (\n",
    "        e, val_loss, val_acc, best_val_acc, best_te_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(malicious_scores[-10:],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1907e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
