{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for CIFAR10 with Fang distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e300ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu122/4403391/ipykernel_3957723/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "import copy\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77aa30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_models import *\n",
    "from cifar10_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "\n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fang_train_data(trainset, num_workers=100, bias=0.5, force=False):\n",
    "    dist_file = 'fang_nworkers%d_bias%.1f.pkl' % (num_workers, bias)\n",
    "    if not force and os.path.exists(dist_file):\n",
    "        print('Loading fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "        return pickle.load(open(dist_file, 'rb'))\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    per_participant_list = defaultdict(list)\n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        per_participant_list[selected_worker].extend([i])\n",
    "    \n",
    "    print('Saving fang distribution for num_workers %d and bias %.1f to memory' % (num_workers, bias))\n",
    "    pickle.dump(per_participant_list, open(dist_file, 'wb'))\n",
    "    return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799ffb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_federated_data(trainset, num_workers, distribution='fang', param=1, force=False):\n",
    "    if distribution == 'fang':\n",
    "        per_participant_list = get_fang_train_data(trainset, num_workers, bias=param, force=force)\n",
    "    elif distribution == 'dirichlet':\n",
    "        per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=param, force=force)\n",
    "\n",
    "    each_worker_idx = [[] for _ in range(num_workers)] \n",
    "    each_worker_te_idx = [[] for _ in range(num_workers)]\n",
    "    np.random.seed(0)\n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(5*w_len/6)\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        each_worker_idx[worker_idx] = w_indices[tr_idx]\n",
    "        each_worker_te_idx[worker_idx] = w_indices[te_idx]\n",
    "    global_test_idx = np.concatenate(each_worker_te_idx)\n",
    "    return each_worker_idx, each_worker_te_idx, global_test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2590db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(dataset, indices, batch_size=32, shuffle=False):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acbb06d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/home/vshejwalkar_umass_edu/data/'\n",
    "# load the train dataset\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_train)\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_train)\n",
    "\n",
    "te_cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_test)\n",
    "te_cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.sum(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(trainloader, model, model_received, criterion, optimizer, pgd=False, eps=2):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size()[0])\n",
    "        top1.update(prec1.item()/100.0, inputs.size()[0])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if pgd:\n",
    "            curr_model = list(model.parameters())\n",
    "            curr_model_vec = parameters_to_vector(curr_model)\n",
    "            if torch.norm(curr_model_vec - model_received) > eps:\n",
    "                curr_model_vec = eps*(curr_model_vec - model_received)/torch.norm(curr_model_vec - model_received) + model_received\n",
    "                vector_to_parameters(curr_model_vec, curr_model)\n",
    "        \n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def test(testloader, model, criterion):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(testloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        top1.update(prec1/100.0, inputs.size()[0])\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8b7ed",
   "metadata": {},
   "source": [
    "# resnet-BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5204c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e25e2ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fang distribution for num_workers 100 and bias 0.5 from memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 100, 10038)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "batch_size = 32\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = False\n",
    "\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=100, distribution=distribution, param=param, force=force)\n",
    "\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    train_loaders.append((pos, get_train(all_data, indices, batch_size)))\n",
    "test_loaders = []\n",
    "for pos, indices in enumerate(each_worker_te_idx):\n",
    "    test_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "len(train_loaders), len(test_loaders), len(global_test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "963baed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loaders[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62611dc3",
   "metadata": {},
   "source": [
    "# Mean with FedNOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed6f34",
   "metadata": {},
   "source": [
    "# Attack Resnet20BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb89ff50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data len:  49962\n",
      "client_data_len shape:  (100,)\n",
      "tau_per_client shape:  (100,)\n",
      "client_ws shape:  (100,)\n",
      "len tau_per_client 100 client_data_len 100\n",
      "len client_ws 100 renormalized_client_ws 100\n",
      "e 0 round_ema_decay 0.1000 | val loss 2.319 val acc 0.117 best val_acc 0.117 | ema loss 2.321 acc 0.116 best acc 0.116\n",
      "e 1 round_ema_decay 0.1818 | val loss 2.318 val acc 0.103 best val_acc 0.117 | ema loss 2.317 acc 0.103 best acc 0.116\n",
      "e 2 round_ema_decay 0.2500 | val loss 2.278 val acc 0.111 best val_acc 0.117 | ema loss 2.287 acc 0.105 best acc 0.116\n",
      "e 3 round_ema_decay 0.3077 | val loss 2.217 val acc 0.141 best val_acc 0.141 | ema loss 2.249 acc 0.126 best acc 0.126\n",
      "e 4 round_ema_decay 0.3571 | val loss 2.148 val acc 0.166 best val_acc 0.166 | ema loss 2.194 acc 0.151 best acc 0.151\n",
      "e 5 round_ema_decay 0.4000 | val loss 2.103 val acc 0.185 best val_acc 0.185 | ema loss 2.147 acc 0.165 best acc 0.165\n",
      "e 6 round_ema_decay 0.4375 | val loss 2.052 val acc 0.210 best val_acc 0.210 | ema loss 2.099 acc 0.186 best acc 0.186\n",
      "e 7 round_ema_decay 0.4706 | val loss 2.012 val acc 0.223 best val_acc 0.223 | ema loss 2.058 acc 0.204 best acc 0.204\n",
      "e 8 round_ema_decay 0.5000 | val loss 1.980 val acc 0.237 best val_acc 0.237 | ema loss 2.021 acc 0.219 best acc 0.219\n",
      "e 9 round_ema_decay 0.5263 | val loss 1.932 val acc 0.246 best val_acc 0.246 | ema loss 1.981 acc 0.228 best acc 0.228\n",
      "e 10 round_ema_decay 0.5500 | val loss 1.874 val acc 0.267 best val_acc 0.267 | ema loss 1.933 acc 0.247 best acc 0.247\n",
      "e 11 round_ema_decay 0.5714 | val loss 1.804 val acc 0.317 best val_acc 0.317 | ema loss 1.881 acc 0.272 best acc 0.272\n",
      "e 12 round_ema_decay 0.5909 | val loss 1.775 val acc 0.329 best val_acc 0.329 | ema loss 1.839 acc 0.295 best acc 0.295\n",
      "e 13 round_ema_decay 0.6087 | val loss 1.763 val acc 0.340 best val_acc 0.340 | ema loss 1.810 acc 0.314 best acc 0.314\n",
      "e 14 round_ema_decay 0.6250 | val loss 1.744 val acc 0.357 best val_acc 0.357 | ema loss 1.783 acc 0.336 best acc 0.336\n",
      "e 15 round_ema_decay 0.6400 | val loss 1.711 val acc 0.366 best val_acc 0.366 | ema loss 1.758 acc 0.345 best acc 0.345\n",
      "e 16 round_ema_decay 0.6538 | val loss 1.750 val acc 0.352 best val_acc 0.366 | ema loss 1.755 acc 0.349 best acc 0.349\n",
      "e 17 round_ema_decay 0.6667 | val loss 1.707 val acc 0.371 best val_acc 0.371 | ema loss 1.739 acc 0.360 best acc 0.360\n",
      "e 18 round_ema_decay 0.6786 | val loss 1.741 val acc 0.362 best val_acc 0.371 | ema loss 1.737 acc 0.360 best acc 0.360\n",
      "e 19 round_ema_decay 0.6897 | val loss 1.718 val acc 0.365 best val_acc 0.371 | ema loss 1.731 acc 0.361 best acc 0.361\n",
      "e 20 round_ema_decay 0.7000 | val loss 1.727 val acc 0.365 best val_acc 0.371 | ema loss 1.728 acc 0.363 best acc 0.363\n",
      "e 21 round_ema_decay 0.7097 | val loss 1.771 val acc 0.354 best val_acc 0.371 | ema loss 1.737 acc 0.360 best acc 0.363\n",
      "e 22 round_ema_decay 0.7188 | val loss 1.811 val acc 0.339 best val_acc 0.371 | ema loss 1.752 acc 0.357 best acc 0.363\n",
      "e 23 round_ema_decay 0.7273 | val loss 1.747 val acc 0.366 best val_acc 0.371 | ema loss 1.750 acc 0.358 best acc 0.363\n",
      "e 24 round_ema_decay 0.7353 | val loss 1.690 val acc 0.383 best val_acc 0.383 | ema loss 1.737 acc 0.365 best acc 0.365\n",
      "e 25 round_ema_decay 0.7429 | val loss 1.675 val acc 0.389 best val_acc 0.389 | ema loss 1.724 acc 0.372 best acc 0.372\n",
      "e 26 round_ema_decay 0.7500 | val loss 1.728 val acc 0.375 best val_acc 0.389 | ema loss 1.728 acc 0.372 best acc 0.372\n",
      "e 27 round_ema_decay 0.7568 | val loss 1.672 val acc 0.389 best val_acc 0.389 | ema loss 1.715 acc 0.376 best acc 0.376\n",
      "e 28 round_ema_decay 0.7632 | val loss 1.644 val acc 0.399 best val_acc 0.399 | ema loss 1.698 acc 0.382 best acc 0.382\n",
      "e 29 round_ema_decay 0.7692 | val loss 1.640 val acc 0.401 best val_acc 0.401 | ema loss 1.684 acc 0.387 best acc 0.387\n",
      "e 30 round_ema_decay 0.7750 | val loss 1.607 val acc 0.405 best val_acc 0.405 | ema loss 1.663 acc 0.391 best acc 0.391\n",
      "e 31 round_ema_decay 0.7805 | val loss 1.627 val acc 0.405 best val_acc 0.405 | ema loss 1.655 acc 0.393 best acc 0.393\n",
      "e 32 round_ema_decay 0.7857 | val loss 1.589 val acc 0.415 best val_acc 0.415 | ema loss 1.637 acc 0.401 best acc 0.401\n",
      "e 35 round_ema_decay 0.8000 | val loss 1.603 val acc 0.419 best val_acc 0.421 | ema loss 1.615 acc 0.411 best acc 0.411\n",
      "e 36 round_ema_decay 0.8043 | val loss 1.599 val acc 0.419 best val_acc 0.421 | ema loss 1.612 acc 0.412 best acc 0.412\n",
      "e 37 round_ema_decay 0.8085 | val loss 1.606 val acc 0.416 best val_acc 0.421 | ema loss 1.610 acc 0.413 best acc 0.413\n",
      "e 38 round_ema_decay 0.8125 | val loss 1.649 val acc 0.407 best val_acc 0.421 | ema loss 1.615 acc 0.413 best acc 0.413\n",
      "e 39 round_ema_decay 0.8163 | val loss 1.628 val acc 0.413 best val_acc 0.421 | ema loss 1.616 acc 0.413 best acc 0.413\n",
      "e 40 round_ema_decay 0.8200 | val loss 1.592 val acc 0.419 best val_acc 0.421 | ema loss 1.608 acc 0.415 best acc 0.415\n",
      "e 41 round_ema_decay 0.8235 | val loss 1.596 val acc 0.421 best val_acc 0.421 | ema loss 1.603 acc 0.418 best acc 0.418\n",
      "e 42 round_ema_decay 0.8269 | val loss 1.569 val acc 0.432 best val_acc 0.432 | ema loss 1.596 acc 0.421 best acc 0.421\n",
      "e 43 round_ema_decay 0.8302 | val loss 1.574 val acc 0.437 best val_acc 0.437 | ema loss 1.591 acc 0.424 best acc 0.424\n",
      "e 44 round_ema_decay 0.8333 | val loss 1.576 val acc 0.434 best val_acc 0.437 | ema loss 1.588 acc 0.425 best acc 0.425\n",
      "e 45 round_ema_decay 0.8364 | val loss 1.550 val acc 0.441 best val_acc 0.441 | ema loss 1.580 acc 0.429 best acc 0.429\n",
      "e 46 round_ema_decay 0.8393 | val loss 1.530 val acc 0.452 best val_acc 0.452 | ema loss 1.570 acc 0.433 best acc 0.433\n",
      "e 47 round_ema_decay 0.8421 | val loss 1.537 val acc 0.449 best val_acc 0.452 | ema loss 1.564 acc 0.437 best acc 0.437\n",
      "e 48 round_ema_decay 0.8448 | val loss 1.526 val acc 0.452 best val_acc 0.452 | ema loss 1.557 acc 0.438 best acc 0.438\n",
      "e 49 round_ema_decay 0.8475 | val loss 1.521 val acc 0.454 best val_acc 0.454 | ema loss 1.549 acc 0.441 best acc 0.441\n",
      "e 50 round_ema_decay 0.8500 | val loss 1.491 val acc 0.468 best val_acc 0.468 | ema loss 1.539 acc 0.446 best acc 0.446\n",
      "e 51 round_ema_decay 0.8525 | val loss 1.466 val acc 0.478 best val_acc 0.478 | ema loss 1.527 acc 0.450 best acc 0.450\n",
      "e 52 round_ema_decay 0.8548 | val loss 1.485 val acc 0.471 best val_acc 0.478 | ema loss 1.519 acc 0.454 best acc 0.454\n",
      "e 53 round_ema_decay 0.8571 | val loss 1.480 val acc 0.474 best val_acc 0.478 | ema loss 1.513 acc 0.457 best acc 0.457\n",
      "e 54 round_ema_decay 0.8594 | val loss 1.521 val acc 0.465 best val_acc 0.478 | ema loss 1.513 acc 0.460 best acc 0.460\n",
      "e 55 round_ema_decay 0.8615 | val loss 1.512 val acc 0.468 best val_acc 0.478 | ema loss 1.512 acc 0.462 best acc 0.462\n",
      "e 56 round_ema_decay 0.8636 | val loss 1.519 val acc 0.464 best val_acc 0.478 | ema loss 1.513 acc 0.464 best acc 0.464\n",
      "e 57 round_ema_decay 0.8657 | val loss 1.490 val acc 0.472 best val_acc 0.478 | ema loss 1.510 acc 0.466 best acc 0.466\n",
      "e 58 round_ema_decay 0.8676 | val loss 1.499 val acc 0.468 best val_acc 0.478 | ema loss 1.508 acc 0.467 best acc 0.467\n",
      "e 59 round_ema_decay 0.8696 | val loss 1.499 val acc 0.467 best val_acc 0.478 | ema loss 1.505 acc 0.468 best acc 0.468\n",
      "e 60 round_ema_decay 0.8714 | val loss 1.486 val acc 0.472 best val_acc 0.478 | ema loss 1.501 acc 0.469 best acc 0.469\n",
      "e 61 round_ema_decay 0.8732 | val loss 1.488 val acc 0.470 best val_acc 0.478 | ema loss 1.498 acc 0.470 best acc 0.470\n",
      "e 62 round_ema_decay 0.8750 | val loss 1.474 val acc 0.476 best val_acc 0.478 | ema loss 1.494 acc 0.471 best acc 0.471\n",
      "e 63 round_ema_decay 0.8767 | val loss 1.467 val acc 0.478 best val_acc 0.478 | ema loss 1.491 acc 0.472 best acc 0.472\n",
      "e 64 round_ema_decay 0.8784 | val loss 1.472 val acc 0.480 best val_acc 0.480 | ema loss 1.489 acc 0.472 best acc 0.472\n",
      "e 65 round_ema_decay 0.8800 | val loss 1.493 val acc 0.473 best val_acc 0.480 | ema loss 1.488 acc 0.471 best acc 0.472\n",
      "e 66 round_ema_decay 0.8816 | val loss 1.479 val acc 0.478 best val_acc 0.480 | ema loss 1.487 acc 0.472 best acc 0.472\n",
      "e 67 round_ema_decay 0.8831 | val loss 1.486 val acc 0.474 best val_acc 0.480 | ema loss 1.487 acc 0.475 best acc 0.475\n",
      "e 68 round_ema_decay 0.8846 | val loss 1.510 val acc 0.469 best val_acc 0.480 | ema loss 1.488 acc 0.474 best acc 0.475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 69 round_ema_decay 0.8861 | val loss 1.511 val acc 0.472 best val_acc 0.480 | ema loss 1.490 acc 0.473 best acc 0.475\n",
      "e 70 round_ema_decay 0.8875 | val loss 1.498 val acc 0.472 best val_acc 0.480 | ema loss 1.491 acc 0.471 best acc 0.475\n",
      "e 71 round_ema_decay 0.8889 | val loss 1.484 val acc 0.471 best val_acc 0.480 | ema loss 1.490 acc 0.471 best acc 0.475\n",
      "e 72 round_ema_decay 0.8902 | val loss 1.504 val acc 0.465 best val_acc 0.480 | ema loss 1.492 acc 0.472 best acc 0.475\n",
      "e 73 round_ema_decay 0.8916 | val loss 1.469 val acc 0.478 best val_acc 0.480 | ema loss 1.490 acc 0.472 best acc 0.475\n",
      "e 74 round_ema_decay 0.8929 | val loss 1.511 val acc 0.469 best val_acc 0.480 | ema loss 1.493 acc 0.472 best acc 0.475\n",
      "e 75 round_ema_decay 0.8941 | val loss 1.534 val acc 0.466 best val_acc 0.480 | ema loss 1.497 acc 0.470 best acc 0.475\n",
      "e 76 round_ema_decay 0.8953 | val loss 1.494 val acc 0.473 best val_acc 0.480 | ema loss 1.498 acc 0.469 best acc 0.475\n",
      "e 77 round_ema_decay 0.8966 | val loss 1.546 val acc 0.462 best val_acc 0.480 | ema loss 1.503 acc 0.468 best acc 0.475\n",
      "e 78 round_ema_decay 0.8977 | val loss 1.533 val acc 0.466 best val_acc 0.480 | ema loss 1.507 acc 0.468 best acc 0.475\n",
      "e 79 round_ema_decay 0.8989 | val loss 1.540 val acc 0.462 best val_acc 0.480 | ema loss 1.511 acc 0.467 best acc 0.475\n",
      "e 80 round_ema_decay 0.9000 | val loss 1.562 val acc 0.456 best val_acc 0.480 | ema loss 1.517 acc 0.465 best acc 0.475\n",
      "e 81 round_ema_decay 0.9000 | val loss 1.577 val acc 0.451 best val_acc 0.480 | ema loss 1.524 acc 0.464 best acc 0.475\n",
      "e 82 round_ema_decay 0.9000 | val loss 1.611 val acc 0.442 best val_acc 0.480 | ema loss 1.532 acc 0.463 best acc 0.475\n",
      "e 83 round_ema_decay 0.9000 | val loss 1.633 val acc 0.440 best val_acc 0.480 | ema loss 1.540 acc 0.462 best acc 0.475\n",
      "e 84 round_ema_decay 0.9000 | val loss 1.639 val acc 0.441 best val_acc 0.480 | ema loss 1.549 acc 0.462 best acc 0.475\n",
      "e 85 round_ema_decay 0.9000 | val loss 1.576 val acc 0.453 best val_acc 0.480 | ema loss 1.552 acc 0.461 best acc 0.475\n",
      "e 86 round_ema_decay 0.9000 | val loss 1.525 val acc 0.473 best val_acc 0.480 | ema loss 1.551 acc 0.461 best acc 0.475\n",
      "e 87 round_ema_decay 0.9000 | val loss 1.583 val acc 0.459 best val_acc 0.480 | ema loss 1.555 acc 0.461 best acc 0.475\n",
      "e 88 round_ema_decay 0.9000 | val loss 1.602 val acc 0.454 best val_acc 0.480 | ema loss 1.561 acc 0.460 best acc 0.475\n",
      "e 89 round_ema_decay 0.9000 | val loss 1.635 val acc 0.446 best val_acc 0.480 | ema loss 1.568 acc 0.458 best acc 0.475\n",
      "e 90 round_ema_decay 0.9000 | val loss 1.625 val acc 0.446 best val_acc 0.480 | ema loss 1.574 acc 0.457 best acc 0.475\n",
      "e 91 round_ema_decay 0.9000 | val loss 1.646 val acc 0.445 best val_acc 0.480 | ema loss 1.582 acc 0.454 best acc 0.475\n",
      "e 92 round_ema_decay 0.9000 | val loss 1.664 val acc 0.431 best val_acc 0.480 | ema loss 1.591 acc 0.452 best acc 0.475\n",
      "e 93 round_ema_decay 0.9000 | val loss 1.642 val acc 0.436 best val_acc 0.480 | ema loss 1.598 acc 0.448 best acc 0.475\n",
      "e 94 round_ema_decay 0.9000 | val loss 1.663 val acc 0.438 best val_acc 0.480 | ema loss 1.604 acc 0.449 best acc 0.475\n",
      "e 95 round_ema_decay 0.9000 | val loss 1.597 val acc 0.457 best val_acc 0.480 | ema loss 1.603 acc 0.450 best acc 0.475\n",
      "e 96 round_ema_decay 0.9000 | val loss 1.531 val acc 0.467 best val_acc 0.480 | ema loss 1.596 acc 0.452 best acc 0.475\n",
      "e 97 round_ema_decay 0.9000 | val loss 1.590 val acc 0.458 best val_acc 0.480 | ema loss 1.594 acc 0.453 best acc 0.475\n",
      "e 98 round_ema_decay 0.9000 | val loss 1.537 val acc 0.464 best val_acc 0.480 | ema loss 1.589 acc 0.455 best acc 0.475\n",
      "e 99 round_ema_decay 0.9000 | val loss 1.581 val acc 0.453 best val_acc 0.480 | ema loss 1.588 acc 0.456 best acc 0.475\n",
      "e 100 round_ema_decay 0.9000 | val loss 1.575 val acc 0.459 best val_acc 0.480 | ema loss 1.586 acc 0.457 best acc 0.475\n",
      "e 101 round_ema_decay 0.9000 | val loss 1.536 val acc 0.469 best val_acc 0.480 | ema loss 1.581 acc 0.458 best acc 0.475\n",
      "e 102 round_ema_decay 0.9000 | val loss 1.546 val acc 0.463 best val_acc 0.480 | ema loss 1.578 acc 0.459 best acc 0.475\n",
      "e 103 round_ema_decay 0.9000 | val loss 1.563 val acc 0.463 best val_acc 0.480 | ema loss 1.578 acc 0.459 best acc 0.475\n",
      "e 104 round_ema_decay 0.9000 | val loss 1.550 val acc 0.466 best val_acc 0.480 | ema loss 1.576 acc 0.460 best acc 0.475\n",
      "e 105 round_ema_decay 0.9000 | val loss 1.506 val acc 0.479 best val_acc 0.480 | ema loss 1.569 acc 0.462 best acc 0.475\n",
      "e 106 round_ema_decay 0.9000 | val loss 1.547 val acc 0.463 best val_acc 0.480 | ema loss 1.568 acc 0.462 best acc 0.475\n",
      "e 107 round_ema_decay 0.9000 | val loss 1.543 val acc 0.469 best val_acc 0.480 | ema loss 1.567 acc 0.462 best acc 0.475\n",
      "e 108 round_ema_decay 0.9000 | val loss 1.593 val acc 0.457 best val_acc 0.480 | ema loss 1.570 acc 0.461 best acc 0.475\n",
      "e 109 round_ema_decay 0.9000 | val loss 1.555 val acc 0.473 best val_acc 0.480 | ema loss 1.570 acc 0.462 best acc 0.475\n",
      "e 110 round_ema_decay 0.9000 | val loss 1.531 val acc 0.475 best val_acc 0.480 | ema loss 1.566 acc 0.464 best acc 0.475\n",
      "e 111 round_ema_decay 0.9000 | val loss 1.500 val acc 0.478 best val_acc 0.480 | ema loss 1.559 acc 0.464 best acc 0.475\n",
      "e 112 round_ema_decay 0.9000 | val loss 1.523 val acc 0.477 best val_acc 0.480 | ema loss 1.556 acc 0.465 best acc 0.475\n",
      "e 113 round_ema_decay 0.9000 | val loss 1.484 val acc 0.478 best val_acc 0.480 | ema loss 1.548 acc 0.466 best acc 0.475\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m local_lr\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m0.999\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mepoch_num), momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(local_epochs):\n\u001b[0;32m---> 73\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_received\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m params \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, (name, param) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mitems()):\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(trainloader, model, model_received, criterion, optimizer, pgd, eps)\u001b[0m\n\u001b[1;32m     14\u001b[0m top1\u001b[38;5;241m.\u001b[39mupdate(prec1\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100.0\u001b[39m, inputs\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pgd:\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 32\n",
    "num_workers = 100\n",
    "local_lr = 0.1\n",
    "global_lr = 1\n",
    "nbyz = 20\n",
    "total_data_len = 0\n",
    "client_data_len = []\n",
    "tau_per_client = []\n",
    "\n",
    "for client_idx, worker_indices in enumerate(each_worker_idx[20-nbyz:]):\n",
    "    if client_idx < nbyz:\n",
    "        total_data_len += len(worker_indices)\n",
    "        client_data_len.append(len(worker_indices))\n",
    "        tau_per_client.append(len(train_loaders[client_idx][1]))\n",
    "    else:\n",
    "        total_data_len += len(worker_indices)\n",
    "        client_data_len.append(len(worker_indices))\n",
    "        tau_per_client.append(len(train_loaders[client_idx][1]))\n",
    "client_data_len = np.array(client_data_len)\n",
    "tau_per_client = np.array(tau_per_client)\n",
    "client_ws = client_data_len / total_data_len\n",
    "\n",
    "print('total data len: ', total_data_len)\n",
    "print('client_data_len shape: ', client_data_len.shape)\n",
    "print('tau_per_client shape: ', tau_per_client.shape)\n",
    "print('client_ws shape: ', client_ws.shape)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_global_acc = best_ema_acc = 0\n",
    "epoch_num = 0\n",
    "ema_decay = 0.9\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "ema_model_received = copy.deepcopy(model_received)\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(20-nbyz, num_workers)\n",
    "    round_benign = round_clients[nbyz:]\n",
    "    round_malicious = round_clients[:nbyz]\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    \n",
    "    for client_idx in round_malicious:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[client_idx][1], model, model_received, criterion, optimizer)\n",
    "        params = []\n",
    "        for _, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "        update =  (params - model_received)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    for client_idx in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[client_idx][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for _, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    if epoch_num == 0:\n",
    "        renormalized_client_weights = client_ws * np.sum(client_ws * tau_per_client) / tau_per_client\n",
    "        print('len tau_per_client %d client_data_len %d' % (len(tau_per_client), len(client_data_len)))\n",
    "        print('len client_ws %d renormalized_client_ws %d' % (len(client_ws), len(renormalized_client_weights)))\n",
    "\n",
    "    user_updates *= torch.from_numpy(renormalized_client_weights)[:, None].to(device)\n",
    "    user_updates[:nbyz] = full_trim(user_updates[:nbyz], nbyz)\n",
    "    agg_update = tr_mean(user_updates, nbyz)\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * (.999**epoch_num) * agg_update\n",
    "    round_ema_decay = min(ema_decay, (1+epoch_num)/(10+epoch_num))\n",
    "    ema_model_received = ema_model_received * round_ema_decay + model_received * (1 - round_ema_decay)\n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "\n",
    "    ema_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = ema_model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    ema_model.load_state_dict(state_dict)\n",
    "\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    ema_loss, ema_acc = test(cifar10_test_loader, ema_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    best_ema_acc = max(best_ema_acc, ema_acc)\n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "\n",
    "    if epoch_num%1==0 or epoch_num==nepochs-1:\n",
    "        print('e %d round_ema_decay %.4f | val loss %.3f val acc %.3f best val_acc %.3f | ema loss %.3f acc %.3f best acc %.3f'% (\n",
    "            epoch_num, round_ema_decay,\n",
    "            val_loss, val_acc, best_global_acc,\n",
    "            ema_loss, ema_acc, best_ema_acc))\n",
    "\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c1a42d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data len:  49962\n",
      "client_data_len shape:  (100,)\n",
      "tau_per_client shape:  (100,)\n",
      "client_ws shape:  (100,)\n",
      "len tau_per_client 100 client_data_len 100\n",
      "len client_ws 100 renormalized_client_ws 100\n",
      "e 0 round_ema_decay 0.1000 | val loss 2.448 val acc 0.121 best val_acc 0.121 | ema loss 2.477 acc 0.134 best acc 0.134\n",
      "e 1 round_ema_decay 0.1818 | val loss 2.296 val acc 0.173 best val_acc 0.173 | ema loss 2.310 acc 0.173 best acc 0.173\n",
      "e 2 round_ema_decay 0.2500 | val loss 2.238 val acc 0.173 best val_acc 0.173 | ema loss 2.252 acc 0.173 best acc 0.173\n",
      "e 3 round_ema_decay 0.3077 | val loss 2.196 val acc 0.175 best val_acc 0.175 | ema loss 2.217 acc 0.173 best acc 0.173\n",
      "e 4 round_ema_decay 0.3571 | val loss 2.126 val acc 0.186 best val_acc 0.186 | ema loss 2.162 acc 0.179 best acc 0.179\n",
      "e 5 round_ema_decay 0.4000 | val loss 2.061 val acc 0.202 best val_acc 0.202 | ema loss 2.104 acc 0.192 best acc 0.192\n",
      "e 6 round_ema_decay 0.4375 | val loss 1.966 val acc 0.248 best val_acc 0.248 | ema loss 2.026 acc 0.218 best acc 0.218\n",
      "e 7 round_ema_decay 0.4706 | val loss 1.935 val acc 0.249 best val_acc 0.249 | ema loss 1.978 acc 0.234 best acc 0.234\n",
      "e 8 round_ema_decay 0.5000 | val loss 1.863 val acc 0.286 best val_acc 0.286 | ema loss 1.923 acc 0.257 best acc 0.257\n",
      "e 9 round_ema_decay 0.5263 | val loss 1.825 val acc 0.291 best val_acc 0.291 | ema loss 1.877 acc 0.270 best acc 0.270\n",
      "e 10 round_ema_decay 0.5500 | val loss 1.797 val acc 0.311 best val_acc 0.311 | ema loss 1.843 acc 0.284 best acc 0.284\n",
      "e 11 round_ema_decay 0.5714 | val loss 1.792 val acc 0.319 best val_acc 0.319 | ema loss 1.822 acc 0.295 best acc 0.295\n",
      "e 12 round_ema_decay 0.5909 | val loss 1.765 val acc 0.334 best val_acc 0.334 | ema loss 1.798 acc 0.311 best acc 0.311\n",
      "e 13 round_ema_decay 0.6087 | val loss 1.777 val acc 0.324 best val_acc 0.334 | ema loss 1.788 acc 0.315 best acc 0.315\n",
      "e 14 round_ema_decay 0.6250 | val loss 1.780 val acc 0.322 best val_acc 0.334 | ema loss 1.784 acc 0.316 best acc 0.316\n",
      "e 15 round_ema_decay 0.6400 | val loss 1.746 val acc 0.347 best val_acc 0.347 | ema loss 1.770 acc 0.328 best acc 0.328\n",
      "e 16 round_ema_decay 0.6538 | val loss 1.702 val acc 0.369 best val_acc 0.369 | ema loss 1.748 acc 0.342 best acc 0.342\n",
      "e 17 round_ema_decay 0.6667 | val loss 1.705 val acc 0.367 best val_acc 0.369 | ema loss 1.736 acc 0.349 best acc 0.349\n",
      "e 18 round_ema_decay 0.6786 | val loss 1.696 val acc 0.371 best val_acc 0.371 | ema loss 1.724 acc 0.355 best acc 0.355\n",
      "e 19 round_ema_decay 0.6897 | val loss 1.646 val acc 0.394 best val_acc 0.394 | ema loss 1.702 acc 0.367 best acc 0.367\n",
      "e 20 round_ema_decay 0.7000 | val loss 1.606 val acc 0.408 best val_acc 0.408 | ema loss 1.672 acc 0.380 best acc 0.380\n",
      "e 21 round_ema_decay 0.7097 | val loss 1.604 val acc 0.410 best val_acc 0.410 | ema loss 1.650 acc 0.392 best acc 0.392\n",
      "e 22 round_ema_decay 0.7188 | val loss 1.591 val acc 0.416 best val_acc 0.416 | ema loss 1.631 acc 0.401 best acc 0.401\n",
      "e 23 round_ema_decay 0.7273 | val loss 1.588 val acc 0.416 best val_acc 0.416 | ema loss 1.618 acc 0.406 best acc 0.406\n",
      "e 24 round_ema_decay 0.7353 | val loss 1.615 val acc 0.404 best val_acc 0.416 | ema loss 1.616 acc 0.404 best acc 0.406\n",
      "e 25 round_ema_decay 0.7429 | val loss 1.619 val acc 0.407 best val_acc 0.416 | ema loss 1.616 acc 0.407 best acc 0.407\n",
      "e 26 round_ema_decay 0.7500 | val loss 1.599 val acc 0.415 best val_acc 0.416 | ema loss 1.613 acc 0.411 best acc 0.411\n",
      "e 27 round_ema_decay 0.7568 | val loss 1.595 val acc 0.420 best val_acc 0.420 | ema loss 1.610 acc 0.412 best acc 0.412\n",
      "e 28 round_ema_decay 0.7632 | val loss 1.566 val acc 0.430 best val_acc 0.430 | ema loss 1.602 acc 0.416 best acc 0.416\n",
      "e 29 round_ema_decay 0.7692 | val loss 1.576 val acc 0.425 best val_acc 0.430 | ema loss 1.597 acc 0.416 best acc 0.416\n",
      "e 30 round_ema_decay 0.7750 | val loss 1.579 val acc 0.425 best val_acc 0.430 | ema loss 1.595 acc 0.417 best acc 0.417\n",
      "e 31 round_ema_decay 0.7805 | val loss 1.592 val acc 0.420 best val_acc 0.430 | ema loss 1.595 acc 0.417 best acc 0.417\n",
      "e 32 round_ema_decay 0.7857 | val loss 1.603 val acc 0.418 best val_acc 0.430 | ema loss 1.596 acc 0.416 best acc 0.417\n",
      "e 33 round_ema_decay 0.7907 | val loss 1.575 val acc 0.430 best val_acc 0.430 | ema loss 1.592 acc 0.421 best acc 0.421\n",
      "e 34 round_ema_decay 0.7955 | val loss 1.576 val acc 0.424 best val_acc 0.430 | ema loss 1.587 acc 0.423 best acc 0.423\n",
      "e 35 round_ema_decay 0.8000 | val loss 1.564 val acc 0.428 best val_acc 0.430 | ema loss 1.585 acc 0.423 best acc 0.423\n",
      "e 36 round_ema_decay 0.8043 | val loss 1.587 val acc 0.428 best val_acc 0.430 | ema loss 1.585 acc 0.423 best acc 0.423\n",
      "e 37 round_ema_decay 0.8085 | val loss 1.603 val acc 0.420 best val_acc 0.430 | ema loss 1.589 acc 0.423 best acc 0.423\n",
      "e 38 round_ema_decay 0.8125 | val loss 1.563 val acc 0.431 best val_acc 0.431 | ema loss 1.584 acc 0.423 best acc 0.423\n",
      "e 39 round_ema_decay 0.8163 | val loss 1.532 val acc 0.443 best val_acc 0.443 | ema loss 1.575 acc 0.427 best acc 0.427\n",
      "e 40 round_ema_decay 0.8200 | val loss 1.562 val acc 0.432 best val_acc 0.443 | ema loss 1.573 acc 0.428 best acc 0.428\n",
      "e 41 round_ema_decay 0.8235 | val loss 1.565 val acc 0.427 best val_acc 0.443 | ema loss 1.568 acc 0.431 best acc 0.431\n",
      "e 42 round_ema_decay 0.8269 | val loss 1.520 val acc 0.445 best val_acc 0.445 | ema loss 1.558 acc 0.434 best acc 0.434\n",
      "e 43 round_ema_decay 0.8302 | val loss 1.506 val acc 0.451 best val_acc 0.451 | ema loss 1.548 acc 0.438 best acc 0.438\n",
      "e 44 round_ema_decay 0.8333 | val loss 1.463 val acc 0.467 best val_acc 0.467 | ema loss 1.533 acc 0.440 best acc 0.440\n",
      "e 45 round_ema_decay 0.8364 | val loss 1.463 val acc 0.465 best val_acc 0.467 | ema loss 1.521 acc 0.446 best acc 0.446\n",
      "e 46 round_ema_decay 0.8393 | val loss 1.471 val acc 0.458 best val_acc 0.467 | ema loss 1.513 acc 0.449 best acc 0.449\n",
      "e 47 round_ema_decay 0.8421 | val loss 1.474 val acc 0.462 best val_acc 0.467 | ema loss 1.507 acc 0.450 best acc 0.450\n",
      "e 48 round_ema_decay 0.8448 | val loss 1.456 val acc 0.469 best val_acc 0.469 | ema loss 1.499 acc 0.453 best acc 0.453\n",
      "e 49 round_ema_decay 0.8475 | val loss 1.467 val acc 0.466 best val_acc 0.469 | ema loss 1.494 acc 0.455 best acc 0.455\n",
      "e 50 round_ema_decay 0.8500 | val loss 1.453 val acc 0.472 best val_acc 0.472 | ema loss 1.488 acc 0.455 best acc 0.455\n",
      "e 51 round_ema_decay 0.8525 | val loss 1.413 val acc 0.490 best val_acc 0.490 | ema loss 1.476 acc 0.460 best acc 0.460\n",
      "e 52 round_ema_decay 0.8548 | val loss 1.404 val acc 0.491 best val_acc 0.491 | ema loss 1.464 acc 0.466 best acc 0.466\n",
      "e 53 round_ema_decay 0.8571 | val loss 1.423 val acc 0.488 best val_acc 0.491 | ema loss 1.456 acc 0.468 best acc 0.468\n",
      "e 54 round_ema_decay 0.8594 | val loss 1.413 val acc 0.489 best val_acc 0.491 | ema loss 1.449 acc 0.472 best acc 0.472\n",
      "e 55 round_ema_decay 0.8615 | val loss 1.392 val acc 0.498 best val_acc 0.498 | ema loss 1.438 acc 0.478 best acc 0.478\n",
      "e 56 round_ema_decay 0.8636 | val loss 1.409 val acc 0.492 best val_acc 0.498 | ema loss 1.432 acc 0.483 best acc 0.483\n",
      "e 57 round_ema_decay 0.8657 | val loss 1.378 val acc 0.503 best val_acc 0.503 | ema loss 1.421 acc 0.485 best acc 0.485\n",
      "e 58 round_ema_decay 0.8676 | val loss 1.391 val acc 0.504 best val_acc 0.504 | ema loss 1.414 acc 0.488 best acc 0.488\n",
      "e 59 round_ema_decay 0.8696 | val loss 1.397 val acc 0.498 best val_acc 0.504 | ema loss 1.409 acc 0.491 best acc 0.491\n",
      "e 60 round_ema_decay 0.8714 | val loss 1.377 val acc 0.507 best val_acc 0.507 | ema loss 1.404 acc 0.495 best acc 0.495\n",
      "e 61 round_ema_decay 0.8732 | val loss 1.379 val acc 0.503 best val_acc 0.507 | ema loss 1.400 acc 0.495 best acc 0.495\n",
      "e 62 round_ema_decay 0.8750 | val loss 1.381 val acc 0.503 best val_acc 0.507 | ema loss 1.398 acc 0.497 best acc 0.497\n",
      "e 63 round_ema_decay 0.8767 | val loss 1.387 val acc 0.504 best val_acc 0.507 | ema loss 1.396 acc 0.498 best acc 0.498\n",
      "e 64 round_ema_decay 0.8784 | val loss 1.405 val acc 0.499 best val_acc 0.507 | ema loss 1.396 acc 0.499 best acc 0.499\n",
      "e 65 round_ema_decay 0.8800 | val loss 1.401 val acc 0.498 best val_acc 0.507 | ema loss 1.396 acc 0.499 best acc 0.499\n",
      "e 66 round_ema_decay 0.8816 | val loss 1.375 val acc 0.509 best val_acc 0.509 | ema loss 1.393 acc 0.501 best acc 0.501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 67 round_ema_decay 0.8831 | val loss 1.352 val acc 0.513 best val_acc 0.513 | ema loss 1.386 acc 0.504 best acc 0.504\n",
      "e 68 round_ema_decay 0.8846 | val loss 1.326 val acc 0.521 best val_acc 0.521 | ema loss 1.377 acc 0.505 best acc 0.505\n",
      "e 69 round_ema_decay 0.8861 | val loss 1.316 val acc 0.525 best val_acc 0.525 | ema loss 1.366 acc 0.507 best acc 0.507\n",
      "e 70 round_ema_decay 0.8875 | val loss 1.321 val acc 0.518 best val_acc 0.525 | ema loss 1.358 acc 0.510 best acc 0.510\n",
      "e 71 round_ema_decay 0.8889 | val loss 1.323 val acc 0.523 best val_acc 0.525 | ema loss 1.351 acc 0.512 best acc 0.512\n",
      "e 72 round_ema_decay 0.8902 | val loss 1.328 val acc 0.524 best val_acc 0.525 | ema loss 1.345 acc 0.515 best acc 0.515\n",
      "e 73 round_ema_decay 0.8916 | val loss 1.336 val acc 0.517 best val_acc 0.525 | ema loss 1.342 acc 0.517 best acc 0.517\n",
      "e 74 round_ema_decay 0.8929 | val loss 1.343 val acc 0.513 best val_acc 0.525 | ema loss 1.340 acc 0.517 best acc 0.517\n",
      "e 75 round_ema_decay 0.8941 | val loss 1.324 val acc 0.523 best val_acc 0.525 | ema loss 1.337 acc 0.518 best acc 0.518\n",
      "e 76 round_ema_decay 0.8953 | val loss 1.316 val acc 0.529 best val_acc 0.529 | ema loss 1.333 acc 0.519 best acc 0.519\n",
      "e 77 round_ema_decay 0.8966 | val loss 1.319 val acc 0.529 best val_acc 0.529 | ema loss 1.329 acc 0.521 best acc 0.521\n",
      "e 78 round_ema_decay 0.8977 | val loss 1.326 val acc 0.529 best val_acc 0.529 | ema loss 1.327 acc 0.522 best acc 0.522\n",
      "e 79 round_ema_decay 0.8989 | val loss 1.317 val acc 0.530 best val_acc 0.530 | ema loss 1.324 acc 0.522 best acc 0.522\n",
      "e 80 round_ema_decay 0.9000 | val loss 1.312 val acc 0.533 best val_acc 0.533 | ema loss 1.321 acc 0.523 best acc 0.523\n",
      "e 81 round_ema_decay 0.9000 | val loss 1.309 val acc 0.534 best val_acc 0.534 | ema loss 1.319 acc 0.526 best acc 0.526\n",
      "e 82 round_ema_decay 0.9000 | val loss 1.332 val acc 0.525 best val_acc 0.534 | ema loss 1.319 acc 0.526 best acc 0.526\n",
      "e 83 round_ema_decay 0.9000 | val loss 1.312 val acc 0.534 best val_acc 0.534 | ema loss 1.316 acc 0.526 best acc 0.526\n",
      "e 84 round_ema_decay 0.9000 | val loss 1.312 val acc 0.537 best val_acc 0.537 | ema loss 1.314 acc 0.527 best acc 0.527\n",
      "e 85 round_ema_decay 0.9000 | val loss 1.329 val acc 0.527 best val_acc 0.537 | ema loss 1.312 acc 0.529 best acc 0.529\n",
      "e 86 round_ema_decay 0.9000 | val loss 1.342 val acc 0.523 best val_acc 0.537 | ema loss 1.313 acc 0.529 best acc 0.529\n",
      "e 87 round_ema_decay 0.9000 | val loss 1.312 val acc 0.530 best val_acc 0.537 | ema loss 1.312 acc 0.530 best acc 0.530\n",
      "e 88 round_ema_decay 0.9000 | val loss 1.314 val acc 0.535 best val_acc 0.537 | ema loss 1.311 acc 0.533 best acc 0.533\n",
      "e 89 round_ema_decay 0.9000 | val loss 1.319 val acc 0.534 best val_acc 0.537 | ema loss 1.311 acc 0.532 best acc 0.533\n",
      "e 90 round_ema_decay 0.9000 | val loss 1.315 val acc 0.535 best val_acc 0.537 | ema loss 1.310 acc 0.532 best acc 0.533\n",
      "e 91 round_ema_decay 0.9000 | val loss 1.317 val acc 0.534 best val_acc 0.537 | ema loss 1.310 acc 0.532 best acc 0.533\n",
      "e 92 round_ema_decay 0.9000 | val loss 1.334 val acc 0.528 best val_acc 0.537 | ema loss 1.310 acc 0.533 best acc 0.533\n",
      "e 93 round_ema_decay 0.9000 | val loss 1.352 val acc 0.522 best val_acc 0.537 | ema loss 1.311 acc 0.534 best acc 0.534\n",
      "e 94 round_ema_decay 0.9000 | val loss 1.328 val acc 0.532 best val_acc 0.537 | ema loss 1.310 acc 0.535 best acc 0.535\n",
      "e 95 round_ema_decay 0.9000 | val loss 1.321 val acc 0.535 best val_acc 0.537 | ema loss 1.310 acc 0.535 best acc 0.535\n",
      "e 96 round_ema_decay 0.9000 | val loss 1.326 val acc 0.534 best val_acc 0.537 | ema loss 1.311 acc 0.535 best acc 0.535\n",
      "e 97 round_ema_decay 0.9000 | val loss 1.308 val acc 0.539 best val_acc 0.539 | ema loss 1.311 acc 0.535 best acc 0.535\n",
      "e 98 round_ema_decay 0.9000 | val loss 1.293 val acc 0.542 best val_acc 0.542 | ema loss 1.308 acc 0.537 best acc 0.537\n",
      "e 99 round_ema_decay 0.9000 | val loss 1.302 val acc 0.542 best val_acc 0.542 | ema loss 1.307 acc 0.537 best acc 0.537\n",
      "e 100 round_ema_decay 0.9000 | val loss 1.297 val acc 0.541 best val_acc 0.542 | ema loss 1.306 acc 0.536 best acc 0.537\n",
      "e 101 round_ema_decay 0.9000 | val loss 1.302 val acc 0.538 best val_acc 0.542 | ema loss 1.305 acc 0.537 best acc 0.537\n",
      "e 102 round_ema_decay 0.9000 | val loss 1.307 val acc 0.538 best val_acc 0.542 | ema loss 1.303 acc 0.539 best acc 0.539\n",
      "e 103 round_ema_decay 0.9000 | val loss 1.305 val acc 0.540 best val_acc 0.542 | ema loss 1.303 acc 0.540 best acc 0.540\n",
      "e 104 round_ema_decay 0.9000 | val loss 1.298 val acc 0.540 best val_acc 0.542 | ema loss 1.302 acc 0.540 best acc 0.540\n",
      "e 105 round_ema_decay 0.9000 | val loss 1.332 val acc 0.528 best val_acc 0.542 | ema loss 1.303 acc 0.538 best acc 0.540\n",
      "e 106 round_ema_decay 0.9000 | val loss 1.330 val acc 0.527 best val_acc 0.542 | ema loss 1.304 acc 0.536 best acc 0.540\n",
      "e 107 round_ema_decay 0.9000 | val loss 1.334 val acc 0.530 best val_acc 0.542 | ema loss 1.306 acc 0.536 best acc 0.540\n",
      "e 108 round_ema_decay 0.9000 | val loss 1.315 val acc 0.535 best val_acc 0.542 | ema loss 1.306 acc 0.537 best acc 0.540\n",
      "e 109 round_ema_decay 0.9000 | val loss 1.314 val acc 0.537 best val_acc 0.542 | ema loss 1.307 acc 0.538 best acc 0.540\n",
      "e 110 round_ema_decay 0.9000 | val loss 1.318 val acc 0.537 best val_acc 0.542 | ema loss 1.306 acc 0.539 best acc 0.540\n",
      "e 111 round_ema_decay 0.9000 | val loss 1.275 val acc 0.550 best val_acc 0.550 | ema loss 1.303 acc 0.539 best acc 0.540\n",
      "e 112 round_ema_decay 0.9000 | val loss 1.267 val acc 0.554 best val_acc 0.554 | ema loss 1.299 acc 0.539 best acc 0.540\n",
      "e 113 round_ema_decay 0.9000 | val loss 1.299 val acc 0.541 best val_acc 0.554 | ema loss 1.298 acc 0.539 best acc 0.540\n",
      "e 114 round_ema_decay 0.9000 | val loss 1.308 val acc 0.542 best val_acc 0.554 | ema loss 1.299 acc 0.539 best acc 0.540\n",
      "e 115 round_ema_decay 0.9000 | val loss 1.327 val acc 0.536 best val_acc 0.554 | ema loss 1.301 acc 0.539 best acc 0.540\n",
      "e 116 round_ema_decay 0.9000 | val loss 1.368 val acc 0.522 best val_acc 0.554 | ema loss 1.307 acc 0.538 best acc 0.540\n",
      "e 117 round_ema_decay 0.9000 | val loss 1.330 val acc 0.538 best val_acc 0.554 | ema loss 1.309 acc 0.539 best acc 0.540\n",
      "e 118 round_ema_decay 0.9000 | val loss 1.342 val acc 0.537 best val_acc 0.554 | ema loss 1.311 acc 0.539 best acc 0.540\n",
      "e 119 round_ema_decay 0.9000 | val loss 1.342 val acc 0.533 best val_acc 0.554 | ema loss 1.314 acc 0.538 best acc 0.540\n",
      "e 120 round_ema_decay 0.9000 | val loss 1.392 val acc 0.511 best val_acc 0.554 | ema loss 1.320 acc 0.536 best acc 0.540\n",
      "e 121 round_ema_decay 0.9000 | val loss 1.360 val acc 0.519 best val_acc 0.554 | ema loss 1.323 acc 0.534 best acc 0.540\n",
      "e 122 round_ema_decay 0.9000 | val loss 1.358 val acc 0.521 best val_acc 0.554 | ema loss 1.325 acc 0.534 best acc 0.540\n",
      "e 123 round_ema_decay 0.9000 | val loss 1.371 val acc 0.519 best val_acc 0.554 | ema loss 1.327 acc 0.533 best acc 0.540\n",
      "e 124 round_ema_decay 0.9000 | val loss 1.415 val acc 0.507 best val_acc 0.554 | ema loss 1.333 acc 0.532 best acc 0.540\n",
      "e 125 round_ema_decay 0.9000 | val loss 1.363 val acc 0.516 best val_acc 0.554 | ema loss 1.334 acc 0.532 best acc 0.540\n",
      "e 126 round_ema_decay 0.9000 | val loss 1.405 val acc 0.505 best val_acc 0.554 | ema loss 1.339 acc 0.531 best acc 0.540\n",
      "e 127 round_ema_decay 0.9000 | val loss 1.413 val acc 0.503 best val_acc 0.554 | ema loss 1.346 acc 0.527 best acc 0.540\n",
      "e 128 round_ema_decay 0.9000 | val loss 1.383 val acc 0.511 best val_acc 0.554 | ema loss 1.348 acc 0.525 best acc 0.540\n",
      "e 129 round_ema_decay 0.9000 | val loss 1.378 val acc 0.513 best val_acc 0.554 | ema loss 1.350 acc 0.525 best acc 0.540\n",
      "e 130 round_ema_decay 0.9000 | val loss 1.381 val acc 0.512 best val_acc 0.554 | ema loss 1.352 acc 0.525 best acc 0.540\n",
      "e 131 round_ema_decay 0.9000 | val loss 1.367 val acc 0.512 best val_acc 0.554 | ema loss 1.352 acc 0.526 best acc 0.540\n",
      "e 132 round_ema_decay 0.9000 | val loss 1.416 val acc 0.504 best val_acc 0.554 | ema loss 1.356 acc 0.524 best acc 0.540\n",
      "e 133 round_ema_decay 0.9000 | val loss 1.410 val acc 0.505 best val_acc 0.554 | ema loss 1.360 acc 0.523 best acc 0.540\n",
      "e 134 round_ema_decay 0.9000 | val loss 1.368 val acc 0.518 best val_acc 0.554 | ema loss 1.360 acc 0.523 best acc 0.540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 135 round_ema_decay 0.9000 | val loss 1.372 val acc 0.519 best val_acc 0.554 | ema loss 1.359 acc 0.524 best acc 0.540\n",
      "e 136 round_ema_decay 0.9000 | val loss 1.384 val acc 0.517 best val_acc 0.554 | ema loss 1.360 acc 0.523 best acc 0.540\n",
      "e 137 round_ema_decay 0.9000 | val loss 1.357 val acc 0.521 best val_acc 0.554 | ema loss 1.356 acc 0.524 best acc 0.540\n",
      "e 138 round_ema_decay 0.9000 | val loss 1.395 val acc 0.508 best val_acc 0.554 | ema loss 1.359 acc 0.523 best acc 0.540\n",
      "e 139 round_ema_decay 0.9000 | val loss 1.337 val acc 0.526 best val_acc 0.554 | ema loss 1.355 acc 0.524 best acc 0.540\n",
      "e 140 round_ema_decay 0.9000 | val loss 1.333 val acc 0.526 best val_acc 0.554 | ema loss 1.351 acc 0.525 best acc 0.540\n",
      "e 141 round_ema_decay 0.9000 | val loss 1.380 val acc 0.519 best val_acc 0.554 | ema loss 1.353 acc 0.526 best acc 0.540\n",
      "e 142 round_ema_decay 0.9000 | val loss 1.385 val acc 0.515 best val_acc 0.554 | ema loss 1.355 acc 0.525 best acc 0.540\n",
      "e 143 round_ema_decay 0.9000 | val loss 1.339 val acc 0.530 best val_acc 0.554 | ema loss 1.353 acc 0.525 best acc 0.540\n",
      "e 144 round_ema_decay 0.9000 | val loss 1.390 val acc 0.514 best val_acc 0.554 | ema loss 1.355 acc 0.523 best acc 0.540\n",
      "e 145 round_ema_decay 0.9000 | val loss 1.383 val acc 0.512 best val_acc 0.554 | ema loss 1.356 acc 0.523 best acc 0.540\n",
      "e 146 round_ema_decay 0.9000 | val loss 1.329 val acc 0.533 best val_acc 0.554 | ema loss 1.352 acc 0.525 best acc 0.540\n",
      "e 149 round_ema_decay 0.9000 | val loss 1.359 val acc 0.528 best val_acc 0.554 | ema loss 1.349 acc 0.527 best acc 0.540\n",
      "e 150 round_ema_decay 0.9000 | val loss 1.351 val acc 0.532 best val_acc 0.554 | ema loss 1.347 acc 0.529 best acc 0.540\n",
      "e 151 round_ema_decay 0.9000 | val loss 1.344 val acc 0.533 best val_acc 0.554 | ema loss 1.345 acc 0.529 best acc 0.540\n",
      "e 152 round_ema_decay 0.9000 | val loss 1.358 val acc 0.531 best val_acc 0.554 | ema loss 1.345 acc 0.529 best acc 0.540\n",
      "e 153 round_ema_decay 0.9000 | val loss 1.322 val acc 0.538 best val_acc 0.554 | ema loss 1.343 acc 0.530 best acc 0.540\n",
      "e 154 round_ema_decay 0.9000 | val loss 1.338 val acc 0.532 best val_acc 0.554 | ema loss 1.342 acc 0.531 best acc 0.540\n",
      "e 155 round_ema_decay 0.9000 | val loss 1.344 val acc 0.533 best val_acc 0.554 | ema loss 1.341 acc 0.530 best acc 0.540\n",
      "e 156 round_ema_decay 0.9000 | val loss 1.329 val acc 0.535 best val_acc 0.554 | ema loss 1.338 acc 0.530 best acc 0.540\n",
      "e 157 round_ema_decay 0.9000 | val loss 1.317 val acc 0.540 best val_acc 0.554 | ema loss 1.334 acc 0.532 best acc 0.540\n",
      "e 158 round_ema_decay 0.9000 | val loss 1.325 val acc 0.539 best val_acc 0.554 | ema loss 1.332 acc 0.532 best acc 0.540\n",
      "e 159 round_ema_decay 0.9000 | val loss 1.337 val acc 0.534 best val_acc 0.554 | ema loss 1.332 acc 0.532 best acc 0.540\n",
      "e 160 round_ema_decay 0.9000 | val loss 1.357 val acc 0.528 best val_acc 0.554 | ema loss 1.334 acc 0.533 best acc 0.540\n",
      "e 161 round_ema_decay 0.9000 | val loss 1.281 val acc 0.549 best val_acc 0.554 | ema loss 1.327 acc 0.534 best acc 0.540\n",
      "e 162 round_ema_decay 0.9000 | val loss 1.329 val acc 0.540 best val_acc 0.554 | ema loss 1.326 acc 0.534 best acc 0.540\n",
      "e 163 round_ema_decay 0.9000 | val loss 1.322 val acc 0.541 best val_acc 0.554 | ema loss 1.324 acc 0.535 best acc 0.540\n",
      "e 164 round_ema_decay 0.9000 | val loss 1.353 val acc 0.539 best val_acc 0.554 | ema loss 1.324 acc 0.536 best acc 0.540\n",
      "e 165 round_ema_decay 0.9000 | val loss 1.361 val acc 0.538 best val_acc 0.554 | ema loss 1.325 acc 0.538 best acc 0.540\n",
      "e 166 round_ema_decay 0.9000 | val loss 1.378 val acc 0.530 best val_acc 0.554 | ema loss 1.327 acc 0.537 best acc 0.540\n",
      "e 167 round_ema_decay 0.9000 | val loss 1.372 val acc 0.534 best val_acc 0.554 | ema loss 1.329 acc 0.538 best acc 0.540\n",
      "e 168 round_ema_decay 0.9000 | val loss 1.375 val acc 0.534 best val_acc 0.554 | ema loss 1.329 acc 0.539 best acc 0.540\n",
      "e 169 round_ema_decay 0.9000 | val loss 1.377 val acc 0.533 best val_acc 0.554 | ema loss 1.331 acc 0.539 best acc 0.540\n",
      "e 170 round_ema_decay 0.9000 | val loss 1.360 val acc 0.535 best val_acc 0.554 | ema loss 1.331 acc 0.540 best acc 0.540\n",
      "e 171 round_ema_decay 0.9000 | val loss 1.348 val acc 0.534 best val_acc 0.554 | ema loss 1.330 acc 0.543 best acc 0.543\n",
      "e 172 round_ema_decay 0.9000 | val loss 1.384 val acc 0.528 best val_acc 0.554 | ema loss 1.334 acc 0.541 best acc 0.543\n",
      "e 173 round_ema_decay 0.9000 | val loss 1.339 val acc 0.541 best val_acc 0.554 | ema loss 1.334 acc 0.542 best acc 0.543\n",
      "e 174 round_ema_decay 0.9000 | val loss 1.367 val acc 0.535 best val_acc 0.554 | ema loss 1.336 acc 0.541 best acc 0.543\n",
      "e 175 round_ema_decay 0.9000 | val loss 1.358 val acc 0.534 best val_acc 0.554 | ema loss 1.338 acc 0.541 best acc 0.543\n",
      "e 176 round_ema_decay 0.9000 | val loss 1.389 val acc 0.530 best val_acc 0.554 | ema loss 1.342 acc 0.541 best acc 0.543\n",
      "e 177 round_ema_decay 0.9000 | val loss 1.390 val acc 0.528 best val_acc 0.554 | ema loss 1.344 acc 0.540 best acc 0.543\n",
      "e 178 round_ema_decay 0.9000 | val loss 1.404 val acc 0.527 best val_acc 0.554 | ema loss 1.348 acc 0.540 best acc 0.543\n",
      "e 179 round_ema_decay 0.9000 | val loss 1.387 val acc 0.532 best val_acc 0.554 | ema loss 1.351 acc 0.539 best acc 0.543\n",
      "e 180 round_ema_decay 0.9000 | val loss 1.431 val acc 0.522 best val_acc 0.554 | ema loss 1.358 acc 0.539 best acc 0.543\n",
      "e 181 round_ema_decay 0.9000 | val loss 1.419 val acc 0.524 best val_acc 0.554 | ema loss 1.362 acc 0.537 best acc 0.543\n",
      "e 182 round_ema_decay 0.9000 | val loss 1.420 val acc 0.524 best val_acc 0.554 | ema loss 1.367 acc 0.536 best acc 0.543\n",
      "e 183 round_ema_decay 0.9000 | val loss 1.416 val acc 0.524 best val_acc 0.554 | ema loss 1.370 acc 0.535 best acc 0.543\n",
      "e 184 round_ema_decay 0.9000 | val loss 1.350 val acc 0.543 best val_acc 0.554 | ema loss 1.367 acc 0.537 best acc 0.543\n",
      "e 185 round_ema_decay 0.9000 | val loss 1.352 val acc 0.542 best val_acc 0.554 | ema loss 1.365 acc 0.539 best acc 0.543\n",
      "e 186 round_ema_decay 0.9000 | val loss 1.399 val acc 0.529 best val_acc 0.554 | ema loss 1.366 acc 0.537 best acc 0.543\n",
      "e 187 round_ema_decay 0.9000 | val loss 1.404 val acc 0.528 best val_acc 0.554 | ema loss 1.367 acc 0.536 best acc 0.543\n",
      "e 188 round_ema_decay 0.9000 | val loss 1.444 val acc 0.520 best val_acc 0.554 | ema loss 1.372 acc 0.535 best acc 0.543\n",
      "e 189 round_ema_decay 0.9000 | val loss 1.439 val acc 0.520 best val_acc 0.554 | ema loss 1.377 acc 0.535 best acc 0.543\n",
      "e 190 round_ema_decay 0.9000 | val loss 1.416 val acc 0.528 best val_acc 0.554 | ema loss 1.380 acc 0.534 best acc 0.543\n",
      "e 191 round_ema_decay 0.9000 | val loss 1.395 val acc 0.528 best val_acc 0.554 | ema loss 1.381 acc 0.535 best acc 0.543\n",
      "e 192 round_ema_decay 0.9000 | val loss 1.423 val acc 0.523 best val_acc 0.554 | ema loss 1.383 acc 0.535 best acc 0.543\n",
      "e 193 round_ema_decay 0.9000 | val loss 1.445 val acc 0.516 best val_acc 0.554 | ema loss 1.388 acc 0.533 best acc 0.543\n",
      "e 194 round_ema_decay 0.9000 | val loss 1.514 val acc 0.501 best val_acc 0.554 | ema loss 1.398 acc 0.530 best acc 0.543\n",
      "e 195 round_ema_decay 0.9000 | val loss 1.491 val acc 0.508 best val_acc 0.554 | ema loss 1.405 acc 0.529 best acc 0.543\n",
      "e 196 round_ema_decay 0.9000 | val loss 1.511 val acc 0.504 best val_acc 0.554 | ema loss 1.414 acc 0.527 best acc 0.543\n",
      "e 197 round_ema_decay 0.9000 | val loss 1.494 val acc 0.508 best val_acc 0.554 | ema loss 1.421 acc 0.526 best acc 0.543\n",
      "e 198 round_ema_decay 0.9000 | val loss 1.526 val acc 0.507 best val_acc 0.554 | ema loss 1.431 acc 0.524 best acc 0.543\n",
      "e 199 round_ema_decay 0.9000 | val loss 1.544 val acc 0.501 best val_acc 0.554 | ema loss 1.441 acc 0.523 best acc 0.543\n",
      "e 200 round_ema_decay 0.9000 | val loss 1.520 val acc 0.508 best val_acc 0.554 | ema loss 1.448 acc 0.520 best acc 0.543\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 32\n",
    "num_workers = 100\n",
    "local_lr = 0.05\n",
    "global_lr = 1\n",
    "nbyz = 20\n",
    "total_data_len = 0\n",
    "client_data_len = []\n",
    "tau_per_client = []\n",
    "\n",
    "for client_idx, worker_indices in enumerate(each_worker_idx[20-nbyz:]):\n",
    "    if client_idx < nbyz:\n",
    "        total_data_len += len(worker_indices)\n",
    "        client_data_len.append(len(worker_indices))\n",
    "        tau_per_client.append(len(train_loaders[client_idx][1]))\n",
    "    else:\n",
    "        total_data_len += len(worker_indices)\n",
    "        client_data_len.append(len(worker_indices))\n",
    "        tau_per_client.append(len(train_loaders[client_idx][1]))\n",
    "client_data_len = np.array(client_data_len)\n",
    "tau_per_client = np.array(tau_per_client)\n",
    "client_ws = client_data_len / total_data_len\n",
    "\n",
    "print('total data len: ', total_data_len)\n",
    "print('client_data_len shape: ', client_data_len.shape)\n",
    "print('tau_per_client shape: ', tau_per_client.shape)\n",
    "print('client_ws shape: ', client_ws.shape)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_global_acc = best_ema_acc = 0\n",
    "epoch_num = 0\n",
    "ema_decay = 0.9\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "ema_model_received = copy.deepcopy(model_received)\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(20-nbyz, num_workers)\n",
    "    round_benign = round_clients[nbyz:]\n",
    "    round_malicious = round_clients[:nbyz]\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    \n",
    "    for client_idx in round_malicious:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[client_idx][1], model, model_received, criterion, optimizer)\n",
    "        params = []\n",
    "        for _, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "        update =  (params - model_received)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    for client_idx in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[client_idx][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for _, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    if epoch_num == 0:\n",
    "        renormalized_client_weights = client_ws * np.sum(client_ws * tau_per_client) / tau_per_client\n",
    "        print('len tau_per_client %d client_data_len %d' % (len(tau_per_client), len(client_data_len)))\n",
    "        print('len client_ws %d renormalized_client_ws %d' % (len(client_ws), len(renormalized_client_weights)))\n",
    "\n",
    "    user_updates *= torch.from_numpy(renormalized_client_weights)[:, None].to(device)\n",
    "    user_updates[:nbyz] = full_trim(user_updates[:nbyz], nbyz)\n",
    "    agg_update = tr_mean(user_updates, nbyz)\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * (.999**epoch_num) * agg_update\n",
    "    round_ema_decay = min(ema_decay, (1+epoch_num)/(10+epoch_num))\n",
    "    ema_model_received = ema_model_received * round_ema_decay + model_received * (1 - round_ema_decay)\n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "\n",
    "    ema_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = ema_model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    ema_model.load_state_dict(state_dict)\n",
    "\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    ema_loss, ema_acc = test(cifar10_test_loader, ema_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    best_ema_acc = max(best_ema_acc, ema_acc)\n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "\n",
    "    if epoch_num%1==0 or epoch_num==nepochs-1:\n",
    "        print('e %d round_ema_decay %.4f | val loss %.3f val acc %.3f best val_acc %.3f | ema loss %.3f acc %.3f best acc %.3f'% (\n",
    "            epoch_num, round_ema_decay,\n",
    "            val_loss, val_acc, best_global_acc,\n",
    "            ema_loss, ema_acc, best_ema_acc))\n",
    "\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d13ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
