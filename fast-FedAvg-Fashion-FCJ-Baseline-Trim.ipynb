{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for Fashion MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821b1014",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu044/6605877/ipykernel_2681350/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a52c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "            \n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_train_data(trainset, num_workers=100, bias=0.5):\n",
    "    np.random.seed(42)\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        \n",
    "        if not len(each_worker_data[selected_worker]):\n",
    "            each_worker_data[selected_worker] = x[None, :]\n",
    "        else:\n",
    "            each_worker_data[selected_worker]= torch.concat((each_worker_data[selected_worker], x[None, :]))\n",
    "        \n",
    "        each_worker_label[selected_worker].append(y)\n",
    "    \n",
    "    each_worker_tr_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_tr_label = [[] for _ in range(num_workers)]\n",
    "    each_worker_val_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_val_label = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "\n",
    "    for i in range(num_workers):\n",
    "        w_len = len(each_worker_data[i])\n",
    "        len_tr = int(5 * w_len / 7)\n",
    "        len_val = int(1 * w_len / 7)\n",
    "        len_te = w_len - (len_tr + len_val)\n",
    "        # tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        # te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        w = np.arange(w_len)\n",
    "        np.random.shuffle(w)\n",
    "        tr_idx, val_idx, te_idx = w[:len_tr], w[len_tr : (len_tr + len_val)], w[-len_te:]\n",
    "        each_worker_tr_data[i] = each_worker_data[i][tr_idx]\n",
    "        each_worker_tr_label[i] = torch.Tensor(each_worker_label[i])[tr_idx]\n",
    "        each_worker_val_data[i] = each_worker_data[i][val_idx]\n",
    "        each_worker_val_label[i] = torch.Tensor(each_worker_label[i])[val_idx]\n",
    "        each_worker_te_data[i] = each_worker_data[i][te_idx]\n",
    "        each_worker_te_label[i] = torch.Tensor(each_worker_label[i])[te_idx]\n",
    "        \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    del each_worker_data, each_worker_label\n",
    "    return each_worker_tr_data, each_worker_tr_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f36f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_data_dirichlet(trainset, num_workers, alpha=1, force=False):\n",
    "    per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=alpha, force=force)\n",
    "    \n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(6*w_len/7)\n",
    "        len_te = w_len - len_tr\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        \n",
    "        for idx in w_indices[tr_idx]:\n",
    "            each_worker_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_data[worker_idx] = torch.stack(each_worker_data[worker_idx])\n",
    "        each_worker_label[worker_idx] = torch.Tensor(each_worker_label[worker_idx]).long()\n",
    "        \n",
    "        for idx in w_indices[te_idx]:\n",
    "            each_worker_te_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_te_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_te_data[worker_idx] = torch.stack(each_worker_te_data[worker_idx])\n",
    "        each_worker_te_label[worker_idx] = torch.Tensor(each_worker_te_label[worker_idx]).long()\n",
    "    \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    \n",
    "    return each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "319ce946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_attack_median(all_updates, n_attackers, dev_type='unit_vec'):\n",
    "    model_re = (all_updates)\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "    lamda = torch.Tensor([10.0]).cuda() #compute_lambda_our(all_updates, model_re, n_attackers)\n",
    "    threshold_diff = 1e-5\n",
    "    prev_loss = -1\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    iters = 0 \n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        agg_grads = torch.median(mal_updates, 0)[0]\n",
    "        \n",
    "        loss = torch.norm(agg_grads - model_re)\n",
    "        \n",
    "        if prev_loss < loss:\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "        prev_loss = loss\n",
    "        \n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "    mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "    return mal_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6b5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 30, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(30, 50, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(1250, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "#         print(x.shape)\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae94ee06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266060"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cnn()\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ace04bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = True\n",
    "if distribution=='fang':\n",
    "    each_worker_data, each_worker_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_train_data(all_data, num_workers=100, bias=param)\n",
    "elif distribution == 'dirichlet':\n",
    "    each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_data_dirichlet(all_data, num_workers, alpha=param, force=force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a30decba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10081"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(global_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb5ef3",
   "metadata": {},
   "source": [
    "# Good FedAvg baseline Fashion MNIST + Fang distribution + 80 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24164616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(train_data, labels, model, optimizer, batch_size=20):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    len_t = (len(train_data) // batch_size)\n",
    "    if len(train_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    r=np.arange(len(train_data))\n",
    "    np.random.shuffle(r)\n",
    "    \n",
    "    train_data = train_data[r]\n",
    "    labels = labels[r]\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "\n",
    "        inputs = train_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "        targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "\n",
    "def test(test_data, labels, model, criterion, use_cuda, debug_='MEDIUM', batch_size=64):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    len_t = (len(test_data) // batch_size)\n",
    "    if len(test_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ind in range(len_t):\n",
    "            # measure data loading time\n",
    "            inputs = test_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "            targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092fa012",
   "metadata": {},
   "source": [
    "# Mean without any attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2189af91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 val loss 1.484 val acc 50.412 best val_acc 50.412\n",
      "e 10 val loss 0.482 val acc 81.460 best val_acc 81.460\n",
      "e 20 val loss 0.408 val acc 84.783 best val_acc 84.783\n",
      "e 30 val loss 0.373 val acc 86.172 best val_acc 86.172\n",
      "e 40 val loss 0.349 val acc 87.095 best val_acc 87.095\n",
      "e 49 val loss 0.334 val acc 87.739 best val_acc 87.739\n",
      "e 50 val loss 0.335 val acc 87.729 best val_acc 87.739\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "fed_model = cnn().to(device)\n",
    "fed_model.apply(init_weights)\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    \n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(\n",
    "                each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    agg_update = torch.mean(user_updates, 0)\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    \n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, val_loss, val_acc, best_global_acc))\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22299d37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Processing FCJ 0.1 run 0\n",
      "FCJ 0.1 r 0 e 0 val loss 0.953 val acc 68.258 best val_acc 68.258\n",
      "FCJ 0.1 r 0 e 10 val loss 0.450 val acc 83.181 best val_acc 83.181\n",
      "FCJ 0.1 r 0 e 20 val loss 0.379 val acc 86.386 best val_acc 86.386\n",
      "FCJ 0.1 r 0 e 30 val loss 0.342 val acc 87.458 best val_acc 87.507\n",
      "FCJ 0.1 r 0 e 40 val loss 0.315 val acc 88.172 best val_acc 88.172\n",
      "FCJ 0.1 r 0 e 49 val loss 0.302 val acc 88.797 best val_acc 88.797\n",
      "FCJ 0.1 r 0 e 50 val loss 0.301 val acc 88.817 best val_acc 88.817\n",
      "===> Processing FCJ 0.1 run 1\n",
      "FCJ 0.1 r 1 e 0 val loss 0.922 val acc 68.853 best val_acc 68.853\n",
      "FCJ 0.1 r 1 e 10 val loss 0.451 val acc 82.864 best val_acc 82.864\n",
      "FCJ 0.1 r 1 e 20 val loss 0.379 val acc 86.148 best val_acc 86.148\n",
      "FCJ 0.1 r 1 e 30 val loss 0.343 val acc 87.200 best val_acc 87.249\n",
      "FCJ 0.1 r 1 e 49 val loss 0.300 val acc 88.946 best val_acc 88.946\n",
      "FCJ 0.1 r 1 e 50 val loss 0.300 val acc 88.897 best val_acc 88.946\n",
      "===> Processing FCJ 0.1 run 2\n",
      "FCJ 0.1 r 2 e 0 val loss 1.107 val acc 64.785 best val_acc 64.785\n",
      "FCJ 0.1 r 2 e 10 val loss 0.452 val acc 83.052 best val_acc 83.052\n",
      "FCJ 0.1 r 2 e 20 val loss 0.384 val acc 85.682 best val_acc 85.741\n",
      "FCJ 0.1 r 2 e 30 val loss 0.350 val acc 87.021 best val_acc 87.021\n",
      "FCJ 0.1 r 2 e 40 val loss 0.327 val acc 87.636 best val_acc 87.636\n",
      "FCJ 0.1 r 2 e 49 val loss 0.315 val acc 88.113 best val_acc 88.113\n",
      "FCJ 0.1 r 2 e 50 val loss 0.314 val acc 88.202 best val_acc 88.202\n",
      "===> Processing FCJ 0.3 run 0\n",
      "FCJ 0.3 r 0 e 0 val loss 1.076 val acc 64.880 best val_acc 64.880\n",
      "FCJ 0.3 r 0 e 10 val loss 0.469 val acc 82.644 best val_acc 82.644\n",
      "FCJ 0.3 r 0 e 20 val loss 0.397 val acc 85.432 best val_acc 85.432\n",
      "FCJ 0.3 r 0 e 30 val loss 0.362 val acc 86.603 best val_acc 86.603\n",
      "FCJ 0.3 r 0 e 40 val loss 0.340 val acc 87.546 best val_acc 87.566\n",
      "FCJ 0.3 r 0 e 49 val loss 0.325 val acc 88.062 best val_acc 88.062\n",
      "FCJ 0.3 r 0 e 50 val loss 0.324 val acc 88.300 best val_acc 88.300\n",
      "===> Processing FCJ 0.3 run 1\n",
      "FCJ 0.3 r 1 e 0 val loss 1.085 val acc 66.220 best val_acc 66.220\n",
      "FCJ 0.3 r 1 e 10 val loss 0.468 val acc 82.544 best val_acc 82.544\n",
      "FCJ 0.3 r 1 e 30 val loss 0.374 val acc 86.226 best val_acc 86.276\n",
      "FCJ 0.3 r 1 e 40 val loss 0.352 val acc 87.218 best val_acc 87.218\n",
      "FCJ 0.3 r 1 e 49 val loss 0.339 val acc 87.715 best val_acc 87.764\n",
      "FCJ 0.3 r 1 e 50 val loss 0.337 val acc 87.863 best val_acc 87.863\n",
      "===> Processing FCJ 0.3 run 2\n",
      "FCJ 0.3 r 2 e 0 val loss 1.039 val acc 69.376 best val_acc 69.376\n",
      "FCJ 0.3 r 2 e 10 val loss 0.479 val acc 81.919 best val_acc 81.919\n",
      "FCJ 0.3 r 2 e 20 val loss 0.410 val acc 84.896 best val_acc 84.896\n",
      "FCJ 0.3 r 2 e 30 val loss 0.377 val acc 85.909 best val_acc 85.968\n",
      "FCJ 0.3 r 2 e 40 val loss 0.356 val acc 87.060 best val_acc 87.060\n",
      "FCJ 0.3 r 2 e 49 val loss 0.340 val acc 87.556 best val_acc 87.556\n",
      "FCJ 0.3 r 2 e 50 val loss 0.340 val acc 87.576 best val_acc 87.576\n",
      "===> Processing FCJ 0.5 run 0\n",
      "FCJ 0.5 r 0 e 0 val loss 1.507 val acc 57.504 best val_acc 57.504\n",
      "FCJ 0.5 r 0 e 10 val loss 0.479 val acc 81.510 best val_acc 81.510\n",
      "FCJ 0.5 r 0 e 20 val loss 0.406 val acc 85.289 best val_acc 85.289\n",
      "FCJ 0.5 r 0 e 30 val loss 0.365 val acc 86.618 best val_acc 86.668\n",
      "FCJ 0.5 r 0 e 40 val loss 0.342 val acc 87.551 best val_acc 87.551\n",
      "FCJ 0.5 r 0 e 49 val loss 0.324 val acc 87.938 best val_acc 87.977\n",
      "FCJ 0.5 r 0 e 50 val loss 0.323 val acc 88.007 best val_acc 88.007\n",
      "===> Processing FCJ 0.5 run 1\n",
      "FCJ 0.5 r 1 e 0 val loss 1.468 val acc 62.077 best val_acc 62.077\n",
      "FCJ 0.5 r 1 e 10 val loss 0.478 val acc 81.579 best val_acc 81.579\n",
      "FCJ 0.5 r 1 e 20 val loss 0.409 val acc 85.001 best val_acc 85.001\n",
      "FCJ 0.5 r 1 e 30 val loss 0.370 val acc 86.480 best val_acc 86.480\n",
      "FCJ 0.5 r 1 e 40 val loss 0.348 val acc 87.422 best val_acc 87.422\n",
      "FCJ 0.5 r 1 e 49 val loss 0.329 val acc 88.146 best val_acc 88.146\n",
      "FCJ 0.5 r 1 e 50 val loss 0.328 val acc 88.077 best val_acc 88.146\n",
      "===> Processing FCJ 0.5 run 2\n",
      "FCJ 0.5 r 2 e 0 val loss 1.368 val acc 59.677 best val_acc 59.677\n",
      "FCJ 0.5 r 2 e 10 val loss 0.505 val acc 80.151 best val_acc 80.151\n",
      "FCJ 0.5 r 2 e 20 val loss 0.429 val acc 83.762 best val_acc 83.762\n",
      "FCJ 0.5 r 2 e 30 val loss 0.383 val acc 85.894 best val_acc 85.894\n",
      "FCJ 0.5 r 2 e 40 val loss 0.356 val acc 86.985 best val_acc 86.985\n",
      "FCJ 0.7 r 0 e 10 val loss 0.552 val acc 79.189 best val_acc 79.189\n",
      "FCJ 0.7 r 0 e 20 val loss 0.477 val acc 82.431 best val_acc 82.431\n",
      "FCJ 0.7 r 0 e 30 val loss 0.435 val acc 84.255 best val_acc 84.255\n",
      "FCJ 0.7 r 0 e 40 val loss 0.405 val acc 85.118 best val_acc 85.138\n",
      "FCJ 0.7 r 0 e 49 val loss 0.385 val acc 86.189 best val_acc 86.189\n",
      "FCJ 0.7 r 0 e 50 val loss 0.382 val acc 86.278 best val_acc 86.278\n",
      "===> Processing FCJ 0.7 run 1\n",
      "FCJ 0.7 r 1 e 0 val loss 1.767 val acc 50.248 best val_acc 50.248\n",
      "FCJ 0.7 r 1 e 10 val loss 0.537 val acc 79.417 best val_acc 79.417\n",
      "FCJ 0.7 r 1 e 20 val loss 0.468 val acc 82.857 best val_acc 82.857\n",
      "FCJ 0.7 r 1 e 30 val loss 0.423 val acc 84.543 best val_acc 84.543\n",
      "FCJ 0.7 r 1 e 40 val loss 0.394 val acc 85.594 best val_acc 85.594\n",
      "FCJ 0.7 r 1 e 49 val loss 0.375 val acc 86.268 best val_acc 86.268\n",
      "FCJ 0.7 r 1 e 50 val loss 0.374 val acc 86.337 best val_acc 86.337\n",
      "===> Processing FCJ 0.7 run 2\n",
      "FCJ 0.7 r 2 e 0 val loss 1.761 val acc 53.688 best val_acc 53.688\n",
      "FCJ 0.7 r 2 e 10 val loss 0.541 val acc 79.348 best val_acc 79.348\n",
      "FCJ 0.7 r 2 e 20 val loss 0.467 val acc 82.689 best val_acc 82.689\n",
      "FCJ 0.7 r 2 e 30 val loss 0.421 val acc 84.830 best val_acc 84.830\n",
      "FCJ 0.7 r 2 e 40 val loss 0.393 val acc 85.723 best val_acc 85.723\n",
      "FCJ 0.7 r 2 e 49 val loss 0.375 val acc 86.347 best val_acc 86.347\n",
      "FCJ 0.7 r 2 e 50 val loss 0.373 val acc 86.486 best val_acc 86.486\n",
      "===> Processing FCJ 0.9 run 0\n",
      "FCJ 0.9 r 0 e 0 val loss 2.144 val acc 35.723 best val_acc 35.723\n",
      "FCJ 0.9 r 0 e 20 val loss 0.598 val acc 76.555 best val_acc 76.555\n",
      "FCJ 0.9 r 0 e 30 val loss 0.545 val acc 78.965 best val_acc 78.965\n",
      "FCJ 0.9 r 0 e 40 val loss 0.509 val acc 80.651 best val_acc 80.651\n",
      "FCJ 0.9 r 0 e 49 val loss 0.484 val acc 82.148 best val_acc 82.148\n",
      "FCJ 0.9 r 0 e 50 val loss 0.482 val acc 82.406 best val_acc 82.406\n",
      "===> Processing FCJ 0.9 run 1\n",
      "FCJ 0.9 r 1 e 0 val loss 2.180 val acc 35.952 best val_acc 35.952\n",
      "FCJ 0.9 r 1 e 10 val loss 0.730 val acc 73.163 best val_acc 73.163\n",
      "FCJ 0.9 r 1 e 20 val loss 0.600 val acc 76.763 best val_acc 76.763\n",
      "FCJ 0.9 r 1 e 30 val loss 0.543 val acc 79.252 best val_acc 79.252\n",
      "FCJ 0.9 r 1 e 40 val loss 0.507 val acc 80.988 best val_acc 80.988\n",
      "FCJ 0.9 r 1 e 49 val loss 0.481 val acc 82.138 best val_acc 82.138\n",
      "FCJ 0.9 r 1 e 50 val loss 0.479 val acc 82.228 best val_acc 82.228\n",
      "===> Processing FCJ 0.9 run 2\n",
      "FCJ 0.9 r 2 e 0 val loss 2.174 val acc 33.472 best val_acc 33.472\n",
      "FCJ 0.9 r 2 e 10 val loss 0.737 val acc 72.488 best val_acc 72.488\n",
      "FCJ 0.9 r 2 e 20 val loss 0.610 val acc 76.178 best val_acc 76.227\n",
      "FCJ 0.9 r 2 e 30 val loss 0.550 val acc 78.508 best val_acc 78.508\n",
      "FCJ 0.9 r 2 e 40 val loss 0.512 val acc 80.611 best val_acc 80.611\n",
      "FCJ 0.9 r 2 e 49 val loss 0.486 val acc 81.960 best val_acc 81.960\n",
      "FCJ 0.9 r 2 e 50 val loss 0.484 val acc 82.099 best val_acc 82.099\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "distribution='fang'\n",
    "params = [.1, .3, .5, .7, .9]\n",
    "\n",
    "for fcj_p in params:\n",
    "    for run in range(3):\n",
    "        print('===> Processing FCJ %.1f run %d' % (fcj_p, run))\n",
    "        each_worker_data, each_worker_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_train_data(all_data, num_workers=num_workers, bias=fcj_p)\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = cnn().to(device)\n",
    "        fed_model.apply(init_weights)\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(nbyz, num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_updates=[]\n",
    "            benign_norm = 0\n",
    "\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(\n",
    "                        each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                        (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "            agg_update = torch.mean(user_updates, 0)\n",
    "            del user_updates\n",
    "            model_received = model_received + global_lr * agg_update\n",
    "            fed_model = cnn().to(device)\n",
    "            fed_model.apply(init_weights)\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "            if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "                print('FCJ %.1f r %d e %d val loss %.3f val acc %.3f best val_acc %.3f'% (fcj_p, run, epoch_num, val_loss, val_acc, best_global_acc))\n",
    "            epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f0d4ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 val loss 1.904 val acc 57.204 best val_acc 57.204\n",
      "e 10 val loss 0.557 val acc 78.941 best val_acc 78.941\n",
      "e 20 val loss 0.507 val acc 81.470 best val_acc 81.470\n",
      "e 30 val loss 0.498 val acc 82.137 best val_acc 82.376\n",
      "e 40 val loss 0.489 val acc 82.585 best val_acc 82.655\n",
      "e 49 val loss 0.480 val acc 83.142 best val_acc 83.142\n",
      "e 50 val loss 0.481 val acc 83.142 best val_acc 83.142\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "fed_model = cnn().to(device)\n",
    "fed_model.apply(init_weights)\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    \n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr, momentum=0.9, weight_decay=1e-4)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(\n",
    "                each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    user_updates = full_trim(user_updates, nbyz)\n",
    "    agg_update = tr_mean(user_updates, nbyz)\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    \n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, val_loss, val_acc, best_global_acc))\n",
    "    epoch_num+=1\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    client_loss, client_acc = test(each_worker_te_data[i], each_worker_te_label[i].long(),\n",
    "                                   fed_model, criterion, use_cuda)\n",
    "    final_accs_per_client.append(client_acc)\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    "    loss_per_round=np.array(loss_per_round)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_fashion_fast_trmean_trim20.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3234004",
   "metadata": {},
   "source": [
    "# Trim attack on FCJ + TrMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d7b1108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Processing FCJ 0.1 run 0\n",
      "Trim on FCJ 0.1 + TrMean | r 0 e 0 val loss 1.246 val acc 62.006 best val_acc 62.006\n",
      "Trim on FCJ 0.1 + TrMean | r 0 e 10 val loss 0.553 val acc 78.835 best val_acc 78.835\n",
      "Trim on FCJ 0.1 + TrMean | r 0 e 20 val loss 0.514 val acc 80.254 best val_acc 80.254\n",
      "Trim on FCJ 0.1 + TrMean | r 0 e 30 val loss 0.496 val acc 81.117 best val_acc 81.117\n",
      "Trim on FCJ 0.1 + TrMean | r 0 e 40 val loss 0.478 val acc 82.000 best val_acc 82.000\n",
      "Trim on FCJ 0.1 + TrMean | r 0 e 49 val loss 0.471 val acc 82.417 best val_acc 82.497\n",
      "Trim on FCJ 0.1 + TrMean | r 0 e 50 val loss 0.470 val acc 82.477 best val_acc 82.497\n",
      "===> Processing FCJ 0.1 run 1\n",
      "Trim on FCJ 0.1 + TrMean | r 1 e 0 val loss 1.105 val acc 68.010 best val_acc 68.010\n",
      "Trim on FCJ 0.1 + TrMean | r 1 e 10 val loss 0.543 val acc 79.113 best val_acc 79.202\n",
      "Trim on FCJ 0.1 + TrMean | r 1 e 20 val loss 0.512 val acc 80.413 best val_acc 80.462\n",
      "Trim on FCJ 0.1 + TrMean | r 1 e 30 val loss 0.485 val acc 81.524 best val_acc 81.524\n",
      "Trim on FCJ 0.1 + TrMean | r 1 e 40 val loss 0.461 val acc 82.596 best val_acc 82.596\n",
      "Trim on FCJ 0.1 + TrMean | r 1 e 49 val loss 0.451 val acc 82.923 best val_acc 82.923\n",
      "Trim on FCJ 0.1 + TrMean | r 1 e 50 val loss 0.453 val acc 82.844 best val_acc 82.923\n",
      "===> Processing FCJ 0.1 run 2\n",
      "Trim on FCJ 0.1 + TrMean | r 2 e 0 val loss 1.165 val acc 67.027 best val_acc 67.027\n",
      "Trim on FCJ 0.1 + TrMean | r 2 e 10 val loss 0.549 val acc 79.014 best val_acc 79.014\n",
      "Trim on FCJ 0.1 + TrMean | r 2 e 30 val loss 0.488 val acc 81.117 best val_acc 81.117\n",
      "Trim on FCJ 0.1 + TrMean | r 2 e 40 val loss 0.472 val acc 81.723 best val_acc 81.732\n",
      "Trim on FCJ 0.1 + TrMean | r 2 e 49 val loss 0.464 val acc 82.278 best val_acc 82.278\n",
      "Trim on FCJ 0.1 + TrMean | r 2 e 50 val loss 0.462 val acc 82.427 best val_acc 82.427\n",
      "===> Processing FCJ 0.3 run 0\n",
      "Trim on FCJ 0.3 + TrMean | r 0 e 0 val loss 1.185 val acc 69.356 best val_acc 69.356\n",
      "Trim on FCJ 0.3 + TrMean | r 0 e 10 val loss 0.579 val acc 77.841 best val_acc 77.841\n",
      "Trim on FCJ 0.3 + TrMean | r 0 e 20 val loss 0.541 val acc 79.587 best val_acc 79.587\n",
      "Trim on FCJ 0.3 + TrMean | r 0 e 30 val loss 0.527 val acc 80.153 best val_acc 80.153\n",
      "Trim on FCJ 0.3 + TrMean | r 0 e 40 val loss 0.514 val acc 80.699 best val_acc 80.699\n",
      "Trim on FCJ 0.3 + TrMean | r 0 e 49 val loss 0.498 val acc 81.522 best val_acc 81.522\n",
      "Trim on FCJ 0.3 + TrMean | r 0 e 50 val loss 0.500 val acc 81.393 best val_acc 81.522\n",
      "===> Processing FCJ 0.3 run 1\n",
      "Trim on FCJ 0.3 + TrMean | r 1 e 0 val loss 1.496 val acc 64.692 best val_acc 64.692\n",
      "Trim on FCJ 0.3 + TrMean | r 1 e 10 val loss 0.563 val acc 78.714 best val_acc 78.714\n",
      "Trim on FCJ 0.3 + TrMean | r 1 e 20 val loss 0.526 val acc 80.153 best val_acc 80.153\n",
      "Trim on FCJ 0.3 + TrMean | r 1 e 30 val loss 0.513 val acc 80.649 best val_acc 80.649\n",
      "Trim on FCJ 0.3 + TrMean | r 1 e 40 val loss 0.503 val acc 81.264 best val_acc 81.274\n",
      "Trim on FCJ 0.3 + TrMean | r 1 e 49 val loss 0.489 val acc 81.959 best val_acc 81.959\n",
      "Trim on FCJ 0.3 + TrMean | r 1 e 50 val loss 0.491 val acc 81.949 best val_acc 81.959\n",
      "===> Processing FCJ 0.3 run 2\n",
      "Trim on FCJ 0.3 + TrMean | r 2 e 0 val loss 1.569 val acc 64.225 best val_acc 64.225\n",
      "Trim on FCJ 0.3 + TrMean | r 2 e 10 val loss 0.567 val acc 78.942 best val_acc 78.942\n",
      "Trim on FCJ 0.3 + TrMean | r 2 e 20 val loss 0.529 val acc 79.944 best val_acc 79.944\n",
      "Trim on FCJ 0.3 + TrMean | r 2 e 30 val loss 0.514 val acc 80.907 best val_acc 80.907\n",
      "Trim on FCJ 0.3 + TrMean | r 2 e 40 val loss 0.506 val acc 81.334 best val_acc 81.334\n",
      "Trim on FCJ 0.3 + TrMean | r 2 e 49 val loss 0.495 val acc 81.969 best val_acc 81.969\n",
      "Trim on FCJ 0.3 + TrMean | r 2 e 50 val loss 0.497 val acc 81.870 best val_acc 81.969\n",
      "===> Processing FCJ 0.5 run 0\n",
      "Trim on FCJ 0.5 + TrMean | r 0 e 0 val loss 1.852 val acc 50.620 best val_acc 50.620\n",
      "Trim on FCJ 0.5 + TrMean | r 0 e 10 val loss 0.590 val acc 77.066 best val_acc 77.066\n",
      "Trim on FCJ 0.5 + TrMean | r 0 e 20 val loss 0.547 val acc 79.169 best val_acc 79.169\n",
      "Trim on FCJ 0.5 + TrMean | r 0 e 30 val loss 0.529 val acc 80.062 best val_acc 80.062\n",
      "Trim on FCJ 0.5 + TrMean | r 0 e 40 val loss 0.516 val acc 80.875 best val_acc 80.875\n",
      "Trim on FCJ 0.5 + TrMean | r 0 e 49 val loss 0.505 val acc 81.589 best val_acc 81.609\n",
      "Trim on FCJ 0.5 + TrMean | r 0 e 50 val loss 0.503 val acc 81.599 best val_acc 81.609\n",
      "===> Processing FCJ 0.5 run 1\n",
      "Trim on FCJ 0.5 + TrMean | r 1 e 0 val loss 1.895 val acc 51.295 best val_acc 51.295\n",
      "Trim on FCJ 0.5 + TrMean | r 1 e 10 val loss 0.584 val acc 77.592 best val_acc 77.592\n",
      "Trim on FCJ 0.5 + TrMean | r 1 e 20 val loss 0.547 val acc 79.268 best val_acc 79.268\n",
      "Trim on FCJ 0.5 + TrMean | r 1 e 30 val loss 0.528 val acc 80.309 best val_acc 80.309\n",
      "Trim on FCJ 0.5 + TrMean | r 1 e 40 val loss 0.518 val acc 81.133 best val_acc 81.133\n",
      "Trim on FCJ 0.5 + TrMean | r 1 e 49 val loss 0.509 val acc 81.569 best val_acc 81.659\n",
      "Trim on FCJ 0.5 + TrMean | r 1 e 50 val loss 0.504 val acc 81.708 best val_acc 81.708\n",
      "===> Processing FCJ 0.5 run 2\n",
      "Trim on FCJ 0.5 + TrMean | r 2 e 0 val loss 1.849 val acc 56.929 best val_acc 56.929\n",
      "Trim on FCJ 0.5 + TrMean | r 2 e 10 val loss 0.583 val acc 77.572 best val_acc 77.572\n",
      "Trim on FCJ 0.5 + TrMean | r 2 e 20 val loss 0.548 val acc 79.040 best val_acc 79.040\n",
      "Trim on FCJ 0.5 + TrMean | r 2 e 30 val loss 0.531 val acc 80.171 best val_acc 80.171\n",
      "Trim on FCJ 0.5 + TrMean | r 2 e 40 val loss 0.522 val acc 80.677 best val_acc 80.677\n",
      "Trim on FCJ 0.5 + TrMean | r 2 e 49 val loss 0.513 val acc 81.430 best val_acc 81.430\n",
      "Trim on FCJ 0.5 + TrMean | r 2 e 50 val loss 0.511 val acc 81.549 best val_acc 81.549\n",
      "===> Processing FCJ 0.7 run 0\n",
      "Trim on FCJ 0.7 + TrMean | r 0 e 0 val loss 2.140 val acc 38.846 best val_acc 38.846\n",
      "Trim on FCJ 0.7 + TrMean | r 0 e 10 val loss 0.655 val acc 75.977 best val_acc 75.977\n",
      "Trim on FCJ 0.7 + TrMean | r 0 e 20 val loss 0.609 val acc 77.900 best val_acc 77.900\n",
      "Trim on FCJ 0.7 + TrMean | r 0 e 30 val loss 0.591 val acc 78.763 best val_acc 78.882\n",
      "Trim on FCJ 0.7 + TrMean | r 0 e 40 val loss 0.569 val acc 80.111 best val_acc 80.111\n",
      "Trim on FCJ 0.7 + TrMean | r 0 e 49 val loss 0.576 val acc 80.418 best val_acc 80.418\n",
      "Trim on FCJ 0.7 + TrMean | r 0 e 50 val loss 0.579 val acc 80.270 best val_acc 80.418\n",
      "===> Processing FCJ 0.7 run 1\n",
      "Trim on FCJ 0.7 + TrMean | r 1 e 0 val loss 2.192 val acc 33.165 best val_acc 33.165\n",
      "Trim on FCJ 0.7 + TrMean | r 1 e 10 val loss 0.676 val acc 75.144 best val_acc 75.144\n",
      "Trim on FCJ 0.7 + TrMean | r 1 e 20 val loss 0.619 val acc 77.632 best val_acc 77.632\n",
      "Trim on FCJ 0.7 + TrMean | r 1 e 30 val loss 0.588 val acc 79.030 best val_acc 79.030\n",
      "Trim on FCJ 0.7 + TrMean | r 1 e 40 val loss 0.579 val acc 79.705 best val_acc 79.705\n",
      "Trim on FCJ 0.7 + TrMean | r 1 e 49 val loss 0.577 val acc 80.210 best val_acc 80.260\n",
      "Trim on FCJ 0.7 + TrMean | r 1 e 50 val loss 0.580 val acc 80.171 best val_acc 80.260\n",
      "===> Processing FCJ 0.7 run 2\n",
      "Trim on FCJ 0.7 + TrMean | r 2 e 0 val loss 2.161 val acc 37.993 best val_acc 37.993\n",
      "Trim on FCJ 0.7 + TrMean | r 2 e 10 val loss 0.669 val acc 75.411 best val_acc 75.411\n",
      "Trim on FCJ 0.7 + TrMean | r 2 e 20 val loss 0.621 val acc 77.801 best val_acc 77.801\n",
      "Trim on FCJ 0.7 + TrMean | r 2 e 30 val loss 0.595 val acc 79.268 best val_acc 79.268\n",
      "Trim on FCJ 0.7 + TrMean | r 2 e 40 val loss 0.570 val acc 80.666 best val_acc 80.666\n",
      "Trim on FCJ 0.7 + TrMean | r 2 e 49 val loss 0.562 val acc 81.231 best val_acc 81.231\n",
      "Trim on FCJ 0.7 + TrMean | r 2 e 50 val loss 0.561 val acc 81.360 best val_acc 81.360\n",
      "===> Processing FCJ 0.9 run 0\n",
      "Trim on FCJ 0.9 + TrMean | r 0 e 0 val loss 2.287 val acc 18.130 best val_acc 18.130\n",
      "Trim on FCJ 0.9 + TrMean | r 0 e 10 val loss 1.041 val acc 51.572 best val_acc 51.572\n",
      "Trim on FCJ 0.9 + TrMean | r 0 e 20 val loss 0.825 val acc 65.288 best val_acc 65.288\n",
      "Trim on FCJ 0.9 + TrMean | r 0 e 30 val loss 0.759 val acc 70.306 best val_acc 70.634\n",
      "Trim on FCJ 0.9 + TrMean | r 0 e 40 val loss 0.733 val acc 72.141 best val_acc 72.766\n",
      "Trim on FCJ 0.9 + TrMean | r 0 e 49 val loss 0.702 val acc 73.748 best val_acc 73.758\n",
      "Trim on FCJ 0.9 + TrMean | r 0 e 50 val loss 0.697 val acc 73.966 best val_acc 73.966\n",
      "===> Processing FCJ 0.9 run 1\n",
      "Trim on FCJ 0.9 + TrMean | r 1 e 0 val loss 2.333 val acc 11.614 best val_acc 11.614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trim on FCJ 0.9 + TrMean | r 1 e 10 val loss 1.123 val acc 54.339 best val_acc 54.339\n",
      "Trim on FCJ 0.9 + TrMean | r 1 e 20 val loss 0.842 val acc 68.045 best val_acc 68.045\n",
      "Trim on FCJ 0.9 + TrMean | r 1 e 30 val loss 0.776 val acc 71.130 best val_acc 71.130\n",
      "Trim on FCJ 0.9 + TrMean | r 1 e 40 val loss 0.747 val acc 73.093 best val_acc 73.093\n",
      "Trim on FCJ 0.9 + TrMean | r 1 e 49 val loss 0.753 val acc 72.478 best val_acc 73.480\n",
      "Trim on FCJ 0.9 + TrMean | r 1 e 50 val loss 0.744 val acc 72.707 best val_acc 73.480\n",
      "===> Processing FCJ 0.9 run 2\n",
      "Trim on FCJ 0.9 + TrMean | r 2 e 0 val loss 2.297 val acc 14.579 best val_acc 14.579\n",
      "Trim on FCJ 0.9 + TrMean | r 2 e 10 val loss 1.063 val acc 54.151 best val_acc 54.151\n",
      "Trim on FCJ 0.9 + TrMean | r 2 e 20 val loss 0.835 val acc 67.123 best val_acc 67.123\n",
      "Trim on FCJ 0.9 + TrMean | r 2 e 30 val loss 0.751 val acc 71.050 best val_acc 71.050\n",
      "Trim on FCJ 0.9 + TrMean | r 2 e 40 val loss 0.731 val acc 72.102 best val_acc 72.359\n",
      "Trim on FCJ 0.9 + TrMean | r 2 e 49 val loss 0.711 val acc 73.331 best val_acc 73.331\n",
      "Trim on FCJ 0.9 + TrMean | r 2 e 50 val loss 0.705 val acc 73.520 best val_acc 73.520\n",
      "======= Results for FCJ -0.0  =====\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "tensor(-0.0306, device='cuda:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fcj_p \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m======= Results for FCJ \u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[38;5;124m  =====\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m fcj_p)\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfcj_p\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: tensor(-0.0306, device='cuda:0')"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "distribution='fang'\n",
    "params = [.1, .3, .5, .7, .9]\n",
    "\n",
    "results = {}\n",
    "for fcj_p in params:\n",
    "    results[fcj_p] = []\n",
    "    for run in range(3):\n",
    "        print('===> Processing FCJ %.1f run %d' % (fcj_p, run))\n",
    "        \n",
    "        each_worker_data, each_worker_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_train_data(all_data, num_workers=num_workers, bias=fcj_p)\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = cnn().to(device)\n",
    "        fed_model.apply(init_weights)\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_updates=[]\n",
    "            benign_norm = 0\n",
    "\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(\n",
    "                        each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                        (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "            user_updates = full_trim(user_updates, nbyz)\n",
    "            agg_update = tr_mean(user_updates, nbyz)\n",
    "            \n",
    "            del user_updates\n",
    "            model_received = model_received + global_lr * agg_update\n",
    "            fed_model = cnn().to(device)\n",
    "            fed_model.apply(init_weights)\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "            if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "                print('Trim on FCJ %.1f + TrMean | r %d e %d val loss %.3f val acc %.3f best val_acc %.3f'% (fcj_p, run, epoch_num, val_loss, val_acc, best_global_acc))\n",
    "            epoch_num+=1\n",
    "        results[fcj_p].append(best_global_acc)\n",
    "\n",
    "for fcj_p in params:\n",
    "    print('======= Results for FCJ %.1f  =====' % fcj_p)\n",
    "    print(results[fcj_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e09fca4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.1: [82.49652707356738, 82.9231990398597, 82.42706885529927],\n",
       " 0.3: [81.52227845361833, 81.95891634187872, 81.96883993024827],\n",
       " 0.5: [81.60896736813282, 81.70816387944296, 81.54944946316307],\n",
       " 0.7: [80.41840174196733, 80.25976601985862, 81.36030139578918],\n",
       " 0.9: [73.96608151957285, 73.48011505269206, 73.51978578560885]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf6d23",
   "metadata": {},
   "source": [
    "# NDSS attack on FCJ + TrMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f118ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndss21_attack_trmean(all_updates, n_attackers, dev_type='sign', threshold=5.0, threshold_diff=1e-5, fast_ndss=False):\n",
    "    \n",
    "    model_re = torch.mean(all_updates, 0)\n",
    "    \n",
    "    if dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "    if fast_ndss:\n",
    "        return (model_re - threshold * deviation)\n",
    "\n",
    "    lamda = torch.Tensor([threshold]).to(device)  # compute_lambda_our(all_updates, model_re, n_attackers)\n",
    "\n",
    "    threshold_diff = threshold_diff\n",
    "    prev_loss = -1\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        agg_grads = tr_mean(mal_updates, n_attackers)\n",
    "\n",
    "        loss = torch.norm(agg_grads - model_re)\n",
    "\n",
    "        if prev_loss < loss:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "        prev_loss = loss\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb574bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "fast_ndss = True\n",
    "params = [.1, .3, .5, .7, .9]\n",
    "\n",
    "results = {}\n",
    "for fcj_p in params:\n",
    "    results[fcj_p] = []\n",
    "    for run in range(3):\n",
    "        print('===> Processing FCJ %.1f run %d' % (fcj_p, run))\n",
    "        \n",
    "        each_worker_data, each_worker_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_train_data(all_data, num_workers=num_workers, bias=fcj_p)\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = cnn().to(device)\n",
    "        fed_model.apply(init_weights)\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_updates=[]\n",
    "            benign_norm = 0\n",
    "\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(\n",
    "                        each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                        (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "            mal_update = ndss21_attack_trmean(user_updates[:nbyz], 5, dev_type='sign', threshold=20.0, threshold_diff=1e-5, fast_ndss=fast_ndss)\n",
    "            user_updates[:nbyz] = torch.stack([mal_update]*nbyz)\n",
    "            agg_update = tr_mean(user_updates, nbyz)\n",
    "\n",
    "            del user_updates\n",
    "            model_received = model_received + global_lr * agg_update\n",
    "            fed_model = cnn().to(device)\n",
    "            fed_model.apply(init_weights)\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "            if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "                print('Trim on FCJ %.1f + TrMean | r %d e %d val loss %.3f val acc %.3f best val_acc %.3f'% (fcj_p, run, epoch_num, val_loss, val_acc, best_global_acc))\n",
    "            epoch_num+=1\n",
    "        results[fcj_p].append(best_global_acc)\n",
    "\n",
    "for fcj_p in params:\n",
    "    print('======= Results for NDSS attack on FCJ %.1f  =====' % fcj_p)\n",
    "    print(results[fcj_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03637170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f87ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ae2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
