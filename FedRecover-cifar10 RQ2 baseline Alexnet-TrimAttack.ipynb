{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for CIFAR10 with Fang distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e300ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu065/7154297/ipykernel_1156436/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/vshejwalkar_umass_edu/anaconda/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /work/vshejwalkar_umass_edu/anaconda/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "import copy\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_models import *\n",
    "from cifar10_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "\n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fang_train_data(trainset, num_workers=100, bias=0.5, force=False):\n",
    "    dist_file = 'fang_nworkers%d_bias%.1f.pkl' % (num_workers, bias)\n",
    "    if not force and os.path.exists(dist_file):\n",
    "        print('Loading fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "        return pickle.load(open(dist_file, 'rb'))\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    per_participant_list = defaultdict(list)\n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        per_participant_list[selected_worker].extend([i])\n",
    "    \n",
    "    print('Saving fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "    pickle.dump(per_participant_list, open(dist_file, 'wb'))\n",
    "    return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "799ffb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_federated_data(trainset, num_workers, distribution='fang', param=1, force=False):\n",
    "    if distribution == 'fang':\n",
    "        per_participant_list = get_fang_train_data(trainset, num_workers, bias=param, force=force)\n",
    "    elif distribution == 'dirichlet':\n",
    "        per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=param, force=force)\n",
    "\n",
    "    each_worker_idx = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_te_idx = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(5*w_len/6)\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        \n",
    "        each_worker_idx[worker_idx] = w_indices[tr_idx]\n",
    "        each_worker_te_idx[worker_idx] = w_indices[te_idx]\n",
    "    \n",
    "    global_test_idx = np.concatenate(each_worker_te_idx)\n",
    "    \n",
    "    return each_worker_idx, each_worker_te_idx, global_test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2590db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(dataset, indices, batch_size=32, shuffle=False):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acbb06d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/home/vshejwalkar_umass_edu/data/'\n",
    "# load the train dataset\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_train)\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_train)\n",
    "\n",
    "te_cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_test)\n",
    "te_cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faa727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(trainloader, model, model_received, criterion, optimizer, pgd=False, eps=2):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    for batch_ind, (inputs, targets) in enumerate(trainloader):\n",
    "\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size()[0])\n",
    "        top1.update(prec1.item()/100.0, inputs.size()[0])\n",
    "        top5.update(prec5.item()/100.0, inputs.size()[0])\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if pgd:\n",
    "            curr_model = list(model.parameters())\n",
    "            curr_model_vec = parameters_to_vector(curr_model)\n",
    "\n",
    "            if torch.norm(curr_model_vec - model_received) > eps:\n",
    "                curr_model_vec = eps*(curr_model_vec - model_received)/torch.norm(curr_model_vec - model_received) + model_received\n",
    "                vector_to_parameters(curr_model_vec, curr_model)\n",
    "        \n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def test(testloader, model, criterion):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    for batch_ind, (inputs, targets) in enumerate(testloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        top1.update(prec1/100.0, inputs.size()[0])\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62958a8",
   "metadata": {},
   "source": [
    "# Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "382ff809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def alexnet(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca7c704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m param \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.5\u001b[39m\n\u001b[1;32m      7\u001b[0m force \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m each_worker_idx, each_worker_te_idx, global_test_idx \u001b[38;5;241m=\u001b[39m \u001b[43mget_federated_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m train_loaders \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos, indices \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(each_worker_idx):\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mget_federated_data\u001b[0;34m(trainset, num_workers, distribution, param, force)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_federated_data\u001b[39m(trainset, num_workers, distribution\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfang\u001b[39m\u001b[38;5;124m'\u001b[39m, param\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distribution \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfang\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m         per_participant_list \u001b[38;5;241m=\u001b[39m \u001b[43mget_fang_train_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m distribution \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirichlet\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      5\u001b[0m         per_participant_list \u001b[38;5;241m=\u001b[39m sample_dirichlet_train_data(trainset, num_workers, alpha\u001b[38;5;241m=\u001b[39mparam, force\u001b[38;5;241m=\u001b[39mforce)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mget_fang_train_data\u001b[0;34m(trainset, num_workers, bias, force)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_fang_train_data\u001b[39m(trainset, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     dist_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfang_nworkers\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_bias\u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (num_workers, bias)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dist_file):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading fang distribution for num_workers \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m and bias \u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[38;5;124m from memory\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (num_workers, bias))\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(dist_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = False\n",
    "\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=num_workers, distribution=distribution, param=param, force=force)\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    train_loaders.append((pos, get_train(all_data, indices, batch_size)))\n",
    "test_loaders = []\n",
    "for pos, indices in enumerate(each_worker_te_idx):\n",
    "    test_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "len(train_loaders), len(test_loaders), len(global_test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc3c2913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 benign_norm 1.291 val loss 2.300 val acc 0.100 best val_acc 0.100\n",
      "e 2 benign_norm 1.200 val loss 2.264 val acc 0.140 best val_acc 0.140\n",
      "e 4 benign_norm 1.191 val loss 2.179 val acc 0.191 best val_acc 0.191\n",
      "e 6 benign_norm 1.136 val loss 2.041 val acc 0.225 best val_acc 0.225\n",
      "e 8 benign_norm 1.147 val loss 2.005 val acc 0.221 best val_acc 0.225\n",
      "e 10 benign_norm 1.178 val loss 1.974 val acc 0.241 best val_acc 0.241\n",
      "e 12 benign_norm 1.213 val loss 1.945 val acc 0.253 best val_acc 0.255\n",
      "e 14 benign_norm 1.279 val loss 1.905 val acc 0.276 best val_acc 0.276\n",
      "e 16 benign_norm 1.339 val loss 1.869 val acc 0.287 best val_acc 0.287\n",
      "e 18 benign_norm 1.392 val loss 1.840 val acc 0.302 best val_acc 0.302\n",
      "e 20 benign_norm 1.469 val loss 1.840 val acc 0.303 best val_acc 0.307\n",
      "e 22 benign_norm 1.538 val loss 1.817 val acc 0.323 best val_acc 0.323\n",
      "e 24 benign_norm 1.581 val loss 1.829 val acc 0.317 best val_acc 0.323\n",
      "e 26 benign_norm 1.613 val loss 1.783 val acc 0.339 best val_acc 0.339\n",
      "e 28 benign_norm 1.683 val loss 1.829 val acc 0.331 best val_acc 0.339\n",
      "e 30 benign_norm 1.698 val loss 1.795 val acc 0.344 best val_acc 0.344\n",
      "e 32 benign_norm 1.791 val loss 1.769 val acc 0.359 best val_acc 0.359\n",
      "e 34 benign_norm 1.807 val loss 1.758 val acc 0.374 best val_acc 0.374\n",
      "e 36 benign_norm 1.862 val loss 1.743 val acc 0.382 best val_acc 0.382\n",
      "e 38 benign_norm 1.946 val loss 1.746 val acc 0.382 best val_acc 0.383\n",
      "e 40 benign_norm 1.972 val loss 1.741 val acc 0.394 best val_acc 0.394\n",
      "e 42 benign_norm 2.049 val loss 1.756 val acc 0.398 best val_acc 0.398\n",
      "e 44 benign_norm 2.087 val loss 1.738 val acc 0.405 best val_acc 0.406\n",
      "e 46 benign_norm 2.146 val loss 1.730 val acc 0.408 best val_acc 0.408\n",
      "e 48 benign_norm 2.209 val loss 1.762 val acc 0.407 best val_acc 0.409\n",
      "e 50 benign_norm 2.271 val loss 1.779 val acc 0.410 best val_acc 0.411\n",
      "e 52 benign_norm 2.315 val loss 1.798 val acc 0.411 best val_acc 0.415\n",
      "e 54 benign_norm 2.349 val loss 1.828 val acc 0.417 best val_acc 0.417\n",
      "e 56 benign_norm 2.419 val loss 1.928 val acc 0.400 best val_acc 0.419\n",
      "e 58 benign_norm 2.432 val loss 1.927 val acc 0.408 best val_acc 0.419\n",
      "e 60 benign_norm 2.490 val loss 1.837 val acc 0.423 best val_acc 0.423\n",
      "e 62 benign_norm 2.564 val loss 1.972 val acc 0.409 best val_acc 0.423\n",
      "e 64 benign_norm 2.604 val loss 1.923 val acc 0.422 best val_acc 0.423\n",
      "e 66 benign_norm 2.642 val loss 1.955 val acc 0.412 best val_acc 0.423\n",
      "e 68 benign_norm 2.714 val loss 1.929 val acc 0.416 best val_acc 0.423\n",
      "e 70 benign_norm 2.726 val loss 1.911 val acc 0.429 best val_acc 0.429\n",
      "e 72 benign_norm 2.764 val loss 1.860 val acc 0.447 best val_acc 0.447\n",
      "e 74 benign_norm 2.847 val loss 1.930 val acc 0.436 best val_acc 0.447\n",
      "e 78 benign_norm 2.892 val loss 2.086 val acc 0.423 best val_acc 0.447\n",
      "e 80 benign_norm 2.910 val loss 2.009 val acc 0.434 best val_acc 0.447\n",
      "e 82 benign_norm 2.944 val loss 2.024 val acc 0.439 best val_acc 0.447\n",
      "e 84 benign_norm 3.047 val loss 2.011 val acc 0.438 best val_acc 0.447\n",
      "e 86 benign_norm 3.116 val loss 1.903 val acc 0.453 best val_acc 0.453\n",
      "e 88 benign_norm 3.146 val loss 2.049 val acc 0.451 best val_acc 0.453\n",
      "e 90 benign_norm 3.203 val loss 2.019 val acc 0.444 best val_acc 0.453\n",
      "e 92 benign_norm 3.266 val loss 2.198 val acc 0.440 best val_acc 0.453\n",
      "e 94 benign_norm 3.235 val loss 2.086 val acc 0.440 best val_acc 0.453\n",
      "e 96 benign_norm 3.290 val loss 2.048 val acc 0.449 best val_acc 0.453\n",
      "e 98 benign_norm 3.385 val loss 2.051 val acc 0.462 best val_acc 0.462\n",
      "e 100 benign_norm 3.428 val loss 2.079 val acc 0.455 best val_acc 0.462\n",
      "e 102 benign_norm 3.458 val loss 2.028 val acc 0.455 best val_acc 0.462\n",
      "e 104 benign_norm 3.578 val loss 2.144 val acc 0.449 best val_acc 0.462\n",
      "e 106 benign_norm 3.534 val loss 2.038 val acc 0.454 best val_acc 0.462\n",
      "e 108 benign_norm 3.640 val loss 2.073 val acc 0.475 best val_acc 0.475\n",
      "e 110 benign_norm 3.754 val loss 2.052 val acc 0.473 best val_acc 0.475\n",
      "e 112 benign_norm 3.829 val loss 2.146 val acc 0.464 best val_acc 0.476\n",
      "e 114 benign_norm 3.792 val loss 2.058 val acc 0.471 best val_acc 0.476\n",
      "e 116 benign_norm 3.869 val loss 1.980 val acc 0.484 best val_acc 0.484\n",
      "e 118 benign_norm 3.925 val loss 2.055 val acc 0.475 best val_acc 0.484\n",
      "e 120 benign_norm 3.967 val loss 2.062 val acc 0.469 best val_acc 0.484\n",
      "e 122 benign_norm 4.021 val loss 2.197 val acc 0.469 best val_acc 0.484\n",
      "e 124 benign_norm 4.064 val loss 2.114 val acc 0.476 best val_acc 0.484\n",
      "e 126 benign_norm 4.235 val loss 2.062 val acc 0.471 best val_acc 0.484\n",
      "e 128 benign_norm 4.166 val loss 2.041 val acc 0.488 best val_acc 0.488\n",
      "e 130 benign_norm 4.267 val loss 2.082 val acc 0.486 best val_acc 0.488\n",
      "e 132 benign_norm 4.321 val loss 2.091 val acc 0.480 best val_acc 0.488\n",
      "e 134 benign_norm 4.450 val loss 2.221 val acc 0.469 best val_acc 0.488\n",
      "e 136 benign_norm 4.371 val loss 2.127 val acc 0.483 best val_acc 0.488\n",
      "e 138 benign_norm 4.482 val loss 2.098 val acc 0.484 best val_acc 0.489\n",
      "e 140 benign_norm 4.568 val loss 2.170 val acc 0.476 best val_acc 0.489\n",
      "e 142 benign_norm 4.601 val loss 2.123 val acc 0.495 best val_acc 0.495\n",
      "e 144 benign_norm 4.684 val loss 2.248 val acc 0.487 best val_acc 0.499\n",
      "e 146 benign_norm 4.663 val loss 2.266 val acc 0.475 best val_acc 0.499\n",
      "e 148 benign_norm 4.755 val loss 2.212 val acc 0.489 best val_acc 0.499\n",
      "e 150 benign_norm 4.793 val loss 2.239 val acc 0.479 best val_acc 0.499\n",
      "e 152 benign_norm 4.795 val loss 2.168 val acc 0.495 best val_acc 0.499\n",
      "e 154 benign_norm 5.011 val loss 2.147 val acc 0.503 best val_acc 0.503\n",
      "e 156 benign_norm 4.994 val loss 2.183 val acc 0.497 best val_acc 0.503\n",
      "e 158 benign_norm 5.060 val loss 2.258 val acc 0.491 best val_acc 0.503\n",
      "e 160 benign_norm 5.186 val loss 2.317 val acc 0.485 best val_acc 0.503\n",
      "e 162 benign_norm 5.237 val loss 2.334 val acc 0.481 best val_acc 0.503\n",
      "e 164 benign_norm 5.180 val loss 2.145 val acc 0.496 best val_acc 0.503\n",
      "e 166 benign_norm 5.267 val loss 2.193 val acc 0.497 best val_acc 0.503\n",
      "e 168 benign_norm 5.338 val loss 2.087 val acc 0.509 best val_acc 0.509\n",
      "e 170 benign_norm 5.422 val loss 2.225 val acc 0.493 best val_acc 0.509\n",
      "e 172 benign_norm 5.543 val loss 2.100 val acc 0.504 best val_acc 0.509\n",
      "e 174 benign_norm 5.512 val loss 2.182 val acc 0.506 best val_acc 0.509\n",
      "e 176 benign_norm 5.593 val loss 2.240 val acc 0.501 best val_acc 0.509\n",
      "e 178 benign_norm 5.580 val loss 2.223 val acc 0.489 best val_acc 0.509\n",
      "e 180 benign_norm 5.633 val loss 2.158 val acc 0.505 best val_acc 0.509\n",
      "e 182 benign_norm 5.693 val loss 2.299 val acc 0.499 best val_acc 0.517\n",
      "e 184 benign_norm 5.642 val loss 2.265 val acc 0.503 best val_acc 0.517\n",
      "e 186 benign_norm 5.700 val loss 2.159 val acc 0.500 best val_acc 0.517\n",
      "e 188 benign_norm 5.785 val loss 2.098 val acc 0.507 best val_acc 0.517\n",
      "e 190 benign_norm 5.911 val loss 2.155 val acc 0.508 best val_acc 0.517\n",
      "e 192 benign_norm 5.896 val loss 2.061 val acc 0.511 best val_acc 0.517\n",
      "e 194 benign_norm 6.084 val loss 2.141 val acc 0.510 best val_acc 0.517\n",
      "e 196 benign_norm 5.997 val loss 2.143 val acc 0.502 best val_acc 0.517\n",
      "e 198 benign_norm 6.099 val loss 2.030 val acc 0.514 best val_acc 0.517\n",
      "e 199 benign_norm 6.020 val loss 2.229 val acc 0.509 best val_acc 0.517\n",
      "e 200 benign_norm 6.186 val loss 2.070 val acc 0.518 best val_acc 0.518\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nbyz = 20\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "resume=False\n",
    "round_nclients = num_workers\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "fed_model = alexnet().cuda()\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    user_updates[:nbyz] = full_trim(user_updates[:nbyz], nbyz)\n",
    "    agg_update = tr_mean(user_updates, nbyz)\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = alexnet().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "    if epoch_num%2==0 or epoch_num==nepochs-1:\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739565ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
