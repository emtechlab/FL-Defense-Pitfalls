{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for Fashion MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821b1014",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu066/7176230/ipykernel_19785/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a52c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24164616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(train_data, labels, model, optimizer, batch_size=20):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    len_t = (len(train_data) // batch_size)\n",
    "    if len(train_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    r=np.arange(len(train_data))\n",
    "    np.random.shuffle(r)\n",
    "    \n",
    "    train_data = train_data[r]\n",
    "    labels = labels[r]\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "\n",
    "        inputs = train_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "        targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "\n",
    "def test(test_data, labels, model, criterion, use_cuda, debug_='MEDIUM', batch_size=64):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    len_t = (len(test_data) // batch_size)\n",
    "    if len(test_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ind in range(len_t):\n",
    "            # measure data loading time\n",
    "            inputs = test_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "            targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "            \n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b3ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fang_train_data(trainset, num_workers=100, bias=0.5, force=False):\n",
    "    dist_file = 'fang_nworkers%d_bias%.1f.pkl' % (num_workers, bias)\n",
    "    if not force and os.path.exists(dist_file):\n",
    "        print('Loading fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "        return pickle.load(open(dist_file, 'rb'))\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    per_participant_list = defaultdict(list)\n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        per_participant_list[selected_worker].extend([i])\n",
    "    \n",
    "    print('Saving fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "    pickle.dump(per_participant_list, open(dist_file, 'wb'))\n",
    "    return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "229f0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_federated_data(trainset, num_workers, distribution='fang', param=1, force=False):\n",
    "    if distribution == 'fang':\n",
    "        per_participant_list = get_fang_train_data(trainset, num_workers, bias=param, force=force)\n",
    "    elif distribution == 'dirichlet':\n",
    "        per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=param, force=force)\n",
    "\n",
    "    each_worker_idx = [[] for _ in range(num_workers)]\n",
    "    each_worker_val_idx = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_idx = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_val_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_val_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(5 * w_len/7)\n",
    "        len_val = int(w_len/7)\n",
    "        np.random.shuffle(w_indices)\n",
    "        \n",
    "        tr_idx = w_indices[:len_tr]\n",
    "        val_idx = w_indices[len_tr: len_tr+len_val]\n",
    "        te_idx = w_indices[len_tr+len_val:]\n",
    "\n",
    "        for idx in tr_idx:\n",
    "            each_worker_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_data[worker_idx] = torch.stack(each_worker_data[worker_idx])\n",
    "        each_worker_label[worker_idx] = torch.Tensor(each_worker_label[worker_idx]).long()\n",
    "        \n",
    "        for idx in val_idx:\n",
    "            each_worker_val_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_val_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_val_data[worker_idx] = torch.stack(each_worker_val_data[worker_idx])\n",
    "        each_worker_val_label[worker_idx] = torch.Tensor(each_worker_val_label[worker_idx]).long()\n",
    "        \n",
    "        for idx in te_idx:\n",
    "            each_worker_te_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_te_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_te_data[worker_idx] = torch.stack(each_worker_te_data[worker_idx])\n",
    "        each_worker_te_label[worker_idx] = torch.Tensor(each_worker_te_label[worker_idx]).long()\n",
    "    \n",
    "    global_val_data = torch.concatenate(each_worker_val_data)\n",
    "    global_val_label = torch.concatenate(each_worker_val_label)\n",
    "    \n",
    "    global_te_data = torch.concatenate(each_worker_te_data)\n",
    "    global_te_label = torch.concatenate(each_worker_te_label)\n",
    "    \n",
    "    return each_worker_data, each_worker_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_val_data, global_val_label, global_te_data, global_te_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca6b5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 30, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(30, 50, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(1250, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae94ee06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266060"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cnn()\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcf8234f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fang distribution for num_workers 100 and bias 0.5 from memory\n"
     ]
    }
   ],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = False\n",
    "\n",
    "each_worker_data, each_worker_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_val_data, global_val_label, global_te_data, global_te_label = get_federated_data(\n",
    "    all_data, num_workers=num_workers, distribution=distribution, param=param, force=force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a30decba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8664, 8526)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(global_te_label), len(global_val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5a74faa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.0100 ne 5 bs 10 | val acc 71.11 std 5.61 | test acc 71.20 std 5.39\n",
      "lr 0.0100 ne 5 bs 20 | val acc 69.58 std 6.15 | test acc 70.34 std 5.53\n",
      "lr 0.0100 ne 5 bs 30 | val acc 68.24 std 6.43 | test acc 69.17 std 5.81\n",
      "lr 0.0100 ne 5 bs 40 | val acc 66.67 std 7.02 | test acc 67.71 std 6.65\n",
      "lr 0.0100 ne 10 bs 10 | val acc 68.14 std 7.26 | test acc 69.14 std 6.96\n",
      "lr 0.0100 ne 10 bs 20 | val acc 68.99 std 7.14 | test acc 69.95 std 6.83\n",
      "lr 0.0100 ne 10 bs 30 | val acc 69.48 std 7.03 | test acc 70.31 std 6.63\n",
      "lr 0.0100 ne 10 bs 40 | val acc 69.65 std 6.84 | test acc 70.39 std 6.44\n",
      "lr 0.0100 ne 15 bs 10 | val acc 70.28 std 6.88 | test acc 70.98 std 6.50\n",
      "lr 0.0100 ne 15 bs 20 | val acc 70.79 std 6.93 | test acc 71.40 std 6.47\n",
      "lr 0.0100 ne 15 bs 30 | val acc 71.08 std 6.86 | test acc 71.70 std 6.41\n",
      "lr 0.0100 ne 15 bs 40 | val acc 71.20 std 6.77 | test acc 71.83 std 6.33\n",
      "lr 0.0100 ne 20 bs 10 | val acc 71.61 std 6.77 | test acc 72.26 std 6.36\n",
      "lr 0.0100 ne 20 bs 20 | val acc 71.91 std 6.76 | test acc 72.51 std 6.29\n",
      "lr 0.0100 ne 20 bs 30 | val acc 72.10 std 6.70 | test acc 72.77 std 6.27\n",
      "lr 0.0100 ne 20 bs 40 | val acc 72.24 std 6.64 | test acc 72.93 std 6.21\n",
      "lr 0.0050 ne 5 bs 10 | val acc 72.03 std 6.66 | test acc 72.73 std 6.22\n",
      "lr 0.0050 ne 5 bs 20 | val acc 71.60 std 6.84 | test acc 72.33 std 6.37\n",
      "lr 0.0050 ne 5 bs 30 | val acc 70.97 std 7.34 | test acc 71.75 std 6.87\n",
      "lr 0.0050 ne 5 bs 40 | val acc 70.27 std 7.92 | test acc 71.10 std 7.44\n",
      "lr 0.0050 ne 10 bs 10 | val acc 70.43 std 7.85 | test acc 71.25 std 7.37\n",
      "lr 0.0050 ne 10 bs 20 | val acc 70.44 std 7.76 | test acc 71.24 std 7.28\n",
      "lr 0.0050 ne 10 bs 30 | val acc 70.36 std 7.68 | test acc 71.18 std 7.21\n",
      "lr 0.0050 ne 10 bs 40 | val acc 70.20 std 7.63 | test acc 71.04 std 7.19\n",
      "lr 0.0050 ne 15 bs 10 | val acc 70.41 std 7.61 | test acc 71.24 std 7.16\n",
      "lr 0.0050 ne 15 bs 20 | val acc 70.53 std 7.55 | test acc 71.35 std 7.10\n",
      "lr 0.0050 ne 15 bs 30 | val acc 70.59 std 7.48 | test acc 71.41 std 7.02\n",
      "lr 0.0050 ne 15 bs 40 | val acc 70.58 std 7.41 | test acc 71.40 std 6.96\n",
      "lr 0.0050 ne 20 bs 10 | val acc 70.77 std 7.40 | test acc 71.57 std 6.95\n",
      "lr 0.0050 ne 20 bs 20 | val acc 70.89 std 7.37 | test acc 71.70 std 6.92\n",
      "lr 0.0050 ne 20 bs 30 | val acc 70.97 std 7.33 | test acc 71.78 std 6.87\n",
      "lr 0.0050 ne 20 bs 40 | val acc 71.00 std 7.29 | test acc 71.81 std 6.82\n",
      "lr 0.0010 ne 5 bs 10 | val acc 70.63 std 7.55 | test acc 71.48 std 7.05\n",
      "lr 0.0010 ne 5 bs 20 | val acc 70.05 std 8.27 | test acc 70.90 std 7.82\n",
      "lr 0.0010 ne 5 bs 30 | val acc 69.22 std 9.57 | test acc 70.06 std 9.27\n",
      "lr 0.0010 ne 5 bs 40 | val acc 68.26 std 11.13 | test acc 69.09 std 10.89\n",
      "lr 0.0010 ne 10 bs 10 | val acc 68.21 std 11.03 | test acc 69.06 std 10.78\n",
      "lr 0.0010 ne 10 bs 20 | val acc 68.04 std 10.98 | test acc 68.90 std 10.72\n",
      "lr 0.0010 ne 10 bs 30 | val acc 67.76 std 11.02 | test acc 68.63 std 10.75\n",
      "lr 0.0010 ne 10 bs 40 | val acc 67.34 std 11.27 | test acc 68.22 std 11.01\n",
      "lr 0.0010 ne 15 bs 10 | val acc 67.39 std 11.17 | test acc 68.28 std 10.91\n",
      "lr 0.0010 ne 15 bs 20 | val acc 67.35 std 11.08 | test acc 68.24 std 10.81\n",
      "lr 0.0010 ne 15 bs 30 | val acc 67.24 std 11.01 | test acc 68.15 std 10.73\n",
      "lr 0.0010 ne 15 bs 40 | val acc 67.10 std 10.96 | test acc 68.01 std 10.67\n",
      "lr 0.0010 ne 20 bs 10 | val acc 67.19 std 10.89 | test acc 68.12 std 10.60\n",
      "lr 0.0010 ne 20 bs 20 | val acc 67.21 std 10.79 | test acc 68.15 std 10.52\n",
      "lr 0.0010 ne 20 bs 30 | val acc 67.17 std 10.71 | test acc 68.13 std 10.44\n",
      "lr 0.0010 ne 20 bs 40 | val acc 67.12 std 10.64 | test acc 68.08 std 10.38\n",
      "lr 0.0001 ne 5 bs 10 | val acc 66.21 std 12.33 | test acc 67.14 std 12.19\n",
      "lr 0.0001 ne 5 bs 20 | val acc 65.20 std 14.14 | test acc 66.11 std 14.10\n",
      "lr 0.0001 ne 5 bs 30 | val acc 64.18 std 15.75 | test acc 65.08 std 15.78\n",
      "lr 0.0001 ne 5 bs 40 | val acc 63.19 std 17.15 | test acc 64.07 std 17.21\n",
      "lr 0.0001 ne 10 bs 10 | val acc 62.64 std 17.48 | test acc 63.53 std 17.53\n",
      "lr 0.0001 ne 10 bs 20 | val acc 61.92 std 18.14 | test acc 62.80 std 18.19\n",
      "lr 0.0001 ne 10 bs 30 | val acc 61.14 std 18.90 | test acc 62.02 std 18.95\n",
      "lr 0.0001 ne 10 bs 40 | val acc 60.34 std 19.67 | test acc 61.21 std 19.74\n",
      "lr 0.0001 ne 15 bs 10 | val acc 60.02 std 19.67 | test acc 60.91 std 19.73\n",
      "lr 0.0001 ne 15 bs 20 | val acc 59.51 std 19.93 | test acc 60.38 std 19.98\n",
      "lr 0.0001 ne 15 bs 30 | val acc 58.91 std 20.29 | test acc 59.78 std 20.36\n",
      "lr 0.0001 ne 15 bs 40 | val acc 58.28 std 20.73 | test acc 59.14 std 20.81\n",
      "lr 0.0001 ne 20 bs 10 | val acc 58.14 std 20.61 | test acc 59.01 std 20.69\n",
      "lr 0.0001 ne 20 bs 20 | val acc 57.77 std 20.69 | test acc 58.63 std 20.78\n",
      "lr 0.0001 ne 20 bs 30 | val acc 57.30 std 20.88 | test acc 58.14 std 20.99\n",
      "lr 0.0001 ne 20 bs 40 | val acc 56.79 std 21.14 | test acc 57.62 std 21.26\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "\n",
    "ft_lrs = [0.01, 0.005, 0.001, 0.0001]\n",
    "ft_epochs = [5, 10, 15, 20]\n",
    "batch_sizes = [10, 20, 30, 40]\n",
    "te_accs, val_accs = [], []\n",
    "\n",
    "results = {}\n",
    "for ft_lr in ft_lrs:\n",
    "    results[ft_lr] = {}\n",
    "    for ft_ne in ft_epochs:\n",
    "        results[ft_lr][ft_ne] = {}\n",
    "        for bs in batch_sizes:\n",
    "            acc_losses = []\n",
    "            te_acc_losses = []\n",
    "            for i in range(len(each_worker_data)):\n",
    "                model = cnn().to(device)\n",
    "                model.apply(init_weights)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = ft_lr, momentum=.9, weight_decay=5e-5)\n",
    "                for epoch in range(ft_ne):\n",
    "                    train_loss, train_acc = train(each_worker_data[i].reshape(-1, 1, 28, 28), torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size=bs)\n",
    "                val_loss, val_acc = test(each_worker_val_data[i], each_worker_val_label[i], model, criterion, use_cuda, batch_size=100)\n",
    "                te_loss, te_acc = test(each_worker_te_data[i], each_worker_te_label[i], model, criterion, use_cuda, batch_size=100)\n",
    "\n",
    "                val_accs.append(val_acc)\n",
    "                te_accs.append(te_acc)\n",
    "\n",
    "            acc_losses = np.array(acc_losses)\n",
    "            print('lr %.4f ne %d bs %d | val acc %.2f std %.2f | test acc %.2f std %.2f' % (\n",
    "                ft_lr, ft_ne, bs, np.array(val_accs).mean(), np.array(val_accs).std(), np.array(te_accs).mean(), np.array(te_accs).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb22fd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.0100 ne 20 bs 50 | val acc 73.52 std 5.11 | test acc 74.11 std 5.15\n",
      "lr 0.0100 ne 20 bs 60 | val acc 73.19 std 5.19 | test acc 74.01 std 4.93\n",
      "lr 0.0100 ne 20 bs 70 | val acc 72.72 std 5.32 | test acc 73.67 std 4.88\n",
      "lr 0.0100 ne 20 bs 80 | val acc 72.47 std 5.30 | test acc 73.54 std 4.90\n",
      "lr 0.0100 ne 20 bs 90 | val acc 72.17 std 5.36 | test acc 73.36 std 4.92\n",
      "lr 0.0100 ne 20 bs 100 | val acc 71.90 std 5.47 | test acc 73.02 std 5.00\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "\n",
    "ft_lrs = [0.01]\n",
    "ft_epochs = [20]\n",
    "batch_sizes = [50, 60, 70, 80, 90, 100]\n",
    "te_accs, val_accs = [], []\n",
    "\n",
    "results = {}\n",
    "for ft_lr in ft_lrs:\n",
    "    results[ft_lr] = {}\n",
    "    for ft_ne in ft_epochs:\n",
    "        results[ft_lr][ft_ne] = {}\n",
    "        for bs in batch_sizes:\n",
    "            acc_losses = []\n",
    "            te_acc_losses = []\n",
    "            for i in range(len(each_worker_data)):\n",
    "                model = cnn().to(device)\n",
    "                model.apply(init_weights)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = ft_lr, momentum=.9, weight_decay=5e-5)\n",
    "                for epoch in range(ft_ne):\n",
    "                    train_loss, train_acc = train(each_worker_data[i].reshape(-1, 1, 28, 28), torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size=bs)\n",
    "                val_loss, val_acc = test(each_worker_val_data[i], each_worker_val_label[i], model, criterion, use_cuda, batch_size=100)\n",
    "                te_loss, te_acc = test(each_worker_te_data[i], each_worker_te_label[i], model, criterion, use_cuda, batch_size=100)\n",
    "\n",
    "                val_accs.append(val_acc)\n",
    "                te_accs.append(te_acc)\n",
    "\n",
    "            acc_losses = np.array(acc_losses)\n",
    "            print('lr %.4f ne %d bs %d | val acc %.2f std %.2f | test acc %.2f std %.2f' % (\n",
    "                ft_lr, ft_ne, bs, np.array(val_accs).mean(), np.array(val_accs).std(), np.array(te_accs).mean(), np.array(te_accs).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70151d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.0100 ne 20 bs 50 | val acc 73.70 std 5.75 | test acc 74.02 std 4.79\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "\n",
    "ft_lrs = [0.01]\n",
    "ft_epochs = [20]\n",
    "batch_sizes = [50]\n",
    "te_accs, val_accs = [], []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "results = {}\n",
    "for ft_lr in ft_lrs:\n",
    "    results[ft_lr] = {}\n",
    "    for ft_ne in ft_epochs:\n",
    "        results[ft_lr][ft_ne] = {}\n",
    "        for bs in batch_sizes:\n",
    "            acc_losses = []\n",
    "            te_acc_losses = []\n",
    "            for i in range(len(each_worker_data)):\n",
    "                model = cnn().to(device)\n",
    "                model.apply(init_weights)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = ft_lr, momentum=.9, weight_decay=5e-5)\n",
    "                for epoch in range(ft_ne):\n",
    "                    train_loss, train_acc = train(each_worker_data[i].reshape(-1, 1, 28, 28), torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size=bs)\n",
    "                val_loss, val_acc = test(each_worker_val_data[i], each_worker_val_label[i], model, criterion, use_cuda, batch_size=100)\n",
    "                te_loss, te_acc = test(each_worker_te_data[i], each_worker_te_label[i], model, criterion, use_cuda, batch_size=100)\n",
    "\n",
    "                val_accs.append(val_acc)\n",
    "                te_accs.append(te_acc)\n",
    "\n",
    "            acc_losses = np.array(acc_losses)\n",
    "            print('lr %.4f ne %d bs %d | val acc %.2f std %.2f | test acc %.2f std %.2f' % (\n",
    "                ft_lr, ft_ne, bs, np.array(val_accs).mean(), np.array(val_accs).std(), np.array(te_accs).mean(), np.array(te_accs).std()))\n",
    "\n",
    "pickle.dump([val_accs, te_accs], open(os.path.join(home_dir, 'FLDetector_plots_data/fashion_personalized_eval_local_train.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeced92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1fb5ef3",
   "metadata": {},
   "source": [
    "# Good FedAvg baseline Fashion MNIST + Fang distribution + 80 clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092fa012",
   "metadata": {},
   "source": [
    "# Mean without any attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2189af91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 val loss 1.402 val acc 69.700 best val_acc 69.700\n",
      "e 10 val loss 0.442 val acc 83.979 best val_acc 83.979\n",
      "e 20 val loss 0.362 val acc 87.016 best val_acc 87.016\n",
      "e 30 val loss 0.327 val acc 88.270 best val_acc 88.290\n",
      "e 40 val loss 0.306 val acc 89.246 best val_acc 89.266\n",
      "e 49 val loss 0.295 val acc 89.585 best val_acc 89.585\n",
      "e 50 val loss 0.293 val acc 89.565 best val_acc 89.585\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "fed_model = cnn().to(device)\n",
    "fed_model.apply(init_weights)\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    \n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr, momentum=0.9, weight_decay=1e-4)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(\n",
    "                each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    agg_update = torch.mean(user_updates, 0)\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    \n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, val_loss, val_acc, best_global_acc))\n",
    "    epoch_num+=1\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    client_loss, client_acc = test(each_worker_te_data[i], each_worker_te_label[i].long(),\n",
    "                                   fed_model, criterion, use_cuda)\n",
    "    final_accs_per_client.append(client_acc)\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    "    loss_per_round=np.array(loss_per_round)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_fashion_fast_mean.pkl'), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
