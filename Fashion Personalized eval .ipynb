{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# Trim attack on FedAvg + Trimmed-mean for Fashion-MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821b1014",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu124/7176245/ipykernel_89514/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a52c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "            \n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fang_train_data(trainset, num_workers=100, bias=0.5, force=False):\n",
    "    dist_file = 'fang_nworkers%d_bias%.1f.pkl' % (num_workers, bias)\n",
    "    if not force and os.path.exists(dist_file):\n",
    "        print('Loading fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "        return pickle.load(open(dist_file, 'rb'))\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    per_participant_list = defaultdict(list)\n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        per_participant_list[selected_worker].extend([i])\n",
    "    \n",
    "    print('Saving fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "    pickle.dump(per_participant_list, open(dist_file, 'wb'))\n",
    "    return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0f36f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_federated_data(trainset, num_workers, distribution='fang', param=1, force=False):\n",
    "    if distribution == 'fang':\n",
    "        per_participant_list = get_fang_train_data(trainset, num_workers, bias=param, force=force)\n",
    "    elif distribution == 'dirichlet':\n",
    "        per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=param, force=force)\n",
    "\n",
    "    each_worker_idx = [[] for _ in range(num_workers)]\n",
    "    each_worker_val_idx = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_idx = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_val_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_val_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(5 * w_len/7)\n",
    "        len_val = int(w_len/7)\n",
    "        np.random.shuffle(w_indices)\n",
    "        \n",
    "        tr_idx = w_indices[:len_tr]\n",
    "        val_idx = w_indices[len_tr: len_tr+len_val]\n",
    "        te_idx = w_indices[len_tr+len_val:]\n",
    "\n",
    "        for idx in tr_idx:\n",
    "            each_worker_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_data[worker_idx] = torch.stack(each_worker_data[worker_idx])\n",
    "        each_worker_label[worker_idx] = torch.Tensor(each_worker_label[worker_idx]).long()\n",
    "        \n",
    "        for idx in val_idx:\n",
    "            each_worker_val_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_val_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_val_data[worker_idx] = torch.stack(each_worker_val_data[worker_idx])\n",
    "        each_worker_val_label[worker_idx] = torch.Tensor(each_worker_val_label[worker_idx]).long()\n",
    "        \n",
    "        for idx in te_idx:\n",
    "            each_worker_te_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_te_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_te_data[worker_idx] = torch.stack(each_worker_te_data[worker_idx])\n",
    "        each_worker_te_label[worker_idx] = torch.Tensor(each_worker_te_label[worker_idx]).long()\n",
    "    \n",
    "    global_val_data = torch.concatenate(each_worker_val_data)\n",
    "    global_val_label = torch.concatenate(each_worker_val_label)\n",
    "    \n",
    "    global_te_data = torch.concatenate(each_worker_te_data)\n",
    "    global_te_label = torch.concatenate(each_worker_te_label)\n",
    "    \n",
    "    return each_worker_data, each_worker_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_val_data, global_val_label, global_te_data, global_te_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca6b5e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266060"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 30, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(30, 50, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(1250, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "model = cnn()\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c35e83a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(train_data, labels, model, optimizer, batch_size=20):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    len_t = (len(train_data) // batch_size)\n",
    "    if len(train_data)%batch_size:\n",
    "        len_t += 1\n",
    "    r=np.arange(len(train_data))\n",
    "    np.random.shuffle(r)\n",
    "    train_data = train_data[r]\n",
    "    labels = labels[r]\n",
    "    for ind in range(len_t):\n",
    "        inputs = train_data[ind * batch_size:(ind + 1) * batch_size].cuda()\n",
    "        targets = labels[ind * batch_size:(ind + 1) * batch_size].cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "\n",
    "def test(test_data, labels, model, criterion, use_cuda, debug_='MEDIUM', batch_size=64):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    model.eval()\n",
    "    len_t = (len(test_data) // batch_size)\n",
    "    if len(test_data)%batch_size:\n",
    "        len_t += 1\n",
    "    with torch.no_grad():\n",
    "        for ind in range(len_t):\n",
    "            inputs = test_data[ind * batch_size:(ind + 1) * batch_size].cuda()\n",
    "            targets = labels[ind * batch_size:(ind + 1) * batch_size].cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "    return (losses.avg, top1.avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ace04bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fang distribution for num_workers 100 and bias 0.5 from memory\n"
     ]
    }
   ],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = False\n",
    "\n",
    "each_worker_data, each_worker_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_val_data, global_val_label, global_te_data, global_te_label = get_federated_data(\n",
    "    all_data, num_workers=num_workers, distribution=distribution, param=param, force=force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a2cc2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8664, 8526)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(global_te_label), len(global_val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39dd2f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nat 0 | e 0 val loss 1.093 acc 64.004 | test loss 1.075 acc 65.512 || best val_acc 64.004\n",
      "nat 0 | e 5 val loss 0.556 acc 78.489 | test loss 0.545 acc 78.659 || best val_acc 78.489\n",
      "nat 0 | e 10 val loss 0.486 acc 81.316 | test loss 0.474 acc 81.798 || best val_acc 81.316\n",
      "nat 0 | e 15 val loss 0.445 acc 83.533 | test loss 0.431 acc 84.245 || best val_acc 83.533\n",
      "nat 0 | e 20 val loss 0.415 acc 84.987 | test loss 0.399 acc 85.596 || best val_acc 84.987\n",
      "nat 0 | e 25 val loss 0.394 acc 85.820 | test loss 0.376 acc 86.507 || best val_acc 85.820\n",
      "nat 0 | e 30 val loss 0.379 acc 86.395 | test loss 0.360 acc 87.211 || best val_acc 86.406\n",
      "nat 0 | e 35 val loss 0.362 acc 87.098 | test loss 0.343 acc 87.604 || best val_acc 87.204\n",
      "nat 0 | e 40 val loss 0.351 acc 87.650 | test loss 0.333 acc 87.996 || best val_acc 87.650\n",
      "nat 0 | e 45 val loss 0.341 acc 87.990 | test loss 0.323 acc 88.250 || best val_acc 87.990\n",
      "nat 0 | e 49 val loss 0.335 acc 88.177 | test loss 0.316 acc 88.643 || best val_acc 88.177\n",
      "nat 0 | e 50 val loss 0.334 acc 88.259 | test loss 0.315 acc 88.573 || best val_acc 88.259\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz_ = [0]\n",
    "max_nbyz = 20\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "for nbyz in nbyz_:\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "    epoch_num = 0\n",
    "\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    model_received = []\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "    best_accs_per_round, best_te_accs_per_round = [], []\n",
    "    accs_per_round, te_accs_per_round = [], []\n",
    "    loss_per_round = []\n",
    "    home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        torch.cuda.empty_cache()\n",
    "        round_clients = np.arange(max_nbyz-nbyz, num_workers)\n",
    "        round_benign = round_clients\n",
    "        user_updates=[]\n",
    "        benign_norm = 0\n",
    "\n",
    "        for i in round_benign:\n",
    "            model = copy.deepcopy(fed_model)\n",
    "            optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "            for epoch in range(local_epochs):\n",
    "                train_loss, train_acc = train(\n",
    "                    each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "\n",
    "            params = []\n",
    "            for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                    (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "            update =  (params - model_received)\n",
    "            benign_norm += torch.norm(update)/len(round_benign)\n",
    "            user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "        user_updates = full_trim(user_updates, nbyz)\n",
    "        agg_update = tr_mean(user_updates, nbyz)\n",
    "        del user_updates\n",
    "\n",
    "        model_received = model_received + global_lr * agg_update\n",
    "        fed_model = cnn().to(device)\n",
    "        start_idx=0\n",
    "        state_dict = {}\n",
    "        previous_name = 'none'\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "            start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "            params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "            state_dict[name] = params\n",
    "            previous_name = name\n",
    "        fed_model.load_state_dict(state_dict)\n",
    "\n",
    "        val_loss, val_acc = test(global_val_data, global_val_label.long(), fed_model, criterion, use_cuda)\n",
    "        te_loss, te_acc = test(global_te_data, global_te_label.long(), fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        best_accs_per_round.append(best_global_acc)\n",
    "        best_te_accs_per_round.append(best_global_te_acc)\n",
    "        accs_per_round.append(val_acc)\n",
    "        te_accs_per_round.append(te_acc)\n",
    "        loss_per_round.append(val_loss)\n",
    "\n",
    "        if epoch_num%5==0 or epoch_num==nepochs-1:\n",
    "            print('nat %d | e %d val loss %.3f acc %.3f | test loss %.3f acc %.3f || best val_acc %.3f'% (nbyz, epoch_num, val_loss, val_acc, te_loss, te_acc, best_global_acc))\n",
    "\n",
    "        epoch_num+=1\n",
    "\n",
    "    final_accs_per_client=[]\n",
    "    for i in range(max_nbyz, num_workers):\n",
    "        client_loss, client_acc = test(each_worker_te_data[i], each_worker_te_label[i].long(),\n",
    "                                       fed_model, criterion, use_cuda)\n",
    "        final_accs_per_client.append(client_acc)\n",
    "    results = collections.OrderedDict(\n",
    "        final_accs_per_client=np.array(final_accs_per_client),\n",
    "        accs_per_round=np.array(accs_per_round),\n",
    "        best_accs_per_round=np.array(best_accs_per_round),\n",
    "        loss_per_round=np.array(loss_per_round)\n",
    "    )\n",
    "    pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/fashion100_personalized_eval_noattack.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10aa6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "final_accs_per_client=[]\n",
    "for i in range(max_nbyz, num_workers):\n",
    "    client_loss, client_acc = test(each_worker_te_data[i], each_worker_te_label[i].long(),\n",
    "                                   fed_model, criterion, use_cuda)\n",
    "    final_accs_per_client.append(client_acc)\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    "    loss_per_round=np.array(loss_per_round)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/fashion100_personalized_eval_noattack.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3656203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nat 1 | e 0 val loss 1.048 acc 65.705 | test loss 1.032 acc 67.417 || best val_acc 65.705\n",
      "nat 1 | e 10 val loss 0.476 acc 81.703 | test loss 0.464 acc 82.214 || best val_acc 81.703\n",
      "nat 1 | e 20 val loss 0.409 acc 85.057 | test loss 0.394 acc 85.803 || best val_acc 85.057\n",
      "nat 1 | e 30 val loss 0.372 acc 86.887 | test loss 0.358 acc 87.281 || best val_acc 86.887\n",
      "nat 1 | e 40 val loss 0.350 acc 87.661 | test loss 0.337 acc 87.950 || best val_acc 87.661\n",
      "nat 1 | e 49 val loss 0.334 acc 88.201 | test loss 0.323 acc 88.573 || best val_acc 88.201\n",
      "nat 1 | e 50 val loss 0.333 acc 88.271 | test loss 0.321 acc 88.723 || best val_acc 88.271\n",
      "---> len of final_accs_per_client: 100 \n",
      "\n",
      "nat 5 | e 0 val loss 1.132 acc 67.488 | test loss 1.117 acc 69.090 || best val_acc 67.488\n",
      "nat 5 | e 10 val loss 0.508 acc 80.577 | test loss 0.493 acc 81.221 || best val_acc 80.577\n",
      "nat 5 | e 20 val loss 0.451 acc 83.005 | test loss 0.435 acc 84.014 || best val_acc 83.028\n",
      "nat 5 | e 30 val loss 0.416 acc 84.577 | test loss 0.400 acc 85.665 || best val_acc 84.577\n",
      "nat 5 | e 40 val loss 0.392 acc 85.843 | test loss 0.376 acc 86.807 || best val_acc 85.843\n",
      "nat 5 | e 49 val loss 0.375 acc 86.430 | test loss 0.361 acc 87.223 || best val_acc 86.430\n",
      "nat 5 | e 50 val loss 0.374 acc 86.488 | test loss 0.359 acc 87.292 || best val_acc 86.488\n",
      "---> len of final_accs_per_client: 100 \n",
      "\n",
      "nat 10 | e 0 val loss 1.166 acc 66.385 | test loss 1.154 acc 66.909 || best val_acc 66.385\n",
      "nat 10 | e 10 val loss 0.512 acc 80.460 | test loss 0.501 acc 80.921 || best val_acc 80.460\n",
      "nat 10 | e 20 val loss 0.459 acc 82.958 | test loss 0.446 acc 83.691 || best val_acc 82.958\n",
      "nat 10 | e 30 val loss 0.432 acc 84.295 | test loss 0.418 acc 84.603 || best val_acc 84.295\n",
      "nat 10 | e 40 val loss 0.417 acc 85.057 | test loss 0.402 acc 85.572 || best val_acc 85.057\n",
      "nat 10 | e 49 val loss 0.408 acc 85.433 | test loss 0.394 acc 86.092 || best val_acc 85.480\n",
      "nat 10 | e 50 val loss 0.406 acc 85.656 | test loss 0.392 acc 86.138 || best val_acc 85.656\n",
      "---> len of final_accs_per_client: 100 \n",
      "\n",
      "nat 15 | e 0 val loss 1.183 acc 64.462 | test loss 1.167 acc 65.859 || best val_acc 64.462\n",
      "nat 15 | e 10 val loss 0.537 acc 79.240 | test loss 0.526 acc 79.986 || best val_acc 79.240\n",
      "nat 15 | e 20 val loss 0.490 acc 81.492 | test loss 0.479 acc 81.717 || best val_acc 81.492\n",
      "nat 15 | e 30 val loss 0.469 acc 82.325 | test loss 0.457 acc 82.964 || best val_acc 82.325\n",
      "nat 15 | e 40 val loss 0.452 acc 83.415 | test loss 0.439 acc 83.783 || best val_acc 83.556\n",
      "nat 15 | e 49 val loss 0.443 acc 83.802 | test loss 0.429 acc 84.407 || best val_acc 83.802\n",
      "nat 15 | e 50 val loss 0.442 acc 83.873 | test loss 0.428 acc 84.361 || best val_acc 83.873\n",
      "---> len of final_accs_per_client: 100 \n",
      "\n",
      "nat 20 | e 0 val loss 1.218 acc 64.133 | test loss 1.206 acc 64.970 || best val_acc 64.133\n",
      "nat 20 | e 10 val loss 0.563 acc 78.771 | test loss 0.553 acc 79.017 || best val_acc 78.771\n",
      "nat 20 | e 20 val loss 0.510 acc 80.835 | test loss 0.502 acc 81.279 || best val_acc 80.835\n",
      "nat 20 | e 30 val loss 0.487 acc 81.891 | test loss 0.478 acc 82.248 || best val_acc 81.891\n",
      "nat 20 | e 40 val loss 0.467 acc 82.782 | test loss 0.457 acc 83.102 || best val_acc 82.782\n",
      "nat 20 | e 49 val loss 0.462 acc 82.993 | test loss 0.450 acc 83.553 || best val_acc 83.122\n",
      "nat 20 | e 50 val loss 0.463 acc 82.923 | test loss 0.450 acc 83.587 || best val_acc 83.122\n",
      "---> len of final_accs_per_client: 100 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz_ = [1, 5, 10, 15, 20]\n",
    "max_nbyz = 20\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "for nbyz in nbyz_:\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "    epoch_num = 0\n",
    "\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    model_received = []\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "    best_accs_per_round, best_te_accs_per_round = [], []\n",
    "    accs_per_round, te_accs_per_round = [], []\n",
    "    loss_per_round = []\n",
    "    home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        torch.cuda.empty_cache()\n",
    "        round_clients = np.arange(max_nbyz - nbyz, num_workers)\n",
    "        round_benign = round_clients\n",
    "        user_updates=[]\n",
    "        benign_norm = 0\n",
    "\n",
    "        for i in round_benign:\n",
    "            model = copy.deepcopy(fed_model)\n",
    "            optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "            for epoch in range(local_epochs):\n",
    "                train_loss, train_acc = train(\n",
    "                    each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "\n",
    "            params = []\n",
    "            for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                    (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "            update =  (params - model_received)\n",
    "            benign_norm += torch.norm(update)/len(round_benign)\n",
    "            user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "        user_updates = full_trim(user_updates, nbyz)\n",
    "        agg_update = tr_mean(user_updates, nbyz)\n",
    "        del user_updates\n",
    "\n",
    "        model_received = model_received + global_lr * agg_update\n",
    "        fed_model = cnn().to(device)\n",
    "        start_idx=0\n",
    "        state_dict = {}\n",
    "        previous_name = 'none'\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "            start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "            params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "            state_dict[name] = params\n",
    "            previous_name = name\n",
    "        fed_model.load_state_dict(state_dict)\n",
    "\n",
    "        val_loss, val_acc = test(global_val_data, global_val_label.long(), fed_model, criterion, use_cuda)\n",
    "        te_loss, te_acc = test(global_te_data, global_te_label.long(), fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        best_accs_per_round.append(best_global_acc)\n",
    "        best_te_accs_per_round.append(best_global_te_acc)\n",
    "        accs_per_round.append(val_acc)\n",
    "        te_accs_per_round.append(te_acc)\n",
    "        loss_per_round.append(val_loss)\n",
    "\n",
    "        if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "            print('nat %d | e %d val loss %.3f acc %.3f | test loss %.3f acc %.3f || best val_acc %.3f'% (nbyz, epoch_num, val_loss, val_acc, te_loss, te_acc, best_global_acc))\n",
    "\n",
    "        epoch_num+=1\n",
    "\n",
    "    final_accs_per_client=[]\n",
    "    for i in range(num_workers):\n",
    "        client_loss, client_acc = test(each_worker_te_data[i], each_worker_te_label[i].long(),\n",
    "                                       fed_model, criterion, use_cuda)\n",
    "        final_accs_per_client.append(client_acc)\n",
    "    results = collections.OrderedDict(\n",
    "        final_accs_per_client=np.array(final_accs_per_client),\n",
    "        accs_per_round=np.array(accs_per_round),\n",
    "        best_accs_per_round=np.array(best_accs_per_round),\n",
    "        loss_per_round=np.array(loss_per_round)\n",
    "    )\n",
    "    print('---> len of final_accs_per_client: %d \\n'% len(final_accs_per_client))\n",
    "    pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/fashion100_personalized_eval_full_knowledge_trim%d.pkl'%(nbyz)), 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c91678b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nat 1 | e 0 val loss 1.082 acc 67.816 | test loss 1.067 acc 69.183 || best val_acc 67.816\n",
      "nat 1 | e 10 val loss 0.477 acc 82.477 | test loss 0.464 acc 82.872 || best val_acc 82.477\n",
      "nat 1 | e 20 val loss 0.406 acc 85.445 | test loss 0.390 acc 86.184 || best val_acc 85.538\n",
      "nat 1 | e 30 val loss 0.368 acc 87.028 | test loss 0.353 acc 87.385 || best val_acc 87.028\n",
      "nat 1 | e 40 val loss 0.344 acc 87.825 | test loss 0.330 acc 88.066 || best val_acc 87.825\n",
      "nat 1 | e 49 val loss 0.331 acc 88.259 | test loss 0.316 acc 88.504 || best val_acc 88.283\n",
      "nat 1 | e 50 val loss 0.329 acc 88.318 | test loss 0.315 acc 88.631 || best val_acc 88.318\n",
      "---> len of final_accs_per_client: 100 \n",
      "\n",
      "nat 5 | e 0 val loss 1.025 acc 66.561 | test loss 1.005 acc 67.313 || best val_acc 66.561\n",
      "nat 5 | e 10 val loss 0.469 acc 82.524 | test loss 0.455 acc 82.906 || best val_acc 82.524\n",
      "nat 5 | e 20 val loss 0.408 acc 85.233 | test loss 0.394 acc 85.849 || best val_acc 85.233\n",
      "nat 5 | e 30 val loss 0.374 acc 86.254 | test loss 0.361 acc 87.165 || best val_acc 86.289\n",
      "nat 5 | e 40 val loss 0.354 acc 87.133 | test loss 0.342 acc 87.777 || best val_acc 87.169\n",
      "nat 5 | e 49 val loss 0.341 acc 87.579 | test loss 0.330 acc 88.216 || best val_acc 87.603\n",
      "nat 5 | e 50 val loss 0.340 acc 87.790 | test loss 0.328 acc 88.308 || best val_acc 87.790\n",
      "---> len of final_accs_per_client: 100 \n",
      "\n",
      "nat 10 | e 0 val loss 1.150 acc 67.347 | test loss 1.134 acc 67.602 || best val_acc 67.347\n",
      "nat 10 | e 10 val loss 0.518 acc 80.260 | test loss 0.506 acc 80.586 || best val_acc 80.260\n",
      "nat 10 | e 20 val loss 0.449 acc 83.673 | test loss 0.437 acc 84.049 || best val_acc 83.673\n",
      "nat 10 | e 30 val loss 0.415 acc 85.233 | test loss 0.401 acc 85.884 || best val_acc 85.233\n",
      "nat 10 | e 40 val loss 0.395 acc 86.019 | test loss 0.382 acc 86.969 || best val_acc 86.160\n",
      "nat 10 | e 49 val loss 0.381 acc 86.758 | test loss 0.367 acc 87.569 || best val_acc 86.758\n",
      "nat 10 | e 50 val loss 0.381 acc 86.887 | test loss 0.367 acc 87.615 || best val_acc 86.887\n",
      "---> len of final_accs_per_client: 100 \n",
      "\n",
      "nat 15 | e 0 val loss 1.104 acc 67.359 | test loss 1.088 acc 68.410 || best val_acc 67.359\n",
      "nat 15 | e 10 val loss 0.530 acc 79.639 | test loss 0.522 acc 79.975 || best val_acc 79.639\n",
      "nat 15 | e 20 val loss 0.479 acc 82.184 | test loss 0.466 acc 82.999 || best val_acc 82.219\n",
      "nat 15 | e 40 val loss 0.427 acc 85.198 | test loss 0.414 acc 85.561 || best val_acc 85.198\n",
      "nat 15 | e 49 val loss 0.420 acc 85.796 | test loss 0.407 acc 86.057 || best val_acc 85.925\n",
      "nat 15 | e 50 val loss 0.422 acc 85.749 | test loss 0.409 acc 86.011 || best val_acc 85.925\n",
      "---> len of final_accs_per_client: 100 \n",
      "\n",
      "nat 20 | e 0 val loss 1.214 acc 66.162 | test loss 1.199 acc 67.013 || best val_acc 66.162\n",
      "nat 20 | e 10 val loss 0.543 acc 79.451 | test loss 0.535 acc 80.009 || best val_acc 79.451\n",
      "nat 20 | e 20 val loss 0.503 acc 81.457 | test loss 0.494 acc 81.821 || best val_acc 81.762\n",
      "nat 20 | e 30 val loss 0.478 acc 82.923 | test loss 0.466 acc 83.137 || best val_acc 82.923\n",
      "nat 20 | e 40 val loss 0.459 acc 84.319 | test loss 0.448 acc 84.568 || best val_acc 84.319\n",
      "nat 20 | e 49 val loss 0.460 acc 84.494 | test loss 0.449 acc 84.915 || best val_acc 84.717\n",
      "nat 20 | e 50 val loss 0.460 acc 84.624 | test loss 0.448 acc 85.122 || best val_acc 84.717\n",
      "---> len of final_accs_per_client: 100 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz_ = [1, 5, 10, 15, 20]\n",
    "max_nbyz = 20\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "for nbyz in nbyz_:\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "    epoch_num = 0\n",
    "\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    model_received = []\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "    best_accs_per_round, best_te_accs_per_round = [], []\n",
    "    accs_per_round, te_accs_per_round = [], []\n",
    "    loss_per_round = []\n",
    "    home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        torch.cuda.empty_cache()\n",
    "        round_clients = np.arange(max_nbyz - nbyz, num_workers)\n",
    "        round_benign = round_clients\n",
    "        user_updates=[]\n",
    "        benign_norm = 0\n",
    "\n",
    "        for i in round_benign:\n",
    "            model = copy.deepcopy(fed_model)\n",
    "            optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "            for epoch in range(local_epochs):\n",
    "                train_loss, train_acc = train(\n",
    "                    each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "\n",
    "            params = []\n",
    "            for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                    (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "            update =  (params - model_received)\n",
    "            benign_norm += torch.norm(update)/len(round_benign)\n",
    "            user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "        user_updates[:nbyz] = full_trim(user_updates[:nbyz], nbyz)\n",
    "        agg_update = tr_mean(user_updates, nbyz)\n",
    "\n",
    "        del user_updates\n",
    "        model_received = model_received + global_lr * agg_update\n",
    "        fed_model = cnn().to(device)\n",
    "        start_idx=0\n",
    "        state_dict = {}\n",
    "        previous_name = 'none'\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "            start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "            params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "            state_dict[name] = params\n",
    "            previous_name = name\n",
    "        fed_model.load_state_dict(state_dict)\n",
    "\n",
    "        val_loss, val_acc = test(global_val_data, global_val_label.long(), fed_model, criterion, use_cuda)\n",
    "        te_loss, te_acc = test(global_te_data, global_te_label.long(), fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        best_accs_per_round.append(best_global_acc)\n",
    "        best_te_accs_per_round.append(best_global_te_acc)\n",
    "        accs_per_round.append(val_acc)\n",
    "        te_accs_per_round.append(te_acc)\n",
    "        loss_per_round.append(val_loss)\n",
    "\n",
    "        if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "            print('nat %d | e %d val loss %.3f acc %.3f | test loss %.3f acc %.3f || best val_acc %.3f'% (nbyz, epoch_num, val_loss, val_acc, te_loss, te_acc, best_global_acc))\n",
    "\n",
    "        epoch_num+=1\n",
    "\n",
    "    final_accs_per_client=[]\n",
    "    for i in range(num_workers):\n",
    "        client_loss, client_acc = test(each_worker_te_data[i], each_worker_te_label[i].long(),\n",
    "                                       fed_model, criterion, use_cuda)\n",
    "        final_accs_per_client.append(client_acc)\n",
    "    results = collections.OrderedDict(\n",
    "        final_accs_per_client=np.array(final_accs_per_client),\n",
    "        accs_per_round=np.array(accs_per_round),\n",
    "        best_accs_per_round=np.array(best_accs_per_round),\n",
    "        loss_per_round=np.array(loss_per_round)\n",
    "    )\n",
    "    print('---> len of final_accs_per_client: %d \\n'% len(final_accs_per_client))\n",
    "    pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/fashion100_personalized_eval_trim%d.pkl'%(nbyz)), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62c27e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
