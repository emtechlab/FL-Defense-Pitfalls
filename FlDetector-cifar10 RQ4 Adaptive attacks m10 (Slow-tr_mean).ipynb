{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for CIFAR10 with Fang distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e300ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu036/4396566/ipykernel_1699371/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "import copy\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77aa30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_models import *\n",
    "from cifar10_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "\n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fang_train_data(trainset, num_workers=100, bias=0.5, force=False):\n",
    "    dist_file = 'fang_nworkers%d_bias%.1f.pkl' % (num_workers, bias)\n",
    "    if not force and os.path.exists(dist_file):\n",
    "        print('Loading fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "        return pickle.load(open(dist_file, 'rb'))\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    per_participant_list = defaultdict(list)\n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        per_participant_list[selected_worker].extend([i])\n",
    "    \n",
    "    print('Saving fang distribution for num_workers %d and bias %.1f to memory' % (num_workers, bias))\n",
    "    pickle.dump(per_participant_list, open(dist_file, 'wb'))\n",
    "    return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799ffb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_federated_data(trainset, num_workers, distribution='fang', param=1, force=False):\n",
    "    if distribution == 'fang':\n",
    "        per_participant_list = get_fang_train_data(trainset, num_workers, bias=param, force=force)\n",
    "    elif distribution == 'dirichlet':\n",
    "        per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=param, force=force)\n",
    "\n",
    "    each_worker_idx = [[] for _ in range(num_workers)] \n",
    "    each_worker_te_idx = [[] for _ in range(num_workers)]\n",
    "    np.random.seed(0)\n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(5*w_len/6)\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        each_worker_idx[worker_idx] = w_indices[tr_idx]\n",
    "        each_worker_te_idx[worker_idx] = w_indices[te_idx]\n",
    "    global_test_idx = np.concatenate(each_worker_te_idx)\n",
    "    return each_worker_idx, each_worker_te_idx, global_test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2590db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(dataset, indices, batch_size=32, shuffle=False):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acbb06d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/home/vshejwalkar_umass_edu/data/'\n",
    "# load the train dataset\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_train)\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_train)\n",
    "\n",
    "te_cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_test)\n",
    "te_cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(trainloader, model, model_received, criterion, optimizer, pgd=False, eps=2):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size()[0])\n",
    "        top1.update(prec1.item()/100.0, inputs.size()[0])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if pgd:\n",
    "            curr_model = list(model.parameters())\n",
    "            curr_model_vec = parameters_to_vector(curr_model)\n",
    "            if torch.norm(curr_model_vec - model_received) > eps:\n",
    "                curr_model_vec = eps*(curr_model_vec - model_received)/torch.norm(curr_model_vec - model_received) + model_received\n",
    "                vector_to_parameters(curr_model_vec, curr_model)\n",
    "        \n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def test(testloader, model, criterion):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(testloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        top1.update(prec1/100.0, inputs.size()[0])\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f843efb",
   "metadata": {},
   "source": [
    "# resnet-BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bb2ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5202826d",
   "metadata": {},
   "source": [
    "# FLDetector utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cabefee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def fldetector(old_gradients, user_grads, b=0, hvp=None, agr='tr_mean'):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    if agr == 'average':\n",
    "        agg_grads = torch.mean(user_grads, 0)\n",
    "    elif agr == 'median':\n",
    "        agg_grads = torch.median(user_grads, 0)[0]\n",
    "    elif agr == 'tr_mean':\n",
    "        agg_grads = tr_mean(user_grads, b)\n",
    "    return agg_grads, distance\n",
    "\n",
    "def lbfgs(S_k_list, Y_k_list, v):\n",
    "    curr_S_k = torch.stack(S_k_list).T\n",
    "    curr_Y_k = torch.stack(Y_k_list).T\n",
    "    S_k_time_Y_k = np.dot(curr_S_k.T.cpu().numpy(), curr_Y_k.cpu().numpy())\n",
    "    S_k_time_S_k = np.dot(curr_S_k.T.cpu().numpy(), curr_S_k.cpu().numpy())\n",
    "    R_k = np.triu(S_k_time_Y_k)\n",
    "    L_k = S_k_time_Y_k - R_k\n",
    "    sigma_k = np.dot(Y_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()) / (np.dot(S_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()))\n",
    "    D_k_diag = np.diag(S_k_time_Y_k)\n",
    "    upper_mat = np.concatenate((sigma_k * S_k_time_S_k, L_k), axis=1)\n",
    "    lower_mat = np.concatenate((L_k.T, -np.diag(D_k_diag)), axis=1)\n",
    "    mat = np.concatenate((upper_mat, lower_mat), axis=0)\n",
    "    mat_inv = np.linalg.inv(mat)\n",
    "\n",
    "    approx_prod = sigma_k * v.cpu().numpy()\n",
    "    approx_prod = approx_prod.T\n",
    "    p_mat = np.concatenate((np.dot(curr_S_k.T.cpu().numpy(), sigma_k * v.unsqueeze(0).T.cpu().numpy()), np.dot(curr_Y_k.T.cpu().numpy(), v.unsqueeze(0).T.cpu().numpy())), axis=0)\n",
    "    approx_prod -= np.dot(np.dot(np.concatenate((sigma_k * curr_S_k.cpu().numpy(), curr_Y_k.cpu().numpy()), axis=1), mat_inv), p_mat)\n",
    "\n",
    "    return approx_prod\n",
    "\n",
    "def detection(score, nobyz, nworkers):\n",
    "    estimator = KMeans(n_clusters=2)\n",
    "    estimator.fit(score.reshape(-1, 1))\n",
    "    label_pred = estimator.labels_\n",
    "    if np.mean(score[label_pred==0])<np.mean(score[label_pred==1]):\n",
    "        #0 is the label of malicious clients\n",
    "        label_pred = 1 - label_pred\n",
    "    real_label=np.ones(nworkers)\n",
    "    real_label[:nobyz]=0\n",
    "    acc=len(label_pred[label_pred==real_label])/nworkers\n",
    "    recall=1-np.sum(label_pred[:nobyz])/nobyz\n",
    "    fpr=1-np.sum(label_pred[nobyz:])/(nworkers-nobyz)\n",
    "    fnr=np.sum(label_pred[:nobyz])/nobyz\n",
    "    auc = roc_auc_score(real_label, label_pred)\n",
    "    print(\"acc %0.4f; recall %0.4f; fpr %0.4f; fnr %0.4f; auc %.4f\" % (acc, recall, fpr, fnr, auc))\n",
    "    return acc, fpr, fnr, auc\n",
    "    # print(silhouette_score(score.reshape(-1, 1), label_pred))\n",
    "\n",
    "def detection1(score, nobyz):\n",
    "    nrefs = 10\n",
    "    ks = range(1, 8)\n",
    "    gaps = np.zeros(len(ks))\n",
    "    gapDiff = np.zeros(len(ks) - 1)\n",
    "    sdk = np.zeros(len(ks))\n",
    "    min = np.min(score)\n",
    "    max = np.max(score)\n",
    "    score = (score - min)/(max-min)\n",
    "    for i, k in enumerate(ks):\n",
    "        estimator = KMeans(n_clusters=k)\n",
    "        estimator.fit(score.reshape(-1, 1))\n",
    "        label_pred = estimator.labels_\n",
    "        center = estimator.cluster_centers_\n",
    "        Wk = np.sum([np.square(score[m]-center[label_pred[m]]) for m in range(len(score))])\n",
    "        WkRef = np.zeros(nrefs)\n",
    "        for j in range(nrefs):\n",
    "            rand = np.random.uniform(0, 1, len(score))\n",
    "            estimator = KMeans(n_clusters=k)\n",
    "            estimator.fit(rand.reshape(-1, 1))\n",
    "            label_pred = estimator.labels_\n",
    "            center = estimator.cluster_centers_\n",
    "            WkRef[j] = np.sum([np.square(rand[m]-center[label_pred[m]]) for m in range(len(rand))])\n",
    "        gaps[i] = np.log(np.mean(WkRef)) - np.log(Wk)\n",
    "        sdk[i] = np.sqrt((1.0 + nrefs) / nrefs) * np.std(np.log(WkRef))\n",
    "\n",
    "        if i > 0:\n",
    "            gapDiff[i - 1] = gaps[i - 1] - gaps[i] + sdk[i]\n",
    "    #print(gapDiff)\n",
    "    for i in range(len(gapDiff)):\n",
    "        if gapDiff[i] >= 0:\n",
    "            select_k = i+1\n",
    "            break\n",
    "    if select_k == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        # print('Attack Detected!')\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e25e2ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fang distribution for num_workers 100 and bias 0.5 from memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 100, 10038)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = False\n",
    "\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=100, distribution=distribution, param=param, force=force)\n",
    "\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    train_loaders.append((pos, get_train(all_data, indices, len(indices))))\n",
    "test_loaders = []\n",
    "for pos, indices in enumerate(each_worker_te_idx):\n",
    "    test_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "len(train_loaders), len(test_loaders), len(global_test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "771158c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r 0 nmal 10 | e 0 benign_norm 10.436 val loss 36.550 val acc 0.069 best val_acc 0.069\n",
      "r 0 nmal 10 | e 10 benign_norm 7.158 val loss 18.004 val acc 0.099 best val_acc 0.105\n",
      "r 0 nmal 10 | e 20 benign_norm 6.831 val loss 14.230 val acc 0.098 best val_acc 0.105\n",
      "r 0 nmal 10 | e 30 benign_norm 6.683 val loss 10.562 val acc 0.098 best val_acc 0.105\n",
      "r 0 nmal 10 | e 40 benign_norm 6.629 val loss 8.557 val acc 0.098 best val_acc 0.117\n",
      "r 0 nmal 10 | e 50 benign_norm 6.440 val loss 7.526 val acc 0.099 best val_acc 0.117\n",
      "r 0 nmal 10 | e 60 benign_norm 6.272 val loss 6.709 val acc 0.101 best val_acc 0.117\n",
      "r 0 nmal 10 | e 70 benign_norm 6.157 val loss 5.904 val acc 0.103 best val_acc 0.117\n",
      "r 0 nmal 10 | e 80 benign_norm 6.098 val loss 5.325 val acc 0.110 best val_acc 0.126\n",
      "r 0 nmal 10 | e 90 benign_norm 6.054 val loss 4.997 val acc 0.117 best val_acc 0.134\n",
      "!!!! Stop at iteration: 91\n",
      "r 0 nmal 10 | e 91 benign_norm 6.114 val loss 4.997 val acc 0.117 best val_acc 0.134\n",
      "acc 0.7778; recall 1.0000; fpr 0.2500; fnr 0.0000; auc 0.8750\n",
      "r 1 nmal 10 | e 10 benign_norm 6.688 val loss 26.163 val acc 0.098 best val_acc 0.111\n",
      "r 1 nmal 10 | e 20 benign_norm 6.356 val loss 11.354 val acc 0.110 best val_acc 0.126\n",
      "!!!! Stop at iteration: 22\n",
      "r 1 nmal 10 | e 22 benign_norm 6.332 val loss 9.404 val acc 0.124 best val_acc 0.126\n",
      "acc 0.4444; recall 0.0000; fpr 0.5000; fnr 1.0000; auc 0.2500\n",
      "r 2 nmal 10 | e 0 benign_norm 11.391 val loss 24.378 val acc 0.118 best val_acc 0.118\n",
      "r 2 nmal 10 | e 10 benign_norm 6.545 val loss 13.162 val acc 0.099 best val_acc 0.118\n",
      "r 2 nmal 10 | e 20 benign_norm 6.113 val loss 4.804 val acc 0.107 best val_acc 0.118\n",
      "r 2 nmal 10 | e 30 benign_norm 6.003 val loss 4.180 val acc 0.136 best val_acc 0.136\n",
      "r 2 nmal 10 | e 40 benign_norm 5.982 val loss 4.519 val acc 0.139 best val_acc 0.153\n",
      "r 2 nmal 10 | e 50 benign_norm 6.039 val loss 6.338 val acc 0.113 best val_acc 0.153\n",
      "r 2 nmal 10 | e 60 benign_norm 5.968 val loss 6.103 val acc 0.117 best val_acc 0.153\n",
      "!!!! Stop at iteration: 62\n",
      "r 2 nmal 10 | e 62 benign_norm 5.961 val loss 3.074 val acc 0.152 best val_acc 0.153\n",
      "acc 0.8667; recall 1.0000; fpr 0.1500; fnr 0.0000; auc 0.9250\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_malicious = [10]\n",
    "n_runs = 3\n",
    "results={}\n",
    "for n_mal in n_malicious:\n",
    "    nbyz = n_mal\n",
    "    results[n_mal] = {'acc': [], 'fpr': [], 'fnr': [], 'auc': [], 'stop_e': []}\n",
    "    for run in range(n_runs):\n",
    "        torch.cuda.empty_cache()\n",
    "        nepochs=200\n",
    "        local_epochs = 1\n",
    "        batch_size = 16\n",
    "        num_workers = 100\n",
    "        local_lr = 1\n",
    "        global_lr = .1\n",
    "        agr = 'tr_mean'\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = resnet20().cuda()\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        best_accs_per_round = []\n",
    "        accs_per_round = []\n",
    "        loss_per_round = []\n",
    "        home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "        # FLDetector initializations\n",
    "        weight_record = []\n",
    "        grad_record = []\n",
    "        test_grads = []\n",
    "        old_grad_list = []\n",
    "        malicious_scores = np.zeros((1, num_workers-(20-nbyz)))\n",
    "        start_detection_epoch = 10\n",
    "        window_size = 10\n",
    "        assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "        # Adaptive attack initializers\n",
    "        good_distance_range = np.zeros((1, nbyz))\n",
    "        attack_type = 'NDSS21'\n",
    "        dev_type = 'sign'\n",
    "        \n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(20-nbyz, num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_grads=[]\n",
    "            benign_norm = 0\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr)\n",
    "\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "            weight = model_received\n",
    "\n",
    "            if (epoch_num > start_detection_epoch):\n",
    "                hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "                hvp = np.squeeze(hvp)\n",
    "            else:\n",
    "                hvp = None\n",
    "\n",
    "            good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "            if hvp is not None:\n",
    "                pred_grad = copy.deepcopy(good_old_grads)\n",
    "                distance = []\n",
    "                for i in range(len(good_old_grads)):\n",
    "                    pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "                good_distance_range = np.concatenate(\n",
    "                    (good_distance_range, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "            if attack_type != 'none' and (epoch_num < start_detection_epoch):\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "                # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "            elif epoch_num > start_detection_epoch:\n",
    "                if attack_type == 'full_trim':\n",
    "                    user_grads = full_trim(user_grads, nbyz)\n",
    "                elif attack_type == 'none':\n",
    "                    pass\n",
    "                else:\n",
    "                    if attack_type == 'NDSS21':\n",
    "                        distance_bound = np.random.choice(np.mean(good_distance_range[-1:], 0))\n",
    "                        model_re = torch.mean(good_current_grads, dim=0)\n",
    "                        if dev_type == 'unit_vec':\n",
    "                            deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                        elif dev_type == 'sign':\n",
    "                            deviation = torch.sign(model_re)\n",
    "                        elif dev_type == 'std':\n",
    "                            deviation = torch.std(good_current_grads, 0)\n",
    "                        # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_range[-1]))) / torch.norm(deviation))\n",
    "                        noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                    elif attack_type == 'mod_trim':\n",
    "                        mal_grads= full_trim(user_grads[:nbyz], nbyz)\n",
    "                        pass\n",
    "                    else:\n",
    "                        noise = torch.zeros(hvp.shape).to(device)\n",
    "                    for m in range(nbyz):\n",
    "                        user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "            agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "            if distance is not None and epoch_num > (start_detection_epoch - window_size):\n",
    "                malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "            if malicious_scores.shape[0] >= window_size+1:\n",
    "                if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                    print('!!!! Stop at iteration:', epoch_num)\n",
    "                    print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "                    acc, fpr, fnr, auc = detection(\n",
    "                        np.sum(malicious_scores[-window_size:], axis=0), nbyz, len(user_grads))\n",
    "                    results[n_mal]['acc'].append(acc)\n",
    "                    results[n_mal]['fpr'].append(fpr)\n",
    "                    results[n_mal]['fnr'].append(fnr)\n",
    "                    results[n_mal]['auc'].append(auc)\n",
    "                    results[n_mal]['stop_e'].append(epoch_num)\n",
    "                    break\n",
    "                    break\n",
    "\n",
    "            if epoch_num > (start_detection_epoch - window_size):\n",
    "                weight_record.append(weight - last_weight)\n",
    "                grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "            if (len(weight_record) > window_size):\n",
    "                del weight_record[0]\n",
    "                del grad_record[0]\n",
    "\n",
    "            last_weight = weight\n",
    "            last_grad = agg_grads\n",
    "            old_grad_list = user_grads\n",
    "            good_old_grads = good_current_grads\n",
    "            del user_grads\n",
    "\n",
    "            model_received = model_received + global_lr * (0.9999**epoch_num) * agg_grads\n",
    "            fed_model = resnet20().cuda()\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "            if is_best:\n",
    "                best_model = copy.deepcopy(fed_model)\n",
    "\n",
    "            best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "            accs_per_round.append(val_acc.cpu().item())\n",
    "            loss_per_round.append(val_loss.cpu().item())\n",
    "\n",
    "            if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "                print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "            if math.isnan(val_loss) or val_loss > 100000:\n",
    "                print('val loss %f... exit'%val_loss)\n",
    "                break\n",
    "            epoch_num+=1\n",
    "\n",
    "        final_accs_per_client=[]\n",
    "        for i in range(num_workers):\n",
    "            client_loss, client_acc = test(test_loaders[i][1], best_model, criterion)\n",
    "            final_accs_per_client.append(client_acc.cpu().item())\n",
    "        all_results = collections.OrderedDict(\n",
    "            final_accs_per_client=np.array(final_accs_per_client),\n",
    "            accs_per_round=np.array(accs_per_round),\n",
    "            best_accs_per_round=np.array(best_accs_per_round),\n",
    "            loss_per_round=np.array(loss_per_round),\n",
    "            results=results\n",
    "        )\n",
    "        pickle.dump(all_results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq4_adaptive_cifar10_slow_trmean_NDSS21_sign_std_m%d_r%d.pkl' % (nbyz, run)), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4e6d834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r 0 nmal 10 | e 0 benign_norm 9.418 val loss 65.667 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 10 | e 10 benign_norm 7.066 val loss 39.863 val acc 0.099 best val_acc 0.100\n",
      "r 0 nmal 10 | e 20 benign_norm 6.903 val loss 36.876 val acc 0.099 best val_acc 0.100\n",
      "!!!! Stop at iteration: 25\n",
      "r 0 nmal 10 | e 25 benign_norm 7.124 val loss 30.824 val acc 0.098 best val_acc 0.100\n",
      "acc 0.4111; recall 0.0000; fpr 0.5375; fnr 1.0000; auc 0.2313\n",
      "r 1 nmal 10 | e 0 benign_norm 9.727 val loss 106.697 val acc 0.100 best val_acc 0.100\n",
      "r 1 nmal 10 | e 10 benign_norm 7.062 val loss 18.567 val acc 0.098 best val_acc 0.103\n",
      "r 1 nmal 10 | e 20 benign_norm 7.113 val loss 26.159 val acc 0.098 best val_acc 0.140\n",
      "r 1 nmal 10 | e 30 benign_norm 7.452 val loss 37.416 val acc 0.099 best val_acc 0.140\n",
      "r 1 nmal 10 | e 40 benign_norm 8.020 val loss 35.504 val acc 0.101 best val_acc 0.160\n",
      "r 1 nmal 10 | e 50 benign_norm 8.441 val loss 33.905 val acc 0.131 best val_acc 0.160\n",
      "r 1 nmal 10 | e 60 benign_norm 8.880 val loss 36.813 val acc 0.108 best val_acc 0.160\n",
      "!!!! Stop at iteration: 64\n",
      "r 1 nmal 10 | e 64 benign_norm 9.042 val loss 51.767 val acc 0.102 best val_acc 0.160\n",
      "acc 0.7111; recall 1.0000; fpr 0.3250; fnr 0.0000; auc 0.8375\n",
      "r 2 nmal 10 | e 0 benign_norm 10.232 val loss 74.148 val acc 0.099 best val_acc 0.099\n",
      "r 2 nmal 10 | e 10 benign_norm 6.769 val loss 18.772 val acc 0.100 best val_acc 0.101\n",
      "r 2 nmal 10 | e 20 benign_norm 7.041 val loss 37.927 val acc 0.098 best val_acc 0.101\n",
      "r 2 nmal 10 | e 30 benign_norm 7.684 val loss 61.412 val acc 0.098 best val_acc 0.113\n",
      "r 2 nmal 10 | e 40 benign_norm 8.727 val loss 88.481 val acc 0.098 best val_acc 0.116\n",
      "r 2 nmal 10 | e 50 benign_norm 9.592 val loss 86.457 val acc 0.098 best val_acc 0.116\n",
      "r 2 nmal 10 | e 60 benign_norm 10.187 val loss 104.744 val acc 0.099 best val_acc 0.133\n",
      "r 2 nmal 10 | e 70 benign_norm 9.256 val loss 59.675 val acc 0.093 best val_acc 0.133\n",
      "r 2 nmal 10 | e 80 benign_norm 8.940 val loss 85.242 val acc 0.100 best val_acc 0.133\n",
      "!!!! Stop at iteration: 81\n",
      "r 2 nmal 10 | e 81 benign_norm 9.115 val loss 85.242 val acc 0.100 best val_acc 0.133\n",
      "acc 0.8778; recall 1.0000; fpr 0.1375; fnr 0.0000; auc 0.9313\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_malicious = [10]\n",
    "n_runs = 3\n",
    "results={}\n",
    "for n_mal in n_malicious:\n",
    "    nbyz = n_mal\n",
    "    results[n_mal] = {'acc': [], 'fpr': [], 'fnr': [], 'auc': [], 'stop_e': []}\n",
    "    for run in range(n_runs):\n",
    "        torch.cuda.empty_cache()\n",
    "        nepochs=200\n",
    "        local_epochs = 1\n",
    "        batch_size = 16\n",
    "        num_workers = 100\n",
    "        local_lr = 1\n",
    "        global_lr = .1\n",
    "        agr = 'tr_mean'\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = resnet20().cuda()\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        best_accs_per_round = []\n",
    "        accs_per_round = []\n",
    "        loss_per_round = []\n",
    "        home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "        # FLDetector initializations\n",
    "        weight_record = []\n",
    "        grad_record = []\n",
    "        test_grads = []\n",
    "        old_grad_list = []\n",
    "        malicious_scores = np.zeros((1, num_workers-(20-nbyz)))\n",
    "        start_detection_epoch = 10\n",
    "        window_size = 10\n",
    "        assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "        # Adaptive attack initializers\n",
    "        good_distance_range = np.zeros((1, nbyz))\n",
    "        attack_type = 'NDSS21'\n",
    "        dev_type = 'std'\n",
    "        \n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(20-nbyz, num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_grads=[]\n",
    "            benign_norm = 0\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr)\n",
    "\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "            weight = model_received\n",
    "\n",
    "            if (epoch_num > start_detection_epoch):\n",
    "                hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "                hvp = np.squeeze(hvp)\n",
    "            else:\n",
    "                hvp = None\n",
    "\n",
    "            good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "            if hvp is not None:\n",
    "                pred_grad = copy.deepcopy(good_old_grads)\n",
    "                distance = []\n",
    "                for i in range(len(good_old_grads)):\n",
    "                    pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "                good_distance_range = np.concatenate(\n",
    "                    (good_distance_range, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "            if attack_type != 'none' and (epoch_num < start_detection_epoch):\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "                # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "            elif epoch_num > start_detection_epoch:\n",
    "                if attack_type == 'full_trim':\n",
    "                    user_grads = full_trim(user_grads, nbyz)\n",
    "                elif attack_type == 'none':\n",
    "                    pass\n",
    "                else:\n",
    "                    if attack_type == 'NDSS21':\n",
    "                        distance_bound = np.random.choice(np.mean(good_distance_range[-1:], 0))\n",
    "                        model_re = torch.mean(good_current_grads, dim=0)\n",
    "                        if dev_type == 'unit_vec':\n",
    "                            deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                        elif dev_type == 'sign':\n",
    "                            deviation = torch.sign(model_re)\n",
    "                        elif dev_type == 'std':\n",
    "                            deviation = torch.std(good_current_grads, 0)\n",
    "                        # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_range[-1]))) / torch.norm(deviation))\n",
    "                        noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                    elif attack_type == 'mod_trim':\n",
    "                        mal_grads= full_trim(user_grads[:nbyz], nbyz)\n",
    "                        pass\n",
    "                    else:\n",
    "                        noise = torch.zeros(hvp.shape).to(device)\n",
    "                    for m in range(nbyz):\n",
    "                        user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "            agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "            if distance is not None and epoch_num > (start_detection_epoch - window_size):\n",
    "                malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "            if malicious_scores.shape[0] >= window_size+1:\n",
    "                if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                    print('!!!! Stop at iteration:', epoch_num)\n",
    "                    print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "                    acc, fpr, fnr, auc = detection(\n",
    "                        np.sum(malicious_scores[-window_size:], axis=0), nbyz, len(user_grads))\n",
    "                    results[n_mal]['acc'].append(acc)\n",
    "                    results[n_mal]['fpr'].append(fpr)\n",
    "                    results[n_mal]['fnr'].append(fnr)\n",
    "                    results[n_mal]['auc'].append(auc)\n",
    "                    results[n_mal]['stop_e'].append(epoch_num)\n",
    "                    break\n",
    "                    break\n",
    "\n",
    "            if epoch_num > (start_detection_epoch - window_size):\n",
    "                weight_record.append(weight - last_weight)\n",
    "                grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "            if (len(weight_record) > window_size):\n",
    "                del weight_record[0]\n",
    "                del grad_record[0]\n",
    "\n",
    "            last_weight = weight\n",
    "            last_grad = agg_grads\n",
    "            old_grad_list = user_grads\n",
    "            good_old_grads = good_current_grads\n",
    "            del user_grads\n",
    "\n",
    "            model_received = model_received + global_lr * (0.9999**epoch_num) * agg_grads\n",
    "            fed_model = resnet20().cuda()\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "            if is_best:\n",
    "                best_model = copy.deepcopy(fed_model)\n",
    "\n",
    "            best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "            accs_per_round.append(val_acc.cpu().item())\n",
    "            loss_per_round.append(val_loss.cpu().item())\n",
    "\n",
    "            if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "                print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "            if math.isnan(val_loss) or val_loss > 100000:\n",
    "                print('val loss %f... exit'%val_loss)\n",
    "                break\n",
    "            epoch_num+=1\n",
    "\n",
    "        final_accs_per_client=[]\n",
    "        for i in range(num_workers):\n",
    "            client_loss, client_acc = test(test_loaders[i][1], best_model, criterion)\n",
    "            final_accs_per_client.append(client_acc.cpu().item())\n",
    "        all_results = collections.OrderedDict(\n",
    "            final_accs_per_client=np.array(final_accs_per_client),\n",
    "            accs_per_round=np.array(accs_per_round),\n",
    "            best_accs_per_round=np.array(best_accs_per_round),\n",
    "            loss_per_round=np.array(loss_per_round),\n",
    "            results=results\n",
    "        )\n",
    "        pickle.dump(all_results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq4_adaptive_cifar10_slow_trmean_NDSS21_sign_std_m%d_r%d.pkl' % (nbyz, run)), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9be1f569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r 0 nmal 10 | e 0 benign_norm 11.088 val loss 85.563 val acc 0.103 best val_acc 0.103\n",
      "r 0 nmal 10 | e 10 benign_norm 7.321 val loss 24.727 val acc 0.099 best val_acc 0.103\n",
      "r 0 nmal 10 | e 20 benign_norm 6.878 val loss 12.082 val acc 0.099 best val_acc 0.105\n",
      "!!!! Stop at iteration: 29\n",
      "r 0 nmal 10 | e 29 benign_norm 6.838 val loss 10.414 val acc 0.139 best val_acc 0.139\n",
      "acc 0.3778; recall 0.0000; fpr 0.5750; fnr 1.0000; auc 0.2125\n",
      "r 1 nmal 10 | e 0 benign_norm 10.809 val loss 33.304 val acc 0.102 best val_acc 0.102\n",
      "r 1 nmal 10 | e 10 benign_norm 6.978 val loss 9.561 val acc 0.099 best val_acc 0.102\n",
      "r 1 nmal 10 | e 20 benign_norm 6.828 val loss 10.382 val acc 0.100 best val_acc 0.102\n",
      "r 1 nmal 10 | e 30 benign_norm 7.366 val loss 13.231 val acc 0.124 best val_acc 0.159\n",
      "!!!! Stop at iteration: 31\n",
      "r 1 nmal 10 | e 31 benign_norm 7.263 val loss 13.231 val acc 0.124 best val_acc 0.159\n",
      "acc 0.7111; recall 0.0000; fpr 0.2000; fnr 1.0000; auc 0.4000\n",
      "r 2 nmal 10 | e 0 benign_norm 11.587 val loss 25.955 val acc 0.087 best val_acc 0.087\n",
      "r 2 nmal 10 | e 10 benign_norm 6.706 val loss 10.943 val acc 0.098 best val_acc 0.101\n",
      "r 2 nmal 10 | e 20 benign_norm 6.493 val loss 13.438 val acc 0.099 best val_acc 0.113\n",
      "r 2 nmal 10 | e 30 benign_norm 6.766 val loss 27.073 val acc 0.098 best val_acc 0.113\n",
      "r 2 nmal 10 | e 40 benign_norm 6.829 val loss 38.482 val acc 0.098 best val_acc 0.113\n",
      "r 2 nmal 10 | e 50 benign_norm 6.953 val loss 51.757 val acc 0.099 best val_acc 0.113\n",
      "r 2 nmal 10 | e 60 benign_norm 7.116 val loss 67.704 val acc 0.094 best val_acc 0.113\n",
      "r 2 nmal 10 | e 70 benign_norm 7.344 val loss 93.808 val acc 0.098 best val_acc 0.113\n",
      "r 2 nmal 10 | e 80 benign_norm 7.568 val loss 97.584 val acc 0.106 best val_acc 0.113\n",
      "!!!! Stop at iteration: 89\n",
      "r 2 nmal 10 | e 89 benign_norm 8.362 val loss 109.063 val acc 0.098 best val_acc 0.113\n",
      "acc 0.7000; recall 1.0000; fpr 0.3375; fnr 0.0000; auc 0.8313\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_malicious = [10]\n",
    "n_runs = 3\n",
    "results={}\n",
    "for n_mal in n_malicious:\n",
    "    nbyz = n_mal\n",
    "    results[n_mal] = {'acc': [], 'fpr': [], 'fnr': [], 'auc': [], 'stop_e': []}\n",
    "    for run in range(n_runs):\n",
    "        torch.cuda.empty_cache()\n",
    "        nepochs=200\n",
    "        local_epochs = 1\n",
    "        batch_size = 16\n",
    "        num_workers = 100\n",
    "        local_lr = 1\n",
    "        global_lr = .1\n",
    "        agr = 'tr_mean'\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = resnet20().cuda()\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        best_accs_per_round = []\n",
    "        accs_per_round = []\n",
    "        loss_per_round = []\n",
    "        home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "        # FLDetector initializations\n",
    "        weight_record = []\n",
    "        grad_record = []\n",
    "        test_grads = []\n",
    "        old_grad_list = []\n",
    "        malicious_scores = np.zeros((1, num_workers-(20-nbyz)))\n",
    "        start_detection_epoch = 10\n",
    "        window_size = 10\n",
    "        assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "        # Adaptive attack initializers\n",
    "        good_distance_range = np.zeros((1, nbyz))\n",
    "        attack_type = 'NDSS21'\n",
    "        dev_type = 'std'\n",
    "        \n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(20-nbyz, num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_grads=[]\n",
    "            benign_norm = 0\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr)\n",
    "\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "            weight = model_received\n",
    "\n",
    "            if (epoch_num > start_detection_epoch):\n",
    "                hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "                hvp = np.squeeze(hvp)\n",
    "            else:\n",
    "                hvp = None\n",
    "\n",
    "            good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "            if hvp is not None:\n",
    "                pred_grad = copy.deepcopy(good_old_grads)\n",
    "                distance = []\n",
    "                for i in range(len(good_old_grads)):\n",
    "                    pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "                good_distance_range = np.concatenate(\n",
    "                    (good_distance_range, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "            if attack_type != 'none' and (epoch_num < start_detection_epoch):\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "                # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "            elif epoch_num > start_detection_epoch:\n",
    "                if attack_type == 'full_trim':\n",
    "                    user_grads = full_trim(user_grads, nbyz)\n",
    "                elif attack_type == 'none':\n",
    "                    pass\n",
    "                else:\n",
    "                    if attack_type == 'NDSS21':\n",
    "                        distance_bound = np.mean(np.mean(good_distance_range[-1:], 0))\n",
    "                        model_re = torch.mean(good_current_grads, dim=0)\n",
    "                        if dev_type == 'unit_vec':\n",
    "                            deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                        elif dev_type == 'sign':\n",
    "                            deviation = torch.sign(model_re)\n",
    "                        elif dev_type == 'std':\n",
    "                            deviation = torch.std(good_current_grads, 0)\n",
    "                        # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_range[-1]))) / torch.norm(deviation))\n",
    "                        noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                    elif attack_type == 'mod_trim':\n",
    "                        mal_grads= full_trim(user_grads[:nbyz], nbyz)\n",
    "                        pass\n",
    "                    else:\n",
    "                        noise = torch.zeros(hvp.shape).to(device)\n",
    "                    for m in range(nbyz):\n",
    "                        user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "            agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "            if distance is not None and epoch_num > (start_detection_epoch - window_size):\n",
    "                malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "            if malicious_scores.shape[0] >= window_size+1:\n",
    "                if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                    print('!!!! Stop at iteration:', epoch_num)\n",
    "                    print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "                    acc, fpr, fnr, auc = detection(\n",
    "                        np.sum(malicious_scores[-window_size:], axis=0), nbyz, len(user_grads))\n",
    "                    results[n_mal]['acc'].append(acc)\n",
    "                    results[n_mal]['fpr'].append(fpr)\n",
    "                    results[n_mal]['fnr'].append(fnr)\n",
    "                    results[n_mal]['auc'].append(auc)\n",
    "                    results[n_mal]['stop_e'].append(epoch_num)\n",
    "                    break\n",
    "                    break\n",
    "\n",
    "            if epoch_num > (start_detection_epoch - window_size):\n",
    "                weight_record.append(weight - last_weight)\n",
    "                grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "            if (len(weight_record) > window_size):\n",
    "                del weight_record[0]\n",
    "                del grad_record[0]\n",
    "\n",
    "            last_weight = weight\n",
    "            last_grad = agg_grads\n",
    "            old_grad_list = user_grads\n",
    "            good_old_grads = good_current_grads\n",
    "            del user_grads\n",
    "\n",
    "            model_received = model_received + global_lr * (0.9999**epoch_num) * agg_grads\n",
    "            fed_model = resnet20().cuda()\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "            if is_best:\n",
    "                best_model = copy.deepcopy(fed_model)\n",
    "\n",
    "            best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "            accs_per_round.append(val_acc.cpu().item())\n",
    "            loss_per_round.append(val_loss.cpu().item())\n",
    "\n",
    "            if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "                print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "            if math.isnan(val_loss) or val_loss > 100000:\n",
    "                print('val loss %f... exit'%val_loss)\n",
    "                break\n",
    "            epoch_num+=1\n",
    "\n",
    "        final_accs_per_client=[]\n",
    "        for i in range(num_workers):\n",
    "            client_loss, client_acc = test(test_loaders[i][1], best_model, criterion)\n",
    "            final_accs_per_client.append(client_acc.cpu().item())\n",
    "        all_results = collections.OrderedDict(\n",
    "            final_accs_per_client=np.array(final_accs_per_client),\n",
    "            accs_per_round=np.array(accs_per_round),\n",
    "            best_accs_per_round=np.array(best_accs_per_round),\n",
    "            loss_per_round=np.array(loss_per_round),\n",
    "            results=results\n",
    "        )\n",
    "        pickle.dump(all_results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq4_adaptive_cifar10_slow_trmean_NDSS21_sign_std_m%d_r%d.pkl' % (nbyz, run)), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a09e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
