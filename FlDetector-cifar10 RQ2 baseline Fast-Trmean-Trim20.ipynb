{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for CIFAR10with Fang distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e300ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu065/7154297/ipykernel_1169706/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "import copy\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77aa30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_models import *\n",
    "from cifar10_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "\n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fang_train_data(trainset, num_workers=100, bias=0.5, force=False):\n",
    "    dist_file = 'fang_nworkers%d_bias%.1f.pkl' % (num_workers, bias)\n",
    "    if not force and os.path.exists(dist_file):\n",
    "        print('Loading fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "        return pickle.load(open(dist_file, 'rb'))\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    per_participant_list = defaultdict(list)\n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        per_participant_list[selected_worker].extend([i])\n",
    "    \n",
    "    print('Saving fang distribution for num_workers %d and bias %.1f to memory' % (num_workers, bias))\n",
    "    pickle.dump(per_participant_list, open(dist_file, 'wb'))\n",
    "    return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799ffb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_federated_data(trainset, num_workers, distribution='fang', param=1, force=False):\n",
    "    if distribution == 'fang':\n",
    "        per_participant_list = get_fang_train_data(trainset, num_workers, bias=param, force=force)\n",
    "    elif distribution == 'dirichlet':\n",
    "        per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=param, force=force)\n",
    "\n",
    "    each_worker_idx = [[] for _ in range(num_workers)] \n",
    "    each_worker_te_idx = [[] for _ in range(num_workers)]\n",
    "    np.random.seed(0)\n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(5*w_len/6)\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        each_worker_idx[worker_idx] = w_indices[tr_idx]\n",
    "        each_worker_te_idx[worker_idx] = w_indices[te_idx]\n",
    "    global_test_idx = np.concatenate(each_worker_te_idx)\n",
    "    return each_worker_idx, each_worker_te_idx, global_test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2590db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(dataset, indices, batch_size=32, shuffle=False):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acbb06d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/home/vshejwalkar_umass_edu/data/'\n",
    "# load the train dataset\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_train)\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_train)\n",
    "\n",
    "te_cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_test)\n",
    "te_cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(trainloader, model, model_received, criterion, optimizer, pgd=False, eps=2):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size()[0])\n",
    "        top1.update(prec1.item()/100.0, inputs.size()[0])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if pgd:\n",
    "            curr_model = list(model.parameters())\n",
    "            curr_model_vec = parameters_to_vector(curr_model)\n",
    "            if torch.norm(curr_model_vec - model_received) > eps:\n",
    "                curr_model_vec = eps*(curr_model_vec - model_received)/torch.norm(curr_model_vec - model_received) + model_received\n",
    "                vector_to_parameters(curr_model_vec, curr_model)\n",
    "        \n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def test(testloader, model, criterion):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(testloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        top1.update(prec1/100.0, inputs.size()[0])\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62958a8",
   "metadata": {},
   "source": [
    "# densenet-GN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "382ff809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from opacus.validators import ModuleValidator\n",
    "\n",
    "__all__ = ['densenet']\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, inplanes, expansion=4, growthRate=12, dropRate=0):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        planes = expansion * growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, growthRate, kernel_size=3, \n",
    "                               padding=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropRate = dropRate\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        if self.dropRate > 0:\n",
    "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
    "\n",
    "        out = torch.cat((x, out), 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, inplanes, expansion=1, growthRate=12, dropRate=0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        planes = expansion * growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = nn.Conv2d(inplanes, growthRate, kernel_size=3, \n",
    "                               padding=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropRate = dropRate\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        if self.dropRate > 0:\n",
    "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
    "\n",
    "        out = torch.cat((x, out), 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, inplanes, outplanes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = nn.Conv2d(inplanes, outplanes, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "\n",
    "    def __init__(self, depth=22, block=Bottleneck, \n",
    "        dropRate=0, num_classes=10, growthRate=12, compressionRate=2):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        assert (depth - 4) % 3 == 0, 'depth should be 3n+4'\n",
    "        n = (depth - 4) / 3 if block == BasicBlock else (depth - 4) // 6\n",
    "\n",
    "        self.growthRate = growthRate\n",
    "        self.dropRate = dropRate\n",
    "\n",
    "        # self.inplanes is a global variable used across multiple\n",
    "        # helper functions\n",
    "        self.inplanes = growthRate * 2 \n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.dense1 = self._make_denseblock(block, n)\n",
    "        self.trans1 = self._make_transition(compressionRate)\n",
    "        self.dense2 = self._make_denseblock(block, n)\n",
    "        self.trans2 = self._make_transition(compressionRate)\n",
    "        self.dense3 = self._make_denseblock(block, n)\n",
    "        self.bn = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(self.inplanes, num_classes)\n",
    "\n",
    "        # Weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_denseblock(self, block, blocks):\n",
    "        layers = []\n",
    "        for i in range(blocks):\n",
    "            # Currently we fix the expansion ratio as the default value\n",
    "            layers.append(block(self.inplanes, growthRate=self.growthRate, dropRate=self.dropRate))\n",
    "            self.inplanes += self.growthRate\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_transition(self, compressionRate):\n",
    "        inplanes = self.inplanes\n",
    "        outplanes = int(math.floor(self.inplanes // compressionRate))\n",
    "        self.inplanes = outplanes\n",
    "        return Transition(inplanes, outplanes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.trans1(self.dense1(x)) \n",
    "        x = self.trans2(self.dense2(x)) \n",
    "        x = self.dense3(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def densenet(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a ResNet model.\n",
    "    \"\"\"\n",
    "    return DenseNet(**kwargs)\n",
    "\n",
    "config = {\n",
    "    'arch': 'densenet',\n",
    "    'depth': 52,\n",
    "    'growthRate': 12,\n",
    "    'compressionRate': 2,\n",
    "    'drop': 0,\n",
    "    'num_classes': 10,\n",
    "}\n",
    "\n",
    "def densenet_gn():\n",
    "    return densenet(\n",
    "            num_classes=config['num_classes'],\n",
    "            depth=config['depth'],\n",
    "            growthRate=config['growthRate'],\n",
    "            compressionRate=config['compressionRate'],\n",
    "            dropRate=config['drop'],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a8ec2",
   "metadata": {},
   "source": [
    "# VGG-noBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341f587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48b8b7ed",
   "metadata": {},
   "source": [
    "# resnet-BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5204c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e25e2ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fang distribution for num_workers 100 and bias 0.5 from memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 100, 10038)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = False\n",
    "\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=100, distribution=distribution, param=param, force=force)\n",
    "\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    train_loaders.append((pos, get_train(all_data, indices, batch_size)))\n",
    "test_loaders = []\n",
    "for pos, indices in enumerate(each_worker_te_idx):\n",
    "    test_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "len(train_loaders), len(test_loaders), len(global_test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62611dc3",
   "metadata": {},
   "source": [
    "# Mean under partial trim attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c68ca6ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 benign_norm 7.109 val loss 2.316 val acc 0.099 best val_acc 0.099\n",
      "e 10 benign_norm 3.121 val loss 1.934 val acc 0.279 best val_acc 0.279\n",
      "e 20 benign_norm 3.408 val loss 1.847 val acc 0.315 best val_acc 0.315\n",
      "e 30 benign_norm 4.030 val loss 1.799 val acc 0.341 best val_acc 0.341\n",
      "e 40 benign_norm 4.417 val loss 1.750 val acc 0.365 best val_acc 0.365\n",
      "e 50 benign_norm 4.812 val loss 1.707 val acc 0.383 best val_acc 0.384\n",
      "e 60 benign_norm 5.112 val loss 1.667 val acc 0.403 best val_acc 0.406\n",
      "e 70 benign_norm 5.195 val loss 1.665 val acc 0.412 best val_acc 0.412\n",
      "e 80 benign_norm 5.506 val loss 1.638 val acc 0.422 best val_acc 0.422\n",
      "e 90 benign_norm 5.730 val loss 1.660 val acc 0.427 best val_acc 0.432\n",
      "e 100 benign_norm 5.973 val loss 1.702 val acc 0.425 best val_acc 0.436\n",
      "e 110 benign_norm 5.943 val loss 1.641 val acc 0.442 best val_acc 0.442\n",
      "e 120 benign_norm 6.161 val loss 1.627 val acc 0.446 best val_acc 0.446\n",
      "e 130 benign_norm 6.127 val loss 1.618 val acc 0.452 best val_acc 0.452\n",
      "e 140 benign_norm 6.274 val loss 1.633 val acc 0.451 best val_acc 0.453\n",
      "e 150 benign_norm 6.366 val loss 1.585 val acc 0.458 best val_acc 0.458\n",
      "e 160 benign_norm 6.486 val loss 1.613 val acc 0.456 best val_acc 0.459\n",
      "e 170 benign_norm 6.540 val loss 1.598 val acc 0.457 best val_acc 0.465\n",
      "e 180 benign_norm 6.583 val loss 1.571 val acc 0.463 best val_acc 0.466\n",
      "e 190 benign_norm 6.553 val loss 1.598 val acc 0.463 best val_acc 0.466\n",
      "e 199 benign_norm 6.557 val loss 1.594 val acc 0.468 best val_acc 0.474\n",
      "e 200 benign_norm 6.572 val loss 1.599 val acc 0.467 best val_acc 0.474\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m     client_loss, client_acc \u001b[38;5;241m=\u001b[39m test(test_loaders[i][\u001b[38;5;241m1\u001b[39m], fed_model, criterion)\n\u001b[1;32m     82\u001b[0m     final_accs_per_client\u001b[38;5;241m.\u001b[39mappend(client_acc)\n\u001b[1;32m     83\u001b[0m results \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mOrderedDict(\n\u001b[0;32m---> 84\u001b[0m     final_accs_per_client\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_accs_per_client\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     85\u001b[0m     accs_per_round\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(accs_per_round),\n\u001b[1;32m     86\u001b[0m     best_accs_per_round\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(best_accs_per_round),\n\u001b[1;32m     87\u001b[0m     loss_per_round\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(loss_per_round)\n\u001b[1;32m     88\u001b[0m )\n\u001b[1;32m     89\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump(results, \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(home_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFLDetector_plots_data/rq1_baseline_cifar10_fast_trmean_trim20.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torch/_tensor.py:757\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.1\n",
    "global_lr = 1\n",
    "nbyz = 20\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "fed_model = densenet_gn().cuda()\n",
    "fed_model = ModuleValidator.fix(fed_model)\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    user_updates[:nbyz] = full_trim(user_updates[:nbyz], nbyz)\n",
    "    agg_update = tr_mean(user_updates, nbyz)\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = densenet_gn().cuda()\n",
    "    fed_model = ModuleValidator.fix(fed_model)\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    \n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "    \n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "    epoch_num+=1\n",
    "    \n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    client_loss, client_acc = test(test_loaders[i][1], fed_model, criterion)\n",
    "    final_accs_per_client.append(client_acc)\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    "    loss_per_round=np.array(loss_per_round)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_cifar10_fast_trmean_trim20.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52ffcf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_accs_per_client_ = ([n.cpu().item() for n in final_accs_per_client])\n",
    "accs_per_round_ = ([n.cpu().item() for n in accs_per_round])\n",
    "best_accs_per_round_ =([n.cpu().item() for n in best_accs_per_round])\n",
    "loss_per_round_ = ([n.cpu().item() for n in loss_per_round])\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client_),\n",
    "    accs_per_round=np.array(accs_per_round_),\n",
    "    best_accs_per_round=np.array(best_accs_per_round_),\n",
    "    loss_per_round=np.array(loss_per_round_)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_cifar10_fast_trmean_trim20.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed6f34",
   "metadata": {},
   "source": [
    "# Attack Resnet20BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "579a03b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 benign_norm 782.413 val loss 2.310 val acc 0.098 best val_acc 0.098\n",
      "e 10 benign_norm 277.659 val loss 1.768 val acc 0.346 best val_acc 0.346\n",
      "e 20 benign_norm 277.721 val loss 1.667 val acc 0.409 best val_acc 0.413\n",
      "e 30 benign_norm 278.085 val loss 1.646 val acc 0.419 best val_acc 0.438\n",
      "e 40 benign_norm 278.773 val loss 1.695 val acc 0.429 best val_acc 0.451\n",
      "e 50 benign_norm 279.670 val loss 1.637 val acc 0.478 best val_acc 0.478\n",
      "e 60 benign_norm 281.176 val loss 1.695 val acc 0.474 best val_acc 0.483\n",
      "e 70 benign_norm 282.138 val loss 1.683 val acc 0.482 best val_acc 0.492\n",
      "e 80 benign_norm 283.360 val loss 1.581 val acc 0.516 best val_acc 0.516\n",
      "e 90 benign_norm 286.147 val loss 1.540 val acc 0.504 best val_acc 0.523\n",
      "e 100 benign_norm 291.872 val loss 1.792 val acc 0.498 best val_acc 0.533\n",
      "e 110 benign_norm 293.738 val loss 1.615 val acc 0.524 best val_acc 0.533\n",
      "e 120 benign_norm 297.880 val loss 1.694 val acc 0.519 best val_acc 0.533\n",
      "e 130 benign_norm 306.630 val loss 1.681 val acc 0.525 best val_acc 0.541\n",
      "e 140 benign_norm 307.089 val loss 1.799 val acc 0.501 best val_acc 0.541\n",
      "e 150 benign_norm 334.480 val loss 1.974 val acc 0.514 best val_acc 0.541\n",
      "e 160 benign_norm 319.865 val loss 1.550 val acc 0.516 best val_acc 0.541\n",
      "e 170 benign_norm 395.673 val loss 2.136 val acc 0.507 best val_acc 0.541\n",
      "e 180 benign_norm 337.501 val loss 1.762 val acc 0.501 best val_acc 0.541\n",
      "e 190 benign_norm 370.490 val loss 2.050 val acc 0.496 best val_acc 0.541\n",
      "e 199 benign_norm 346.419 val loss 2.020 val acc 0.485 best val_acc 0.541\n",
      "e 200 benign_norm 344.746 val loss 1.911 val acc 0.501 best val_acc 0.541\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m     client_loss, client_acc \u001b[38;5;241m=\u001b[39m test(test_loaders[i][\u001b[38;5;241m1\u001b[39m], fed_model, criterion)\n\u001b[1;32m     80\u001b[0m     final_accs_per_client\u001b[38;5;241m.\u001b[39mappend(client_acc)\n\u001b[1;32m     81\u001b[0m results \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mOrderedDict(\n\u001b[0;32m---> 82\u001b[0m     final_accs_per_client\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_accs_per_client\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     83\u001b[0m     accs_per_round\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(accs_per_round),\n\u001b[1;32m     84\u001b[0m     best_accs_per_round\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(best_accs_per_round),\n\u001b[1;32m     85\u001b[0m     loss_per_round\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(loss_per_round)\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump(results, \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(home_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFLDetector_plots_data/rq1_baseline_cifar10_fast_trmean_trim20_resnet20BN.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torch/_tensor.py:757\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.1\n",
    "global_lr = 1\n",
    "nbyz = 20\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    user_updates[:nbyz] = full_trim(user_updates[:nbyz], nbyz)\n",
    "    agg_update = tr_mean(user_updates, nbyz)\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    \n",
    "    best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "    accs_per_round.append(val_acc.cpu().item())\n",
    "    loss_per_round.append(val_loss.cpu().item())\n",
    "    \n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "    epoch_num+=1\n",
    "    \n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    client_loss, client_acc = test(test_loaders[i][1], fed_model, criterion)\n",
    "    final_accs_per_client.append(client_acc.cpu().item())\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    "    loss_per_round=np.array(loss_per_round)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_cifar10_fast_trmean_trim20_resnet20BN.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9a4d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_accs_per_client_ = ([n.cpu().item() for n in final_accs_per_client])\n",
    "accs_per_round_ = ([n.cpu().item() for n in accs_per_round])\n",
    "best_accs_per_round_ =([n.cpu().item() for n in best_accs_per_round])\n",
    "loss_per_round_ = ([n.cpu().item() for n in loss_per_round])\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client_),\n",
    "    accs_per_round=np.array(accs_per_round_),\n",
    "    best_accs_per_round=np.array(best_accs_per_round_),\n",
    "    loss_per_round=np.array(loss_per_round_)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_cifar10_fast_trmean_trim20_resnet20BN.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c1a42d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 benign_norm 3058.231 val loss 2.311 val acc 0.101 best val_acc 0.101\n",
      "e 10 benign_norm 277.763 val loss 1.754 val acc 0.350 best val_acc 0.350\n",
      "e 20 benign_norm 276.793 val loss 1.626 val acc 0.411 best val_acc 0.411\n",
      "e 30 benign_norm 277.714 val loss 1.648 val acc 0.425 best val_acc 0.434\n",
      "e 40 benign_norm 278.180 val loss 1.561 val acc 0.455 best val_acc 0.455\n",
      "e 50 benign_norm 279.617 val loss 1.534 val acc 0.471 best val_acc 0.473\n",
      "e 60 benign_norm 280.093 val loss 1.668 val acc 0.453 best val_acc 0.473\n",
      "e 70 benign_norm 280.758 val loss 1.511 val acc 0.497 best val_acc 0.497\n",
      "e 80 benign_norm 283.087 val loss 1.515 val acc 0.482 best val_acc 0.497\n",
      "e 90 benign_norm 284.523 val loss 1.516 val acc 0.487 best val_acc 0.497\n",
      "e 100 benign_norm 287.042 val loss 1.528 val acc 0.476 best val_acc 0.497\n",
      "e 110 benign_norm 289.512 val loss 1.593 val acc 0.477 best val_acc 0.502\n",
      "e 120 benign_norm 294.773 val loss 1.555 val acc 0.479 best val_acc 0.508\n",
      "e 130 benign_norm 302.582 val loss 1.746 val acc 0.462 best val_acc 0.508\n",
      "e 140 benign_norm 304.123 val loss 1.814 val acc 0.478 best val_acc 0.516\n",
      "e 150 benign_norm 314.185 val loss 1.622 val acc 0.507 best val_acc 0.516\n",
      "e 160 benign_norm 321.273 val loss 1.830 val acc 0.475 best val_acc 0.516\n",
      "e 170 benign_norm 327.319 val loss 1.811 val acc 0.490 best val_acc 0.516\n",
      "e 180 benign_norm 331.284 val loss 1.780 val acc 0.483 best val_acc 0.516\n",
      "e 190 benign_norm 344.888 val loss 1.742 val acc 0.496 best val_acc 0.516\n",
      "e 199 benign_norm 353.107 val loss 1.792 val acc 0.465 best val_acc 0.516\n",
      "e 200 benign_norm 335.863 val loss 1.493 val acc 0.511 best val_acc 0.516\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.1\n",
    "global_lr = 1\n",
    "nbyz = 20\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "fed_model = resnet20().cuda()\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    for client_idx in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[client_idx][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for _, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    user_updates[:nbyz] = full_trim(user_updates[:nbyz], nbyz)\n",
    "    agg_update = tr_mean(user_updates, nbyz)\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = resnet20().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    \n",
    "    best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "    accs_per_round.append(val_acc.cpu().item())\n",
    "    loss_per_round.append(val_loss.cpu().item())\n",
    "    \n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "    epoch_num+=1\n",
    "    \n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    client_loss, client_acc = test(test_loaders[i][1], fed_model, criterion)\n",
    "    final_accs_per_client.append(client_acc.cpu().item())\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    "    loss_per_round=np.array(loss_per_round)\n",
    ")\n",
    "# pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_cifar10_fast_trmean_trim20_resnet20BN.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12960928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
