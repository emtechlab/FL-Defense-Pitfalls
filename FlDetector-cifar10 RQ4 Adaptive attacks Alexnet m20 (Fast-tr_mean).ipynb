{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for CIFAR10 with Fang distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e300ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu105/4347166/ipykernel_1925315/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "import copy\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77aa30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_models import *\n",
    "from cifar10_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "\n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fang_train_data(trainset, num_workers=100, bias=0.5, force=False):\n",
    "    dist_file = 'fang_nworkers%d_bias%.1f.pkl' % (num_workers, bias)\n",
    "    if not force and os.path.exists(dist_file):\n",
    "        print('Loading fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "        return pickle.load(open(dist_file, 'rb'))\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    per_participant_list = defaultdict(list)\n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        per_participant_list[selected_worker].extend([i])\n",
    "    \n",
    "    print('Saving fang distribution for num_workers %d and bias %.1f to memory' % (num_workers, bias))\n",
    "    pickle.dump(per_participant_list, open(dist_file, 'wb'))\n",
    "    return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799ffb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_federated_data(trainset, num_workers, distribution='fang', param=1, force=False):\n",
    "    if distribution == 'fang':\n",
    "        per_participant_list = get_fang_train_data(trainset, num_workers, bias=param, force=force)\n",
    "    elif distribution == 'dirichlet':\n",
    "        per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=param, force=force)\n",
    "\n",
    "    each_worker_idx = [[] for _ in range(num_workers)] \n",
    "    each_worker_te_idx = [[] for _ in range(num_workers)]\n",
    "    np.random.seed(0)\n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(5*w_len/6)\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        each_worker_idx[worker_idx] = w_indices[tr_idx]\n",
    "        each_worker_te_idx[worker_idx] = w_indices[te_idx]\n",
    "    global_test_idx = np.concatenate(each_worker_te_idx)\n",
    "    return each_worker_idx, each_worker_te_idx, global_test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2590db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(dataset, indices, batch_size=32, shuffle=False):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acbb06d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/home/vshejwalkar_umass_edu/data/'\n",
    "# load the train dataset\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_train)\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_train)\n",
    "\n",
    "te_cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_test)\n",
    "te_cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(trainloader, model, model_received, criterion, optimizer, pgd=False, eps=2):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size()[0])\n",
    "        top1.update(prec1.item()/100.0, inputs.size()[0])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if pgd:\n",
    "            curr_model = list(model.parameters())\n",
    "            curr_model_vec = parameters_to_vector(curr_model)\n",
    "            if torch.norm(curr_model_vec - model_received) > eps:\n",
    "                curr_model_vec = eps*(curr_model_vec - model_received)/torch.norm(curr_model_vec - model_received) + model_received\n",
    "                vector_to_parameters(curr_model_vec, curr_model)\n",
    "        \n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def test(testloader, model, criterion):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(testloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        top1.update(prec1/100.0, inputs.size()[0])\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b186277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5380872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def alexnet(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6391e",
   "metadata": {},
   "source": [
    "# FLDetector utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2a716ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def fldetector(old_gradients, user_grads, b=0, hvp=None, agr='tr_mean'):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    if agr == 'average':\n",
    "        agg_grads = torch.mean(user_grads, 0)\n",
    "    elif agr == 'median':\n",
    "        agg_grads = torch.median(user_grads, 0)[0]\n",
    "    elif agr == 'tr_mean':\n",
    "        agg_grads = tr_mean(user_grads, b)\n",
    "    return agg_grads, distance\n",
    "\n",
    "def lbfgs(S_k_list, Y_k_list, v):\n",
    "    curr_S_k = torch.stack(S_k_list).T\n",
    "    curr_Y_k = torch.stack(Y_k_list).T\n",
    "    S_k_time_Y_k = np.dot(curr_S_k.T.cpu().numpy(), curr_Y_k.cpu().numpy())\n",
    "    S_k_time_S_k = np.dot(curr_S_k.T.cpu().numpy(), curr_S_k.cpu().numpy())\n",
    "    R_k = np.triu(S_k_time_Y_k)\n",
    "    L_k = S_k_time_Y_k - R_k\n",
    "    sigma_k = np.dot(Y_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()) / (np.dot(S_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()))\n",
    "    D_k_diag = np.diag(S_k_time_Y_k)\n",
    "    upper_mat = np.concatenate((sigma_k * S_k_time_S_k, L_k), axis=1)\n",
    "    lower_mat = np.concatenate((L_k.T, -np.diag(D_k_diag)), axis=1)\n",
    "    mat = np.concatenate((upper_mat, lower_mat), axis=0)\n",
    "    mat_inv = np.linalg.inv(mat)\n",
    "\n",
    "    approx_prod = sigma_k * v.cpu().numpy()\n",
    "    approx_prod = approx_prod.T\n",
    "    p_mat = np.concatenate((np.dot(curr_S_k.T.cpu().numpy(), sigma_k * v.unsqueeze(0).T.cpu().numpy()), np.dot(curr_Y_k.T.cpu().numpy(), v.unsqueeze(0).T.cpu().numpy())), axis=0)\n",
    "    approx_prod -= np.dot(np.dot(np.concatenate((sigma_k * curr_S_k.cpu().numpy(), curr_Y_k.cpu().numpy()), axis=1), mat_inv), p_mat)\n",
    "\n",
    "    return approx_prod\n",
    "\n",
    "def detection(score, nobyz, nworkers):\n",
    "    estimator = KMeans(n_clusters=2)\n",
    "    estimator.fit(score.reshape(-1, 1))\n",
    "    label_pred = estimator.labels_\n",
    "    if np.mean(score[label_pred==0])<np.mean(score[label_pred==1]):\n",
    "        #0 is the label of malicious clients\n",
    "        label_pred = 1 - label_pred\n",
    "    real_label=np.ones(nworkers)\n",
    "    real_label[:nobyz]=0\n",
    "    acc=len(label_pred[label_pred==real_label])/nworkers\n",
    "    recall=1-np.sum(label_pred[:nobyz])/nobyz\n",
    "    fpr=1-np.sum(label_pred[nobyz:])/(nworkers-nobyz)\n",
    "    fnr=np.sum(label_pred[:nobyz])/nobyz\n",
    "    auc = roc_auc_score(real_label, label_pred)\n",
    "    print(\"acc %0.4f; recall %0.4f; fpr %0.4f; fnr %0.4f; auc %.4f\" % (acc, recall, fpr, fnr, auc))\n",
    "    return acc, fpr, fnr, auc\n",
    "    # print(silhouette_score(score.reshape(-1, 1), label_pred))\n",
    "\n",
    "def detection1(score, nobyz):\n",
    "    nrefs = 10\n",
    "    ks = range(1, 8)\n",
    "    gaps = np.zeros(len(ks))\n",
    "    gapDiff = np.zeros(len(ks) - 1)\n",
    "    sdk = np.zeros(len(ks))\n",
    "    min = np.min(score)\n",
    "    max = np.max(score)\n",
    "    score = (score - min)/(max-min)\n",
    "    for i, k in enumerate(ks):\n",
    "        estimator = KMeans(n_clusters=k)\n",
    "        estimator.fit(score.reshape(-1, 1))\n",
    "        label_pred = estimator.labels_\n",
    "        center = estimator.cluster_centers_\n",
    "        Wk = np.sum([np.square(score[m]-center[label_pred[m]]) for m in range(len(score))])\n",
    "        WkRef = np.zeros(nrefs)\n",
    "        for j in range(nrefs):\n",
    "            rand = np.random.uniform(0, 1, len(score))\n",
    "            estimator = KMeans(n_clusters=k)\n",
    "            estimator.fit(rand.reshape(-1, 1))\n",
    "            label_pred = estimator.labels_\n",
    "            center = estimator.cluster_centers_\n",
    "            WkRef[j] = np.sum([np.square(rand[m]-center[label_pred[m]]) for m in range(len(rand))])\n",
    "        gaps[i] = np.log(np.mean(WkRef)) - np.log(Wk)\n",
    "        sdk[i] = np.sqrt((1.0 + nrefs) / nrefs) * np.std(np.log(WkRef))\n",
    "\n",
    "        if i > 0:\n",
    "            gapDiff[i - 1] = gaps[i - 1] - gaps[i] + sdk[i]\n",
    "    #print(gapDiff)\n",
    "    for i in range(len(gapDiff)):\n",
    "        if gapDiff[i] >= 0:\n",
    "            select_k = i+1\n",
    "            break\n",
    "    if select_k == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        # print('Attack Detected!')\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e25e2ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fang distribution for num_workers 100 and bias 0.5 from memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 100, 10038)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = False\n",
    "\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=100, distribution=distribution, param=param, force=force)\n",
    "\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    train_loaders.append((pos, get_train(all_data, indices, batch_size)))\n",
    "test_loaders = []\n",
    "for pos, indices in enumerate(each_worker_te_idx):\n",
    "    test_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "len(train_loaders), len(test_loaders), len(global_test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddb0d0b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 benign_norm 1.296 val loss 2.308 val acc 0.101 best val_acc 0.101\n",
      "e 1 benign_norm 1.266 val loss 2.315 val acc 0.167 best val_acc 0.167\n",
      "e 2 benign_norm 1.255 val loss 2.320 val acc 0.123 best val_acc 0.167\n",
      "e 3 benign_norm 1.251 val loss 2.321 val acc 0.128 best val_acc 0.167\n",
      "e 4 benign_norm 1.244 val loss 2.311 val acc 0.162 best val_acc 0.167\n",
      "e 5 benign_norm 1.255 val loss 2.285 val acc 0.177 best val_acc 0.177\n",
      "e 6 benign_norm 1.238 val loss 2.246 val acc 0.192 best val_acc 0.192\n",
      "e 7 benign_norm 1.236 val loss 2.210 val acc 0.183 best val_acc 0.192\n",
      "e 8 benign_norm 1.224 val loss 2.182 val acc 0.182 best val_acc 0.192\n",
      "e 9 benign_norm 1.203 val loss 2.157 val acc 0.182 best val_acc 0.192\n",
      "e 10 benign_norm 1.194 val loss 2.061 val acc 0.202 best val_acc 0.202\n",
      "e 11 benign_norm 1.128 val loss 2.047 val acc 0.181 best val_acc 0.202\n",
      "e 12 benign_norm 1.173 val loss 2.039 val acc 0.184 best val_acc 0.202\n",
      "e 13 benign_norm 1.181 val loss 2.015 val acc 0.192 best val_acc 0.202\n",
      "e 14 benign_norm 1.189 val loss 2.011 val acc 0.199 best val_acc 0.202\n",
      "e 15 benign_norm 1.219 val loss 1.981 val acc 0.217 best val_acc 0.217\n",
      "e 16 benign_norm 1.241 val loss 1.935 val acc 0.233 best val_acc 0.233\n",
      "e 17 benign_norm 1.266 val loss 1.917 val acc 0.243 best val_acc 0.243\n",
      "e 18 benign_norm 1.292 val loss 1.868 val acc 0.281 best val_acc 0.281\n",
      "e 19 benign_norm 1.325 val loss 1.847 val acc 0.304 best val_acc 0.304\n",
      "e 20 benign_norm 1.401 val loss 1.773 val acc 0.324 best val_acc 0.324\n",
      "e 21 benign_norm 1.437 val loss 1.736 val acc 0.339 best val_acc 0.339\n",
      "e 22 benign_norm 1.509 val loss 1.684 val acc 0.360 best val_acc 0.360\n",
      "e 23 benign_norm 1.578 val loss 1.693 val acc 0.370 best val_acc 0.370\n",
      "e 24 benign_norm 1.617 val loss 1.646 val acc 0.385 best val_acc 0.385\n",
      "e 25 benign_norm 1.679 val loss 1.628 val acc 0.398 best val_acc 0.398\n",
      "e 26 benign_norm 1.765 val loss 1.622 val acc 0.398 best val_acc 0.398\n",
      "e 27 benign_norm 1.752 val loss 1.607 val acc 0.413 best val_acc 0.413\n",
      "e 28 benign_norm 1.803 val loss 1.609 val acc 0.412 best val_acc 0.413\n",
      "e 29 benign_norm 1.838 val loss 1.557 val acc 0.432 best val_acc 0.432\n",
      "e 30 benign_norm 1.906 val loss 1.590 val acc 0.427 best val_acc 0.432\n",
      "e 31 benign_norm 1.953 val loss 1.558 val acc 0.437 best val_acc 0.437\n",
      "e 32 benign_norm 1.997 val loss 1.558 val acc 0.438 best val_acc 0.438\n",
      "e 33 benign_norm 2.010 val loss 1.525 val acc 0.449 best val_acc 0.449\n",
      "e 34 benign_norm 2.044 val loss 1.544 val acc 0.456 best val_acc 0.456\n",
      "e 35 benign_norm 2.097 val loss 1.517 val acc 0.456 best val_acc 0.456\n",
      "e 36 benign_norm 2.152 val loss 1.506 val acc 0.459 best val_acc 0.459\n",
      "Stop at iteration: 37\n",
      "acc 0.7800; recall 1.0000; fpr 0.2750; fnr 0.0000; auc 0.8625\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "nepochs=200\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nbyz = 20\n",
    "agr = 'tr_mean'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "\n",
    "fed_model = alexnet().cuda()\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "# FLDetector initializations\n",
    "weight_record = []\n",
    "grad_record = []\n",
    "test_grads = []\n",
    "old_grad_list = []\n",
    "malicious_scores = np.zeros((1, num_workers))\n",
    "start_detection_epoch = 10\n",
    "window_size = 10\n",
    "assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "# Adaptive attack initializers\n",
    "good_distance_range = np.zeros((1, nbyz))\n",
    "attack_type = 'NDSS21'\n",
    "dev_type = 'std'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_grads=[]\n",
    "    benign_norm = 0\n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "    weight = model_received\n",
    "\n",
    "    if (epoch_num > start_detection_epoch):\n",
    "        hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "        hvp = np.squeeze(hvp)\n",
    "    else:\n",
    "        hvp = None\n",
    "\n",
    "    good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "    if hvp is not None:\n",
    "        pred_grad = copy.deepcopy(good_old_grads)\n",
    "        distance = []\n",
    "        for i in range(len(good_old_grads)):\n",
    "            pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "        good_distance_range = np.concatenate(\n",
    "            (good_distance_range, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "        \n",
    "    if attack_type != 'none' and (epoch_num < start_detection_epoch):\n",
    "        user_grads = full_trim(user_grads, nbyz)\n",
    "        # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "    elif epoch_num > start_detection_epoch:\n",
    "        if attack_type == 'full_trim':\n",
    "            user_grads = full_trim(user_grads, nbyz)\n",
    "        elif attack_type == 'none':\n",
    "            pass\n",
    "        else:\n",
    "            if attack_type == 'NDSS21':\n",
    "                distance_bound = np.random.choice(np.mean(good_distance_range[-1:], 0))\n",
    "                model_re = torch.mean(good_current_grads, dim=0)\n",
    "                if dev_type == 'unit_vec':\n",
    "                    deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                elif dev_type == 'sign':\n",
    "                    deviation = torch.sign(model_re)\n",
    "                elif dev_type == 'std':\n",
    "                    deviation = torch.std(good_current_grads, 0)\n",
    "                # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_range[-1]))) / torch.norm(deviation))\n",
    "                noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "            elif attack_type == 'mod_trim':\n",
    "                mal_grads= full_trim(user_grads[:nbyz], nbyz)\n",
    "                pass\n",
    "            else:\n",
    "                noise = torch.zeros(hvp.shape).to(device)\n",
    "            for m in range(nbyz):\n",
    "                user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "    agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "    \n",
    "    if distance is not None and epoch_num > (start_detection_epoch - window_size):\n",
    "        malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "    if malicious_scores.shape[0] >= window_size+1:\n",
    "        if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "            print('Stop at iteration:', epoch_num)\n",
    "            detection(np.sum(malicious_scores[-window_size:], axis=0), nbyz, len(user_grads))\n",
    "            break\n",
    "\n",
    "    if epoch_num > (start_detection_epoch - window_size):\n",
    "        weight_record.append(weight - last_weight)\n",
    "        grad_record.append(agg_grads - last_grad)\n",
    "    \n",
    "    if (len(weight_record) > window_size):\n",
    "        del weight_record[0]\n",
    "        del grad_record[0]\n",
    "    \n",
    "    last_weight = weight\n",
    "    last_grad = agg_grads\n",
    "    old_grad_list = user_grads\n",
    "    good_old_grads = good_current_grads\n",
    "    del user_grads\n",
    "\n",
    "    model_received = model_received + global_lr * agg_grads\n",
    "    fed_model = alexnet().cuda()\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    \n",
    "    best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "    accs_per_round.append(val_acc.cpu().item())\n",
    "    loss_per_round.append(val_loss.cpu().item())\n",
    "    \n",
    "    if epoch_num%1==0 or epoch_num==nepochs-1:\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "    if math.isnan(val_loss) or val_loss > 100000:\n",
    "        print('val loss %f... exit'%val_loss)\n",
    "        break\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6fe296a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r 0 nmal 20 | e 0 benign_norm 3.966 val loss 2.323 val acc 0.099 best val_acc 0.099\n",
      "r 0 nmal 20 | e 5 benign_norm 2.372 val loss 2.332 val acc 0.099 best val_acc 0.099\n",
      "r 0 nmal 20 | e 10 benign_norm 2.402 val loss 2.303 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 15 benign_norm 2.277 val loss 2.309 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 20 benign_norm 2.261 val loss 2.311 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 25 benign_norm 2.327 val loss 2.311 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 30 benign_norm 2.251 val loss 2.311 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 35 benign_norm 2.295 val loss 2.311 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 40 benign_norm 2.258 val loss 2.310 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 45 benign_norm 2.269 val loss 2.311 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 50 benign_norm 2.299 val loss 2.310 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 55 benign_norm 2.291 val loss 2.310 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 60 benign_norm 2.298 val loss 2.311 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 65 benign_norm 2.295 val loss 2.311 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 70 benign_norm 2.272 val loss 2.311 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 75 benign_norm 2.267 val loss 2.310 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 80 benign_norm 2.259 val loss 2.309 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 85 benign_norm 2.271 val loss 2.310 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 90 benign_norm 2.301 val loss 2.309 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 95 benign_norm 2.262 val loss 2.308 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 100 benign_norm 2.278 val loss 2.308 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 105 benign_norm 2.263 val loss 2.308 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 110 benign_norm 2.266 val loss 2.308 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 115 benign_norm 2.281 val loss 2.309 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 120 benign_norm 2.299 val loss 2.309 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 125 benign_norm 2.268 val loss 2.308 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 130 benign_norm 2.286 val loss 2.309 val acc 0.100 best val_acc 0.100\n",
      "r 0 nmal 20 | e 135 benign_norm 2.248 val loss 2.311 val acc 0.102 best val_acc 0.102\n",
      "r 0 nmal 20 | e 140 benign_norm 2.292 val loss 2.312 val acc 0.102 best val_acc 0.102\n",
      "r 0 nmal 20 | e 145 benign_norm 2.279 val loss 2.311 val acc 0.102 best val_acc 0.102\n",
      "r 0 nmal 20 | e 150 benign_norm 2.270 val loss 2.313 val acc 0.102 best val_acc 0.102\n",
      "r 0 nmal 20 | e 155 benign_norm 2.290 val loss 2.309 val acc 0.102 best val_acc 0.102\n",
      "r 0 nmal 20 | e 160 benign_norm 2.268 val loss 2.310 val acc 0.100 best val_acc 0.102\n",
      "r 0 nmal 20 | e 165 benign_norm 2.291 val loss 2.310 val acc 0.100 best val_acc 0.102\n",
      "r 0 nmal 20 | e 170 benign_norm 2.296 val loss 2.309 val acc 0.100 best val_acc 0.102\n",
      "r 0 nmal 20 | e 175 benign_norm 2.290 val loss 2.309 val acc 0.100 best val_acc 0.102\n",
      "r 0 nmal 20 | e 180 benign_norm 2.295 val loss 2.309 val acc 0.100 best val_acc 0.102\n",
      "r 0 nmal 20 | e 185 benign_norm 2.264 val loss 2.309 val acc 0.100 best val_acc 0.102\n",
      "r 0 nmal 20 | e 190 benign_norm 2.242 val loss 2.309 val acc 0.100 best val_acc 0.102\n",
      "r 0 nmal 20 | e 195 benign_norm 2.270 val loss 2.309 val acc 0.100 best val_acc 0.102\n",
      "r 0 nmal 20 | e 199 benign_norm 2.253 val loss 2.310 val acc 0.100 best val_acc 0.102\n",
      "r 0 nmal 20 | e 200 benign_norm 2.306 val loss 2.310 val acc 0.100 best val_acc 0.102\n",
      "r 1 nmal 20 | e 0 benign_norm 4.188 val loss 2.327 val acc 0.100 best val_acc 0.100\n",
      "r 1 nmal 20 | e 5 benign_norm 2.383 val loss 2.331 val acc 0.099 best val_acc 0.100\n",
      "r 1 nmal 20 | e 10 benign_norm 2.370 val loss 2.303 val acc 0.099 best val_acc 0.100\n",
      "r 1 nmal 20 | e 15 benign_norm 2.316 val loss 2.314 val acc 0.101 best val_acc 0.101\n",
      "r 1 nmal 20 | e 20 benign_norm 2.310 val loss 2.314 val acc 0.101 best val_acc 0.101\n",
      "r 1 nmal 20 | e 25 benign_norm 2.312 val loss 2.313 val acc 0.101 best val_acc 0.101\n",
      "r 1 nmal 20 | e 30 benign_norm 2.313 val loss 2.313 val acc 0.101 best val_acc 0.101\n",
      "r 1 nmal 20 | e 35 benign_norm 2.312 val loss 2.313 val acc 0.101 best val_acc 0.101\n",
      "r 1 nmal 20 | e 40 benign_norm 2.290 val loss 2.313 val acc 0.101 best val_acc 0.101\n",
      "r 1 nmal 20 | e 45 benign_norm 2.292 val loss 2.312 val acc 0.101 best val_acc 0.101\n",
      "r 1 nmal 20 | e 50 benign_norm 2.282 val loss 2.313 val acc 0.101 best val_acc 0.101\n",
      "r 1 nmal 20 | e 55 benign_norm 2.327 val loss 2.313 val acc 0.101 best val_acc 0.101\n",
      "r 1 nmal 20 | e 60 benign_norm 2.332 val loss 2.314 val acc 0.101 best val_acc 0.101\n",
      "r 1 nmal 20 | e 65 benign_norm 2.297 val loss 2.315 val acc 0.101 best val_acc 0.101\n",
      "r 1 nmal 20 | e 70 benign_norm 2.316 val loss 2.314 val acc 0.103 best val_acc 0.103\n",
      "r 1 nmal 20 | e 75 benign_norm 2.324 val loss 2.314 val acc 0.103 best val_acc 0.103\n",
      "r 1 nmal 20 | e 80 benign_norm 2.290 val loss 2.313 val acc 0.103 best val_acc 0.103\n",
      "r 1 nmal 20 | e 85 benign_norm 2.300 val loss 2.314 val acc 0.103 best val_acc 0.103\n",
      "r 1 nmal 20 | e 90 benign_norm 2.290 val loss 2.314 val acc 0.101 best val_acc 0.103\n",
      "r 1 nmal 20 | e 95 benign_norm 2.305 val loss 2.314 val acc 0.101 best val_acc 0.103\n",
      "r 1 nmal 20 | e 100 benign_norm 2.327 val loss 2.314 val acc 0.101 best val_acc 0.103\n",
      "r 1 nmal 20 | e 105 benign_norm 2.286 val loss 2.314 val acc 0.101 best val_acc 0.103\n",
      "r 1 nmal 20 | e 110 benign_norm 2.302 val loss 2.316 val acc 0.101 best val_acc 0.103\n",
      "r 1 nmal 20 | e 115 benign_norm 2.305 val loss 2.315 val acc 0.101 best val_acc 0.103\n",
      "r 1 nmal 20 | e 120 benign_norm 2.341 val loss 2.316 val acc 0.101 best val_acc 0.103\n",
      "r 1 nmal 20 | e 125 benign_norm 2.321 val loss 2.320 val acc 0.101 best val_acc 0.103\n",
      "r 1 nmal 20 | e 130 benign_norm 2.356 val loss 2.319 val acc 0.101 best val_acc 0.103\n",
      "r 1 nmal 20 | e 135 benign_norm 2.313 val loss 2.320 val acc 0.101 best val_acc 0.103\n",
      "!!!! Stop at iteration: 136\n",
      "r 1 nmal 20 | e 136 benign_norm 2.304 val loss 2.320 val acc 0.101 best val_acc 0.103\n",
      "acc 0.7500; recall 1.0000; fpr 0.3125; fnr 0.0000; auc 0.8438\n",
      "r 2 nmal 20 | e 0 benign_norm 4.115 val loss 2.328 val acc 0.100 best val_acc 0.100\n",
      "r 2 nmal 20 | e 5 benign_norm 2.379 val loss 2.333 val acc 0.099 best val_acc 0.100\n",
      "r 2 nmal 20 | e 10 benign_norm 2.385 val loss 2.303 val acc 0.100 best val_acc 0.103\n",
      "r 2 nmal 20 | e 15 benign_norm 2.316 val loss 2.317 val acc 0.102 best val_acc 0.103\n",
      "r 2 nmal 20 | e 20 benign_norm 2.317 val loss 2.317 val acc 0.102 best val_acc 0.103\n",
      "r 2 nmal 20 | e 25 benign_norm 2.293 val loss 2.318 val acc 0.102 best val_acc 0.103\n",
      "r 2 nmal 20 | e 30 benign_norm 2.252 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 35 benign_norm 2.251 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 40 benign_norm 2.264 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 45 benign_norm 2.267 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 50 benign_norm 2.265 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 55 benign_norm 2.244 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 60 benign_norm 2.250 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 65 benign_norm 2.264 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 70 benign_norm 2.263 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 75 benign_norm 2.280 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 80 benign_norm 2.273 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 85 benign_norm 2.257 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 90 benign_norm 2.245 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 95 benign_norm 2.284 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 100 benign_norm 2.272 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 105 benign_norm 2.276 val loss 2.305 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 110 benign_norm 2.280 val loss 2.305 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 115 benign_norm 2.278 val loss 2.305 val acc 0.099 best val_acc 0.103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r 2 nmal 20 | e 120 benign_norm 2.292 val loss 2.305 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 125 benign_norm 2.283 val loss 2.306 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 130 benign_norm 2.273 val loss 2.305 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 135 benign_norm 2.260 val loss 2.305 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 140 benign_norm 2.263 val loss 2.305 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 145 benign_norm 2.264 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 150 benign_norm 2.260 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 155 benign_norm 2.247 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 160 benign_norm 2.281 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 165 benign_norm 2.268 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 170 benign_norm 2.281 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 175 benign_norm 2.254 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 180 benign_norm 2.291 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 185 benign_norm 2.283 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 190 benign_norm 2.274 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 195 benign_norm 2.263 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 199 benign_norm 2.287 val loss 2.304 val acc 0.099 best val_acc 0.103\n",
      "r 2 nmal 20 | e 200 benign_norm 2.250 val loss 2.304 val acc 0.099 best val_acc 0.103\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_malicious = [20]\n",
    "n_runs = 3\n",
    "results={}\n",
    "for n_mal in n_malicious:\n",
    "    results[n_mal] = {'acc': [], 'fpr': [], 'fnr': [], 'auc': [], 'stop_e': []}\n",
    "    for run in range(n_runs):\n",
    "        torch.cuda.empty_cache()\n",
    "        nepochs=200\n",
    "        local_epochs = 2\n",
    "        batch_size = 16\n",
    "        num_workers = 100\n",
    "        local_lr = 0.1\n",
    "        global_lr = 1\n",
    "        agr = 'tr_mean'\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = alexnet().cuda()\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        best_accs_per_round = []\n",
    "        accs_per_round = []\n",
    "        loss_per_round = []\n",
    "        home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "        # FLDetector initializations\n",
    "        weight_record = []\n",
    "        grad_record = []\n",
    "        test_grads = []\n",
    "        old_grad_list = []\n",
    "        malicious_scores = np.zeros((1, num_workers-(20-nbyz)))\n",
    "        start_detection_epoch = 10\n",
    "        window_size = 10\n",
    "        assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "        # Adaptive attack initializers\n",
    "        good_distance_range = np.zeros((1, nbyz))\n",
    "        attack_type = 'NDSS21'\n",
    "        dev_type = 'sign'\n",
    "        nbyz = n_mal\n",
    "        \n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(20-nbyz, num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_grads=[]\n",
    "            benign_norm = 0\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "            weight = model_received\n",
    "\n",
    "            if (epoch_num > start_detection_epoch):\n",
    "                hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "                hvp = np.squeeze(hvp)\n",
    "            else:\n",
    "                hvp = None\n",
    "\n",
    "            good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "            if hvp is not None:\n",
    "                pred_grad = copy.deepcopy(good_old_grads)\n",
    "                distance = []\n",
    "                for i in range(len(good_old_grads)):\n",
    "                    pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "                good_distance_range = np.concatenate(\n",
    "                    (good_distance_range, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "            if attack_type != 'none' and (epoch_num < start_detection_epoch):\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "                # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "            elif epoch_num > start_detection_epoch:\n",
    "                if attack_type == 'full_trim':\n",
    "                    user_grads = full_trim(user_grads, nbyz)\n",
    "                elif attack_type == 'none':\n",
    "                    pass\n",
    "                else:\n",
    "                    if attack_type == 'NDSS21':\n",
    "                        distance_bound = np.random.choice(np.mean(good_distance_range[-1:], 0))\n",
    "                        model_re = torch.mean(good_current_grads, dim=0)\n",
    "                        if dev_type == 'unit_vec':\n",
    "                            deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                        elif dev_type == 'sign':\n",
    "                            deviation = torch.sign(model_re)\n",
    "                        elif dev_type == 'std':\n",
    "                            deviation = torch.std(good_current_grads, 0)\n",
    "                        # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_range[-1]))) / torch.norm(deviation))\n",
    "                        noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                    elif attack_type == 'mod_trim':\n",
    "                        mal_grads= full_trim(user_grads[:nbyz], nbyz)\n",
    "                        pass\n",
    "                    else:\n",
    "                        noise = torch.zeros(hvp.shape).to(device)\n",
    "                    for m in range(nbyz):\n",
    "                        user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "            agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "            if distance is not None and epoch_num > (start_detection_epoch - window_size):\n",
    "                malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "            if malicious_scores.shape[0] >= window_size+1:\n",
    "                if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                    print('!!!! Stop at iteration:', epoch_num)\n",
    "                    print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "                    acc, fpr, fnr, auc = detection(\n",
    "                        np.sum(malicious_scores[-window_size:], axis=0), nbyz, len(user_grads))\n",
    "                    results[n_mal]['acc'].append(acc)\n",
    "                    results[n_mal]['fpr'].append(fpr)\n",
    "                    results[n_mal]['fnr'].append(fnr)\n",
    "                    results[n_mal]['auc'].append(auc)\n",
    "                    results[n_mal]['stop_e'].append(epoch_num)\n",
    "                    break\n",
    "                    break\n",
    "\n",
    "            if epoch_num > (start_detection_epoch - window_size):\n",
    "                weight_record.append(weight - last_weight)\n",
    "                grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "            if (len(weight_record) > window_size):\n",
    "                del weight_record[0]\n",
    "                del grad_record[0]\n",
    "\n",
    "            last_weight = weight\n",
    "            last_grad = agg_grads\n",
    "            old_grad_list = user_grads\n",
    "            good_old_grads = good_current_grads\n",
    "            del user_grads\n",
    "\n",
    "            model_received = model_received + global_lr * agg_grads\n",
    "            fed_model = alexnet().cuda()\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "            best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "            accs_per_round.append(val_acc.cpu().item())\n",
    "            loss_per_round.append(val_loss.cpu().item())\n",
    "\n",
    "            if epoch_num%5==0 or epoch_num==nepochs-1:\n",
    "                print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "            if math.isnan(val_loss) or val_loss > 100000:\n",
    "                print('val loss %f... exit'%val_loss)\n",
    "                break\n",
    "            epoch_num+=1\n",
    "\n",
    "        final_accs_per_client=[]\n",
    "        for i in range(num_workers):\n",
    "            client_loss, client_acc = test(test_loaders[i][1], fed_model, criterion)\n",
    "            final_accs_per_client.append(client_acc.cpu().item())\n",
    "        all_results = collections.OrderedDict(\n",
    "            final_accs_per_client=np.array(final_accs_per_client),\n",
    "            accs_per_round=np.array(accs_per_round),\n",
    "            best_accs_per_round=np.array(best_accs_per_round),\n",
    "            loss_per_round=np.array(loss_per_round),\n",
    "            results=results\n",
    "        )\n",
    "        # pickle.dump(all_results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq4_adaptive_cifar10_fast_trmean_NDSS21_std_m%d_r%d.pkl' % (nbyz, run)), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438a632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
