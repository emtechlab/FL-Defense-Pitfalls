{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for MNIST with Dirichlet distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a52c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_data = torch.utils.data.DataLoader(trainset, batch_size=60000, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_data = torch.utils.data.DataLoader(testset, batch_size=5000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "            \n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_train_data(trainset, num_workers=100, bias=0.5):\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        \n",
    "        if not len(each_worker_data[selected_worker]):\n",
    "            each_worker_data[selected_worker] = x[None, :]\n",
    "        else:\n",
    "            each_worker_data[selected_worker]= torch.concat((each_worker_data[selected_worker], x[None, :]))\n",
    "        \n",
    "        each_worker_label[selected_worker].append(y)\n",
    "    for i in range(num_workers):\n",
    "        each_worker_label[i] = torch.Tensor(np.array(each_worker_label[i])).long()\n",
    "    return each_worker_data, each_worker_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f36f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_data_dirichlet(trainset, num_workers, alpha=1, force=False):\n",
    "    per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=alpha, force=force)\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        for idx in per_participant_list[worker_idx]:\n",
    "            each_worker_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_data[worker_idx] = torch.stack(each_worker_data[worker_idx])\n",
    "        each_worker_label[worker_idx] = torch.Tensor(each_worker_label[worker_idx]).long()\n",
    "    return each_worker_data, each_worker_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "585ee015",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating participant indices for alpha 0.1\n"
     ]
    }
   ],
   "source": [
    "num_workers = 100\n",
    "distribution='dirichlet'\n",
    "if distribution=='bias':\n",
    "    each_worker_data, each_worker_label = get_client_train_data(trainset, num_workers=100, bias=0.5)\n",
    "elif distribution == 'dirichlet':\n",
    "    alpha = .1\n",
    "    force = True\n",
    "    each_worker_data, each_worker_label = get_client_data_dirichlet(trainset, num_workers, alpha=alpha, force=force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934d4b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mnist'\n",
    "bias = 0.1\n",
    "net = 'cnn'\n",
    "batch_size = 32\n",
    "# lr = 0.0002\n",
    "# lr = 1e-3\n",
    "lr = 0.01\n",
    "nworkers = 100\n",
    "nepochs = 100\n",
    "gpu = 3\n",
    "seed = 41\n",
    "nbyz = 84\n",
    "byz_type = 'full_trim'\n",
    "aggregation = 'trim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84b05be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbfgs(S_k_list, Y_k_list, v):\n",
    "    curr_S_k = torch.stack(S_k_list).T\n",
    "    curr_Y_k = torch.stack(Y_k_list).T\n",
    "    S_k_time_Y_k = np.dot(curr_S_k.T.cpu().numpy(), curr_Y_k.cpu().numpy())\n",
    "    S_k_time_S_k = np.dot(curr_S_k.T.cpu().numpy(), curr_S_k.cpu().numpy())\n",
    "    R_k = np.triu(S_k_time_Y_k)\n",
    "    L_k = S_k_time_Y_k - R_k\n",
    "    sigma_k = np.dot(Y_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()) / (np.dot(S_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()))\n",
    "    D_k_diag = np.diag(S_k_time_Y_k)\n",
    "    upper_mat = np.concatenate((sigma_k * S_k_time_S_k, L_k), axis=1)\n",
    "    lower_mat = np.concatenate((L_k.T, -np.diag(D_k_diag)), axis=1)\n",
    "    mat = np.concatenate((upper_mat, lower_mat), axis=0)\n",
    "    mat_inv = np.linalg.inv(mat)\n",
    "\n",
    "    approx_prod = sigma_k * v.cpu().numpy()\n",
    "    approx_prod = approx_prod.T\n",
    "    p_mat = np.concatenate((np.dot(curr_S_k.T.cpu().numpy(), sigma_k * v.unsqueeze(0).T.cpu().numpy()), np.dot(curr_Y_k.T.cpu().numpy(), v.unsqueeze(0).T.cpu().numpy())), axis=0)\n",
    "    approx_prod -= np.dot(np.dot(np.concatenate((sigma_k * curr_S_k.cpu().numpy(), curr_Y_k.cpu().numpy()), axis=1), mat_inv), p_mat)\n",
    "\n",
    "    return approx_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(20):\n",
    "        # apply attack to compromised worker devices with randomness\n",
    "        ##random_12 = 1. + np.random.uniform(size=vi_shape)\n",
    "        ##random_12 = torch.Tensor(random_12).float().cuda()\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc5c00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_mean(old_gradients, user_grads, b=0, hvp=None):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    agg_grads = torch.mean(user_grads,dim=0)\n",
    "    \n",
    "    return agg_grads, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffa6d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(old_gradients, user_grads, b=0, hvp=None):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    agg_grads = torch.median(user_grads, 0)[0]\n",
    "    \n",
    "    return agg_grads, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72fe92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimmed_mean(old_gradients, user_grads, b=0, hvp=None):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    agg_grads = tr_mean(user_grads, 20)\n",
    "    \n",
    "    return agg_grads, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67adc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(score, nobyz):\n",
    "    estimator = KMeans(n_clusters=2)\n",
    "    estimator.fit(score.reshape(-1, 1))\n",
    "    label_pred = estimator.labels_\n",
    "    if np.mean(score[label_pred==0])<np.mean(score[label_pred==1]):\n",
    "        #0 is the label of malicious clients\n",
    "        label_pred = 1 - label_pred\n",
    "    real_label=np.ones(100)\n",
    "    real_label[:nobyz]=0\n",
    "    acc=len(label_pred[label_pred==real_label])/100\n",
    "    recall=1-np.sum(label_pred[:nobyz])/nobyz\n",
    "    fpr=1-np.sum(label_pred[nobyz:])/(100-nobyz)\n",
    "    fnr=np.sum(label_pred[:nobyz])/nobyz\n",
    "    print(\"acc %0.4f; recall %0.4f; fpr %0.4f; fnr %0.4f;\" % (acc, recall, fpr, fnr))\n",
    "    print(silhouette_score(score.reshape(-1, 1), label_pred))\n",
    "\n",
    "def detection1(score, nobyz):\n",
    "    nrefs = 10\n",
    "    ks = range(1, 8)\n",
    "    gaps = np.zeros(len(ks))\n",
    "    gapDiff = np.zeros(len(ks) - 1)\n",
    "    sdk = np.zeros(len(ks))\n",
    "    min = np.min(score)\n",
    "    max = np.max(score)\n",
    "    score = (score - min)/(max-min)\n",
    "    for i, k in enumerate(ks):\n",
    "        estimator = KMeans(n_clusters=k)\n",
    "        estimator.fit(score.reshape(-1, 1))\n",
    "        label_pred = estimator.labels_\n",
    "        center = estimator.cluster_centers_\n",
    "        Wk = np.sum([np.square(score[m]-center[label_pred[m]]) for m in range(len(score))])\n",
    "        WkRef = np.zeros(nrefs)\n",
    "        for j in range(nrefs):\n",
    "            rand = np.random.uniform(0, 1, len(score))\n",
    "            estimator = KMeans(n_clusters=k)\n",
    "            estimator.fit(rand.reshape(-1, 1))\n",
    "            label_pred = estimator.labels_\n",
    "            center = estimator.cluster_centers_\n",
    "            WkRef[j] = np.sum([np.square(rand[m]-center[label_pred[m]]) for m in range(len(rand))])\n",
    "        gaps[i] = np.log(np.mean(WkRef)) - np.log(Wk)\n",
    "        sdk[i] = np.sqrt((1.0 + nrefs) / nrefs) * np.std(np.log(WkRef))\n",
    "\n",
    "        if i > 0:\n",
    "            gapDiff[i - 1] = gaps[i - 1] - gaps[i] + sdk[i]\n",
    "    #print(gapDiff)\n",
    "    for i in range(len(gapDiff)):\n",
    "        if gapDiff[i] >= 0:\n",
    "            select_k = i+1\n",
    "            break\n",
    "    if select_k == 1:\n",
    "        # print('No attack detected!')\n",
    "        return 0\n",
    "    else:\n",
    "        print('Attack Detected!')\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72ced92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = nworkers\n",
    "lr = lr\n",
    "epochs = nepochs\n",
    "grad_list = []\n",
    "old_grad_list = []\n",
    "weight_record = []\n",
    "grad_record = []\n",
    "train_acc_list = []\n",
    "distance1 = []\n",
    "distance2 = []\n",
    "auc_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8647b414",
   "metadata": {},
   "source": [
    "# Baseline FLDetector with various data distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "832a19df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.056\n",
      "1 0.0862\n",
      "2 0.1001\n",
      "3 0.101\n",
      "4 0.101\n",
      "5 0.101\n",
      "6 0.101\n",
      "7 0.101\n",
      "8 0.101\n",
      "9 0.0937\n",
      "10 0.0696\n",
      "11 0.098\n",
      "12 0.0784\n",
      "13 0.099\n",
      "14 0.0958\n",
      "15 0.0892\n",
      "16 0.1009\n",
      "17 0.1009\n",
      "18 0.1009\n",
      "19 0.098\n",
      "Attack Detected!\n",
      "Stop at iteration: 20\n",
      "acc 0.9000; recall 1.0000; fpr 0.1250; fnr 0.0000;\n",
      "0.864210377672373\n"
     ]
    }
   ],
   "source": [
    "num_workers = 100\n",
    "distribution='bias'\n",
    "param = 0.5\n",
    "if distribution=='bias':\n",
    "    each_worker_data, each_worker_label = get_client_train_data(trainset, num_workers=num_workers, bias=param)\n",
    "elif distribution == 'dirichlet':\n",
    "    force = True\n",
    "    each_worker_data, each_worker_label = get_client_data_dirichlet(trainset, num_workers, alpha=param, force=force)\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "weight_record = []\n",
    "grad_record = []\n",
    "test_grads = []\n",
    "malicious_scores = np.zeros((1, num_workers))\n",
    "attack_type='LIE'\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = cnn().to(device)\n",
    "lr = 0.15\n",
    "start_detection_epoch = 10\n",
    "window_size = 10\n",
    "assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "for e in range(100):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr*(0.96**e))\n",
    "    user_grads = []\n",
    "\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, each_worker_label[i].cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "    tmp = []\n",
    "    for param in net.parameters():\n",
    "        tmp = param.data.view(-1) if not len(tmp) else torch.cat((tmp, param.data.view(-1)))\n",
    "    weight = tmp    \n",
    "\n",
    "    if (e > start_detection_epoch):\n",
    "        hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "        hvp = np.squeeze(hvp)\n",
    "    else:\n",
    "        hvp = None\n",
    "\n",
    "    if(e < start_detection_epoch):\n",
    "        user_grads = full_trim(user_grads, 20)\n",
    "    elif e > start_detection_epoch:\n",
    "        if attack_type == 'LIE':\n",
    "            z = 0.3*(1.1**(e-20))\n",
    "            noise_avg = torch.mean(user_grads[:20], dim=0)\n",
    "            noise_std = torch.std(user_grads[:20], dim=0)\n",
    "            noise = noise_avg + z * noise_std\n",
    "        elif attack_type == 'NDSS21':\n",
    "            pass\n",
    "        else:\n",
    "            noise = torch.zeros(hvp.shape).to(device)\n",
    "        for m in range(20):\n",
    "            user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "            \n",
    "#     agg_grads, distance = simple_mean(old_grad_list, user_grads, 0, hvp)\n",
    "    agg_grads, distance = trimmed_mean(old_grad_list, user_grads, 20, hvp)\n",
    "    \n",
    "    if distance is not None and e > (start_detection_epoch - window_size):\n",
    "        # print('==>', e, malicious_scores.shape)\n",
    "        malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "    if malicious_scores.shape[0] >= 11:\n",
    "        # print('performing detection at epoch %d' % e)\n",
    "        if detection1(np.sum(malicious_scores[-10:], axis=0), 20):\n",
    "            print('Stop at iteration:', e)\n",
    "            detection(np.sum(malicious_scores[-10:], axis=0), 20)\n",
    "            break\n",
    "\n",
    "    if e > (start_detection_epoch - window_size):\n",
    "        weight_record.append(weight - last_weight)\n",
    "        grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "    if (len(weight_record) > 10):\n",
    "        del weight_record[0]\n",
    "        del grad_record[0]\n",
    "\n",
    "    last_weight = weight\n",
    "    last_grad = agg_grads\n",
    "    old_grad_list = user_grads\n",
    "  \n",
    "    del user_grads\n",
    "    \n",
    "    start_idx=0\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "\n",
    "    model_grads=[]\n",
    "\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_data):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(e,correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dea71d3",
   "metadata": {},
   "source": [
    "# Adaptive attack against FLDetector based on simple noise addition\n",
    "\n",
    "## Method: I added noise in the *little-is-enough attack fashion*\n",
    "\n",
    "## Observation: FLdetector does not detect the attack and the model does not train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a33ec087",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0147\n",
      "1 0.0807\n",
      "2 0.1035\n",
      "3 0.1294\n",
      "4 0.1663\n",
      "5 0.1848\n",
      "6 0.1858\n",
      "7 0.163\n",
      "8 0.1281\n",
      "9 0.2149\n",
      "10 0.1629\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 0.7250; Detection AUC: 0.0000\n",
      "==> 11 (1, 100)\n",
      "11 0.098\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.9625\n",
      "==> 12 (2, 100)\n",
      "12 0.1689\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.0000\n",
      "==> 13 (3, 100)\n",
      "13 0.098\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.0000\n",
      "==> 14 (4, 100)\n",
      "14 0.1062\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 0.4500; Detection AUC: 0.0000\n",
      "==> 15 (5, 100)\n",
      "15 0.135\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.0000\n",
      "==> 16 (6, 100)\n",
      "16 0.098\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 0.9875; Detection AUC: 0.0000\n",
      "==> 17 (7, 100)\n",
      "17 0.098\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 0.6875; Detection AUC: 0.5875\n",
      "==> 18 (8, 100)\n",
      "18 0.098\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 1.0000\n",
      "==> 19 (9, 100)\n",
      "19 0.098\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 0.5000; Detection AUC: 0.1375\n",
      "==> 20 (10, 100)\n",
      "performing detection at epoch 20\n",
      "No attack detected!\n",
      "20 0.098\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.9750\n",
      "==> 21 (11, 100)\n",
      "performing detection at epoch 21\n",
      "No attack detected!\n",
      "21 0.098\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.0000\n",
      "==> 22 (12, 100)\n",
      "performing detection at epoch 22\n",
      "No attack detected!\n",
      "22 0.098\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 1.0000\n",
      "==> 23 (13, 100)\n",
      "performing detection at epoch 23\n",
      "No attack detected!\n",
      "23 0.098\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.0000\n",
      "==> 24 (14, 100)\n",
      "performing detection at epoch 24\n",
      "No attack detected!\n",
      "24 0.098\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.0000\n",
      "==> 25 (15, 100)\n",
      "performing detection at epoch 25\n",
      "No attack detected!\n",
      "25 0.098\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 0.7375; Detection AUC: 0.7375\n",
      "==> 26 (16, 100)\n",
      "performing detection at epoch 26\n",
      "No attack detected!\n",
      "26 0.0958\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.0000\n",
      "==> 27 (17, 100)\n",
      "performing detection at epoch 27\n",
      "No attack detected!\n",
      "27 0.098\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.9750\n",
      "==> 28 (18, 100)\n",
      "performing detection at epoch 28\n",
      "No attack detected!\n",
      "28 0.1135\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 1.0000; Detection AUC: 0.0000\n",
      "==> 29 (19, 100)\n",
      "performing detection at epoch 29\n",
      "No attack detected!\n",
      "29 0.098\n",
      "LIE\n",
      "<class 'torch.Tensor'>\n",
      "Detection AUC: 0.6500; Detection AUC: 0.6500\n",
      "==> 30 (20, 100)\n",
      "performing detection at epoch 30\n",
      "Attack Detected!\n",
      "Stop at iteration: 30\n",
      "acc 0.0000; recall 0.0000; fpr 1.0000; fnr 1.0000;\n",
      "0.8973881561439662\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "weight_record = []\n",
    "grad_record = []\n",
    "test_grads = []\n",
    "malicious_scores = np.zeros((1, num_workers))\n",
    "attack_type = 'LIE'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = cnn().to(device)\n",
    "lr = 0.15\n",
    "start_detection_epoch = 10\n",
    "window_size = 10\n",
    "assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "for e in range(100):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr*(0.96**e))\n",
    "    user_grads = []\n",
    "\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, each_worker_label[i].cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "    tmp = []\n",
    "    for param in net.parameters():\n",
    "        tmp = param.data.view(-1) if not len(tmp) else torch.cat((tmp, param.data.view(-1)))\n",
    "    weight = tmp    \n",
    "\n",
    "    if (e > start_detection_epoch):\n",
    "        hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "        hvp = np.squeeze(hvp)\n",
    "    else:\n",
    "        hvp = None\n",
    "\n",
    "    if(e < start_detection_epoch):\n",
    "        user_grads = full_trim(user_grads, 20)\n",
    "    elif e > start_detection_epoch:\n",
    "        if attack_type == 'LIE':\n",
    "            print(\"LIE\")\n",
    "            z = 0.3*(1.1**(e-20))\n",
    "            noise_avg = torch.mean(user_grads[:20], dim=0)\n",
    "            noise_std = torch.std(user_grads[:20], dim=0)\n",
    "            noise = noise_avg + z * noise_std\n",
    "        elif attack_type == 'NDSS21':\n",
    "            pass\n",
    "        else:\n",
    "            noise = torch.zeros(hvp.shape).to(device)\n",
    "        for m in range(20):\n",
    "            user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "#             user_grads[m] = torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "    agg_grads, distance = simple_mean(old_grad_list, user_grads, 20, hvp)\n",
    "#     agg_grads, distance = trimmed_mean(old_grad_list, user_grads, 20, hvp)\n",
    "    \n",
    "    if distance is not None and e > (start_detection_epoch - window_size):\n",
    "        print('==>', e, malicious_scores.shape)\n",
    "        malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "    if malicious_scores.shape[0] >= 11:\n",
    "        print('performing detection at epoch %d' % e)\n",
    "        if detection1(np.sum(malicious_scores[-10:], axis=0), 20):\n",
    "            print('Stop at iteration:', e)\n",
    "            detection(np.sum(malicious_scores[-10:], axis=0), 20)\n",
    "            break\n",
    "\n",
    "#     agg_grads = tr_mean(user_grads, 20)\n",
    "#     agg_grads=torch.median(user_grads,dim=0)[0]\n",
    "#     agg_grads=torch.mean(user_grads,dim=0)\n",
    "\n",
    "    if e > (start_detection_epoch - window_size):\n",
    "        weight_record.append(weight - last_weight)\n",
    "        grad_record.append(agg_grads - last_grad)\n",
    "    \n",
    "    if (len(weight_record) > 10):\n",
    "        del weight_record[0]\n",
    "        del grad_record[0]\n",
    "    \n",
    "    last_weight = weight\n",
    "    last_grad = agg_grads\n",
    "    old_grad_list = user_grads\n",
    "  \n",
    "    del user_grads\n",
    "    \n",
    "    start_idx=0\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "\n",
    "    model_grads=[]\n",
    "\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_data):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(e,correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd4c77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85151378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1078\n",
      "1 0.1097\n",
      "2 0.1114\n",
      "3 0.1129\n",
      "4 0.1142\n",
      "5 0.1121\n",
      "6 0.1067\n",
      "7 0.1047\n",
      "8 0.3145\n",
      "9 0.1795\n",
      "10 0.1304\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 0.6500\n",
      "==> 11 (1, 100)\n",
      "11 0.101\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 0.0000\n",
      "==> 12 (2, 100)\n",
      "12 0.1032\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 0.0000\n",
      "==> 13 (3, 100)\n",
      "13 0.1365\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 0.0000\n",
      "==> 14 (4, 100)\n",
      "14 0.2116\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 1.0000\n",
      "==> 15 (5, 100)\n",
      "15 0.2325\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 0.9750\n",
      "==> 16 (6, 100)\n",
      "16 0.2072\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 0.9750\n",
      "==> 17 (7, 100)\n",
      "17 0.1739\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 0.7000\n",
      "==> 18 (8, 100)\n",
      "18 0.1208\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 0.9000\n",
      "==> 19 (9, 100)\n",
      "19 0.1597\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 1.0000\n",
      "==> 20 (10, 100)\n",
      "performing detection at epoch 20\n",
      "No attack detected!\n",
      "20 0.2201\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 1.0000\n",
      "==> 21 (11, 100)\n",
      "performing detection at epoch 21\n",
      "No attack detected!\n",
      "21 0.163\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 0.0000\n",
      "==> 22 (12, 100)\n",
      "performing detection at epoch 22\n",
      "No attack detected!\n",
      "22 0.2821\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 0.0000\n",
      "==> 23 (13, 100)\n",
      "performing detection at epoch 23\n",
      "No attack detected!\n",
      "23 0.1028\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 0.8875\n",
      "==> 24 (14, 100)\n",
      "performing detection at epoch 24\n",
      "No attack detected!\n",
      "24 0.2325\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 0.6500\n",
      "==> 25 (15, 100)\n",
      "performing detection at epoch 25\n",
      "No attack detected!\n",
      "25 0.1847\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 0.0000\n",
      "==> 26 (16, 100)\n",
      "performing detection at epoch 26\n",
      "No attack detected!\n",
      "26 0.297\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 1.0000\n",
      "==> 27 (17, 100)\n",
      "performing detection at epoch 27\n",
      "No attack detected!\n",
      "27 0.1028\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 1.0000\n",
      "==> 28 (18, 100)\n",
      "performing detection at epoch 28\n",
      "No attack detected!\n",
      "28 0.2792\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 0.9125\n",
      "==> 29 (19, 100)\n",
      "performing detection at epoch 29\n",
      "No attack detected!\n",
      "29 0.1831\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 1.0000\n",
      "==> 30 (20, 100)\n",
      "performing detection at epoch 30\n",
      "No attack detected!\n",
      "30 0.3326\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 1.0000\n",
      "==> 31 (21, 100)\n",
      "performing detection at epoch 31\n",
      "No attack detected!\n",
      "31 0.1028\n",
      "LIE\n",
      "Detection AUC: 0.0000; Detection AUC: 1.0000\n",
      "==> 32 (22, 100)\n",
      "performing detection at epoch 32\n",
      "Attack Detected!\n",
      "Stop at iteration: 32\n",
      "acc 0.9600; recall 1.0000; fpr 0.0500; fnr 0.0000;\n",
      "0.7986261939217887\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "weight_record = []\n",
    "grad_record = []\n",
    "test_grads = []\n",
    "malicious_scores = np.zeros((1, num_workers))\n",
    "attack_type = 'LIE'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = cnn().to(device)\n",
    "lr = 0.15\n",
    "start_detection_epoch = 10\n",
    "window_size = 10\n",
    "assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "for e in range(100):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr*(0.96**e))\n",
    "    user_grads = []\n",
    "\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, each_worker_label[i].cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "    tmp = []\n",
    "    for param in net.parameters():\n",
    "        tmp = param.data.view(-1) if not len(tmp) else torch.cat((tmp, param.data.view(-1)))\n",
    "    weight = tmp    \n",
    "\n",
    "    if (e > start_detection_epoch):\n",
    "        hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "        hvp = np.squeeze(hvp)\n",
    "    else:\n",
    "        hvp = None\n",
    "\n",
    "    if(e < start_detection_epoch):\n",
    "        user_grads = full_trim(user_grads, 20)\n",
    "    elif e > start_detection_epoch:\n",
    "        if attack_type == 'LIE':\n",
    "            print(\"LIE\")\n",
    "            z = 0.3*(1.03**(e))\n",
    "            noise_avg = torch.mean(user_grads[:20], dim=0)\n",
    "            noise_std = torch.std(user_grads[:20], dim=0)\n",
    "            noise = noise_avg + z * noise_std\n",
    "        elif attack_type == 'NDSS21':\n",
    "            pass\n",
    "        else:\n",
    "            noise = torch.zeros(hvp.shape).to(device)\n",
    "        for m in range(20):\n",
    "            user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "#             user_grads[m] = torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "#     agg_grads, distance = simple_mean(old_grad_list, user_grads, 20, hvp)\n",
    "    agg_grads, distance = trimmed_mean(old_grad_list, user_grads, 20, hvp)\n",
    "    \n",
    "    if distance is not None and e > (start_detection_epoch - window_size):\n",
    "        print('==>', e, malicious_scores.shape)\n",
    "        malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "    if malicious_scores.shape[0] >= 11:\n",
    "        print('performing detection at epoch %d' % e)\n",
    "        if detection1(np.sum(malicious_scores[-window_size:], axis=0), 20):\n",
    "            print('Stop at iteration:', e)\n",
    "            detection(np.sum(malicious_scores[-window_size:], axis=0), 20)\n",
    "            break\n",
    "\n",
    "#     agg_grads = tr_mean(user_grads, 20)\n",
    "#     agg_grads=torch.median(user_grads,dim=0)[0]\n",
    "#     agg_grads=torch.mean(user_grads,dim=0)\n",
    "\n",
    "    if e > (start_detection_epoch - window_size):\n",
    "        weight_record.append(weight - last_weight)\n",
    "        grad_record.append(agg_grads - last_grad)\n",
    "    \n",
    "    if (len(weight_record) > 10):\n",
    "        del weight_record[0]\n",
    "        del grad_record[0]\n",
    "    \n",
    "    last_weight = weight\n",
    "    last_grad = agg_grads\n",
    "    old_grad_list = user_grads\n",
    "  \n",
    "    del user_grads\n",
    "    \n",
    "    start_idx=0\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "\n",
    "    model_grads=[]\n",
    "\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_data):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(e,correct/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
