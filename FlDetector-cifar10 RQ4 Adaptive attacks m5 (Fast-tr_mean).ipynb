{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for CIFAR10 with Fang/Dirichlet distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e300ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu122/4396552/ipykernel_3920808/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "import copy\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77aa30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_models import *\n",
    "from cifar10_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "\n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fang_train_data(trainset, num_workers=100, bias=0.5, force=False):\n",
    "    dist_file = 'fang_nworkers%d_bias%.1f.pkl' % (num_workers, bias)\n",
    "    if not force and os.path.exists(dist_file):\n",
    "        print('Loading fang distribution for num_workers %d and bias %.1f from memory' % (num_workers, bias))\n",
    "        return pickle.load(open(dist_file, 'rb'))\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    per_participant_list = defaultdict(list)\n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        per_participant_list[selected_worker].extend([i])\n",
    "    \n",
    "    print('Saving fang distribution for num_workers %d and bias %.1f to memory' % (num_workers, bias))\n",
    "    pickle.dump(per_participant_list, open(dist_file, 'wb'))\n",
    "    return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799ffb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_federated_data(trainset, num_workers, distribution='fang', param=1, force=False):\n",
    "    if distribution == 'fang':\n",
    "        per_participant_list = get_fang_train_data(trainset, num_workers, bias=param, force=force)\n",
    "    elif distribution == 'dirichlet':\n",
    "        per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=param, force=force)\n",
    "\n",
    "    each_worker_idx = [[] for _ in range(num_workers)] \n",
    "    each_worker_te_idx = [[] for _ in range(num_workers)]\n",
    "    np.random.seed(0)\n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(5*w_len/6)\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        each_worker_idx[worker_idx] = w_indices[tr_idx]\n",
    "        each_worker_te_idx[worker_idx] = w_indices[te_idx]\n",
    "    global_test_idx = np.concatenate(each_worker_te_idx)\n",
    "    return each_worker_idx, each_worker_te_idx, global_test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2590db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(dataset, indices, batch_size=32, shuffle=False):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acbb06d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/home/vshejwalkar_umass_edu/data/'\n",
    "# load the train dataset\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_train)\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_train)\n",
    "\n",
    "te_cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_test)\n",
    "te_cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(trainloader, model, model_received, criterion, optimizer, pgd=False, eps=2):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size()[0])\n",
    "        top1.update(prec1.item()/100.0, inputs.size()[0])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if pgd:\n",
    "            curr_model = list(model.parameters())\n",
    "            curr_model_vec = parameters_to_vector(curr_model)\n",
    "            if torch.norm(curr_model_vec - model_received) > eps:\n",
    "                curr_model_vec = eps*(curr_model_vec - model_received)/torch.norm(curr_model_vec - model_received) + model_received\n",
    "                vector_to_parameters(curr_model_vec, curr_model)\n",
    "        \n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def test(testloader, model, criterion):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    for batch_ind, (inputs, targets) in enumerate(testloader):\n",
    "        inputs = inputs.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data, inputs.size()[0])\n",
    "        top1.update(prec1/100.0, inputs.size()[0])\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f843efb",
   "metadata": {},
   "source": [
    "# resnet-BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bb2ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5202826d",
   "metadata": {},
   "source": [
    "# FLDetector utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cabefee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def fldetector(old_gradients, user_grads, b=0, hvp=None, agr='tr_mean'):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    if agr == 'average':\n",
    "        agg_grads = torch.mean(user_grads, 0)\n",
    "    elif agr == 'median':\n",
    "        agg_grads = torch.median(user_grads, 0)[0]\n",
    "    elif agr == 'tr_mean':\n",
    "        agg_grads = tr_mean(user_grads, b)\n",
    "    return agg_grads, distance\n",
    "\n",
    "def lbfgs(S_k_list, Y_k_list, v):\n",
    "    curr_S_k = torch.stack(S_k_list).T\n",
    "    curr_Y_k = torch.stack(Y_k_list).T\n",
    "    S_k_time_Y_k = np.dot(curr_S_k.T.cpu().numpy(), curr_Y_k.cpu().numpy())\n",
    "    S_k_time_S_k = np.dot(curr_S_k.T.cpu().numpy(), curr_S_k.cpu().numpy())\n",
    "    R_k = np.triu(S_k_time_Y_k)\n",
    "    L_k = S_k_time_Y_k - R_k\n",
    "    sigma_k = np.dot(Y_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()) / (np.dot(S_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()))\n",
    "    D_k_diag = np.diag(S_k_time_Y_k)\n",
    "    upper_mat = np.concatenate((sigma_k * S_k_time_S_k, L_k), axis=1)\n",
    "    lower_mat = np.concatenate((L_k.T, -np.diag(D_k_diag)), axis=1)\n",
    "    mat = np.concatenate((upper_mat, lower_mat), axis=0)\n",
    "    mat_inv = np.linalg.inv(mat)\n",
    "\n",
    "    approx_prod = sigma_k * v.cpu().numpy()\n",
    "    approx_prod = approx_prod.T\n",
    "    p_mat = np.concatenate((np.dot(curr_S_k.T.cpu().numpy(), sigma_k * v.unsqueeze(0).T.cpu().numpy()), np.dot(curr_Y_k.T.cpu().numpy(), v.unsqueeze(0).T.cpu().numpy())), axis=0)\n",
    "    approx_prod -= np.dot(np.dot(np.concatenate((sigma_k * curr_S_k.cpu().numpy(), curr_Y_k.cpu().numpy()), axis=1), mat_inv), p_mat)\n",
    "\n",
    "    return approx_prod\n",
    "\n",
    "def detection(score, nobyz, nworkers):\n",
    "    estimator = KMeans(n_clusters=2)\n",
    "    estimator.fit(score.reshape(-1, 1))\n",
    "    label_pred = estimator.labels_\n",
    "    if np.mean(score[label_pred==0])<np.mean(score[label_pred==1]):\n",
    "        #0 is the label of malicious clients\n",
    "        label_pred = 1 - label_pred\n",
    "    real_label=np.ones(nworkers)\n",
    "    real_label[:nobyz]=0\n",
    "    acc=len(label_pred[label_pred==real_label])/nworkers\n",
    "    recall=1-np.sum(label_pred[:nobyz])/nobyz\n",
    "    fpr=1-np.sum(label_pred[nobyz:])/(nworkers-nobyz)\n",
    "    fnr=np.sum(label_pred[:nobyz])/nobyz\n",
    "    auc = roc_auc_score(real_label, label_pred)\n",
    "    print(\"acc %0.4f; recall %0.4f; fpr %0.4f; fnr %0.4f; auc %.4f\" % (acc, recall, fpr, fnr, auc))\n",
    "    return acc, fpr, fnr, auc\n",
    "    # print(silhouette_score(score.reshape(-1, 1), label_pred))\n",
    "\n",
    "def detection1(score, nobyz):\n",
    "    nrefs = 10\n",
    "    ks = range(1, 8)\n",
    "    gaps = np.zeros(len(ks))\n",
    "    gapDiff = np.zeros(len(ks) - 1)\n",
    "    sdk = np.zeros(len(ks))\n",
    "    min = np.min(score)\n",
    "    max = np.max(score)\n",
    "    score = (score - min)/(max-min)\n",
    "    for i, k in enumerate(ks):\n",
    "        estimator = KMeans(n_clusters=k)\n",
    "        estimator.fit(score.reshape(-1, 1))\n",
    "        label_pred = estimator.labels_\n",
    "        center = estimator.cluster_centers_\n",
    "        Wk = np.sum([np.square(score[m]-center[label_pred[m]]) for m in range(len(score))])\n",
    "        WkRef = np.zeros(nrefs)\n",
    "        for j in range(nrefs):\n",
    "            rand = np.random.uniform(0, 1, len(score))\n",
    "            estimator = KMeans(n_clusters=k)\n",
    "            estimator.fit(rand.reshape(-1, 1))\n",
    "            label_pred = estimator.labels_\n",
    "            center = estimator.cluster_centers_\n",
    "            WkRef[j] = np.sum([np.square(rand[m]-center[label_pred[m]]) for m in range(len(rand))])\n",
    "        gaps[i] = np.log(np.mean(WkRef)) - np.log(Wk)\n",
    "        sdk[i] = np.sqrt((1.0 + nrefs) / nrefs) * np.std(np.log(WkRef))\n",
    "\n",
    "        if i > 0:\n",
    "            gapDiff[i - 1] = gaps[i - 1] - gaps[i] + sdk[i]\n",
    "    #print(gapDiff)\n",
    "    for i in range(len(gapDiff)):\n",
    "        if gapDiff[i] >= 0:\n",
    "            select_k = i+1\n",
    "            break\n",
    "    if select_k == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        # print('Attack Detected!')\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e25e2ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fang distribution for num_workers 100 and bias 0.5 from memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 100, 10042)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((cifar10_train, cifar10_test))\n",
    "all_test_data = torch.utils.data.ConcatDataset((te_cifar10_train, te_cifar10_test))\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = False\n",
    "\n",
    "each_worker_idx, each_worker_te_idx, global_test_idx = get_federated_data(\n",
    "    all_data, num_workers=100, distribution=distribution, param=param, force=force)\n",
    "\n",
    "train_loaders = []\n",
    "for pos, indices in enumerate(each_worker_idx):\n",
    "    train_loaders.append((pos, get_train(all_data, indices, batch_size)))\n",
    "test_loaders = []\n",
    "for pos, indices in enumerate(each_worker_te_idx):\n",
    "    test_loaders.append((pos, get_train(all_test_data, indices, len(indices))))\n",
    "cifar10_test_loader = get_train(all_test_data, global_test_idx)\n",
    "\n",
    "len(train_loaders), len(test_loaders), len(global_test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "771158c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r 0 nmal 5 | e 0 benign_norm 1431.698 val loss 2.309 val acc 0.099 best val_acc 0.099\n",
      "r 0 nmal 5 | e 5 benign_norm 276.077 val loss 2.735 val acc 0.103 best val_acc 0.103\n",
      "r 0 nmal 5 | e 10 benign_norm 273.367 val loss 1.782 val acc 0.319 best val_acc 0.319\n",
      "r 0 nmal 5 | e 15 benign_norm 274.743 val loss 1.456 val acc 0.447 best val_acc 0.447\n",
      "r 0 nmal 5 | e 20 benign_norm 274.796 val loss 1.341 val acc 0.497 best val_acc 0.497\n",
      "r 0 nmal 5 | e 25 benign_norm 275.043 val loss 1.267 val acc 0.532 best val_acc 0.532\n",
      "r 0 nmal 5 | e 30 benign_norm 275.296 val loss 1.186 val acc 0.570 best val_acc 0.570\n",
      "r 0 nmal 5 | e 35 benign_norm 275.707 val loss 1.085 val acc 0.607 best val_acc 0.607\n",
      "r 0 nmal 5 | e 40 benign_norm 276.084 val loss 0.989 val acc 0.645 best val_acc 0.645\n",
      "r 0 nmal 5 | e 45 benign_norm 276.679 val loss 0.937 val acc 0.665 best val_acc 0.665\n",
      "r 0 nmal 5 | e 50 benign_norm 277.528 val loss 0.882 val acc 0.685 best val_acc 0.685\n",
      "r 0 nmal 5 | e 55 benign_norm 277.935 val loss 0.876 val acc 0.692 best val_acc 0.697\n",
      "r 0 nmal 5 | e 60 benign_norm 278.870 val loss 0.819 val acc 0.711 best val_acc 0.711\n",
      "r 0 nmal 5 | e 65 benign_norm 279.712 val loss 0.791 val acc 0.722 best val_acc 0.722\n",
      "r 0 nmal 5 | e 70 benign_norm 280.636 val loss 0.800 val acc 0.722 best val_acc 0.729\n",
      "r 0 nmal 5 | e 75 benign_norm 282.172 val loss 0.743 val acc 0.739 best val_acc 0.739\n",
      "r 0 nmal 5 | e 80 benign_norm 283.875 val loss 0.730 val acc 0.745 best val_acc 0.746\n",
      "r 0 nmal 5 | e 85 benign_norm 284.338 val loss 0.713 val acc 0.753 best val_acc 0.753\n",
      "r 0 nmal 5 | e 90 benign_norm 285.842 val loss 0.698 val acc 0.760 best val_acc 0.760\n",
      "r 0 nmal 5 | e 95 benign_norm 287.598 val loss 0.685 val acc 0.766 best val_acc 0.766\n",
      "r 0 nmal 5 | e 100 benign_norm 289.968 val loss 0.679 val acc 0.769 best val_acc 0.771\n",
      "r 0 nmal 5 | e 105 benign_norm 290.944 val loss 0.673 val acc 0.770 best val_acc 0.774\n",
      "r 0 nmal 5 | e 110 benign_norm 294.382 val loss 0.654 val acc 0.778 best val_acc 0.778\n",
      "r 0 nmal 5 | e 115 benign_norm 294.781 val loss 0.655 val acc 0.779 best val_acc 0.781\n",
      "r 0 nmal 5 | e 120 benign_norm 296.846 val loss 0.639 val acc 0.783 best val_acc 0.783\n",
      "r 0 nmal 5 | e 125 benign_norm 300.199 val loss 0.633 val acc 0.786 best val_acc 0.787\n",
      "r 0 nmal 5 | e 135 benign_norm 301.726 val loss 0.631 val acc 0.787 best val_acc 0.789\n",
      "r 0 nmal 5 | e 140 benign_norm 309.773 val loss 0.616 val acc 0.790 best val_acc 0.791\n",
      "r 0 nmal 5 | e 145 benign_norm 312.926 val loss 0.619 val acc 0.792 best val_acc 0.793\n",
      "r 0 nmal 5 | e 150 benign_norm 312.025 val loss 0.609 val acc 0.794 best val_acc 0.795\n",
      "r 0 nmal 5 | e 155 benign_norm 310.901 val loss 0.612 val acc 0.793 best val_acc 0.795\n",
      "r 0 nmal 5 | e 160 benign_norm 317.855 val loss 0.602 val acc 0.797 best val_acc 0.797\n",
      "r 0 nmal 5 | e 165 benign_norm 314.526 val loss 0.602 val acc 0.799 best val_acc 0.799\n",
      "r 0 nmal 5 | e 170 benign_norm 320.593 val loss 0.599 val acc 0.799 best val_acc 0.799\n",
      "r 0 nmal 5 | e 175 benign_norm 320.396 val loss 0.595 val acc 0.800 best val_acc 0.800\n",
      "r 0 nmal 5 | e 180 benign_norm 325.083 val loss 0.597 val acc 0.801 best val_acc 0.801\n",
      "r 0 nmal 5 | e 185 benign_norm 327.366 val loss 0.589 val acc 0.805 best val_acc 0.805\n",
      "r 0 nmal 5 | e 190 benign_norm 330.390 val loss 0.582 val acc 0.804 best val_acc 0.805\n",
      "r 0 nmal 5 | e 195 benign_norm 331.273 val loss 0.589 val acc 0.803 best val_acc 0.805\n",
      "r 0 nmal 5 | e 199 benign_norm 345.363 val loss 0.586 val acc 0.804 best val_acc 0.805\n",
      "r 0 nmal 5 | e 200 benign_norm 337.450 val loss 0.589 val acc 0.803 best val_acc 0.805\n",
      "r 1 nmal 5 | e 0 benign_norm 818.534 val loss 2.306 val acc 0.102 best val_acc 0.102\n",
      "r 1 nmal 5 | e 5 benign_norm 276.157 val loss 2.118 val acc 0.187 best val_acc 0.187\n",
      "r 1 nmal 5 | e 10 benign_norm 272.918 val loss 1.690 val acc 0.364 best val_acc 0.364\n",
      "r 1 nmal 5 | e 15 benign_norm 274.793 val loss 1.487 val acc 0.438 best val_acc 0.438\n",
      "r 1 nmal 5 | e 20 benign_norm 274.884 val loss 1.270 val acc 0.527 best val_acc 0.527\n",
      "r 1 nmal 5 | e 25 benign_norm 275.394 val loss 1.183 val acc 0.571 best val_acc 0.571\n",
      "r 1 nmal 5 | e 30 benign_norm 275.795 val loss 1.075 val acc 0.613 best val_acc 0.613\n",
      "r 1 nmal 5 | e 35 benign_norm 276.289 val loss 1.042 val acc 0.630 best val_acc 0.630\n",
      "r 1 nmal 5 | e 40 benign_norm 277.077 val loss 0.983 val acc 0.653 best val_acc 0.653\n",
      "r 1 nmal 5 | e 45 benign_norm 277.907 val loss 0.945 val acc 0.668 best val_acc 0.668\n",
      "r 1 nmal 5 | e 50 benign_norm 278.830 val loss 0.886 val acc 0.693 best val_acc 0.693\n",
      "r 1 nmal 5 | e 55 benign_norm 279.791 val loss 0.861 val acc 0.705 best val_acc 0.705\n",
      "r 1 nmal 5 | e 60 benign_norm 281.310 val loss 0.850 val acc 0.710 best val_acc 0.715\n",
      "r 1 nmal 5 | e 65 benign_norm 283.123 val loss 0.809 val acc 0.725 best val_acc 0.725\n",
      "r 1 nmal 5 | e 70 benign_norm 283.969 val loss 0.810 val acc 0.728 best val_acc 0.737\n",
      "r 1 nmal 5 | e 75 benign_norm 286.074 val loss 0.758 val acc 0.746 best val_acc 0.746\n",
      "r 1 nmal 5 | e 80 benign_norm 286.986 val loss 0.727 val acc 0.756 best val_acc 0.756\n",
      "r 1 nmal 5 | e 85 benign_norm 291.268 val loss 0.742 val acc 0.754 best val_acc 0.757\n",
      "r 1 nmal 5 | e 90 benign_norm 291.606 val loss 0.704 val acc 0.764 best val_acc 0.764\n",
      "r 1 nmal 5 | e 95 benign_norm 294.595 val loss 0.702 val acc 0.763 best val_acc 0.770\n",
      "r 1 nmal 5 | e 100 benign_norm 295.957 val loss 0.668 val acc 0.774 best val_acc 0.774\n",
      "r 1 nmal 5 | e 105 benign_norm 299.673 val loss 0.661 val acc 0.777 best val_acc 0.777\n",
      "r 1 nmal 5 | e 110 benign_norm 302.505 val loss 0.660 val acc 0.776 best val_acc 0.779\n",
      "r 1 nmal 5 | e 115 benign_norm 306.410 val loss 0.665 val acc 0.778 best val_acc 0.782\n",
      "r 1 nmal 5 | e 120 benign_norm 305.330 val loss 0.662 val acc 0.777 best val_acc 0.782\n",
      "r 1 nmal 5 | e 125 benign_norm 311.714 val loss 0.647 val acc 0.782 best val_acc 0.782\n",
      "r 1 nmal 5 | e 130 benign_norm 319.528 val loss 0.633 val acc 0.786 best val_acc 0.786\n",
      "r 1 nmal 5 | e 135 benign_norm 318.550 val loss 0.643 val acc 0.783 best val_acc 0.786\n",
      "r 1 nmal 5 | e 140 benign_norm 319.446 val loss 0.622 val acc 0.791 best val_acc 0.791\n",
      "r 1 nmal 5 | e 145 benign_norm 325.665 val loss 0.624 val acc 0.788 best val_acc 0.791\n",
      "r 1 nmal 5 | e 150 benign_norm 326.296 val loss 0.610 val acc 0.793 best val_acc 0.793\n",
      "r 1 nmal 5 | e 155 benign_norm 332.720 val loss 0.607 val acc 0.794 best val_acc 0.798\n",
      "r 1 nmal 5 | e 160 benign_norm 335.064 val loss 0.604 val acc 0.797 best val_acc 0.798\n",
      "!!!! Stop at iteration: 162\n",
      "r 1 nmal 5 | e 162 benign_norm 338.058 val loss 0.593 val acc 0.799 best val_acc 0.799\n",
      "acc 0.6235; recall 1.0000; fpr 0.4000; fnr 0.0000; auc 0.8000\n",
      "r 2 nmal 5 | e 0 benign_norm 1085.270 val loss 2.307 val acc 0.102 best val_acc 0.102\n",
      "r 2 nmal 5 | e 5 benign_norm 275.188 val loss 2.022 val acc 0.212 best val_acc 0.212\n",
      "r 2 nmal 5 | e 10 benign_norm 273.238 val loss 1.616 val acc 0.395 best val_acc 0.395\n",
      "r 2 nmal 5 | e 15 benign_norm 274.970 val loss 1.402 val acc 0.472 best val_acc 0.472\n",
      "r 2 nmal 5 | e 20 benign_norm 275.109 val loss 1.291 val acc 0.529 best val_acc 0.529\n",
      "r 2 nmal 5 | e 25 benign_norm 275.615 val loss 1.122 val acc 0.588 best val_acc 0.588\n",
      "r 2 nmal 5 | e 30 benign_norm 275.968 val loss 1.048 val acc 0.619 best val_acc 0.619\n",
      "r 2 nmal 5 | e 35 benign_norm 276.479 val loss 0.990 val acc 0.641 best val_acc 0.641\n",
      "r 2 nmal 5 | e 40 benign_norm 277.198 val loss 0.944 val acc 0.660 best val_acc 0.660\n",
      "r 2 nmal 5 | e 45 benign_norm 278.168 val loss 0.894 val acc 0.682 best val_acc 0.682\n",
      "r 2 nmal 5 | e 50 benign_norm 279.027 val loss 0.875 val acc 0.694 best val_acc 0.694\n",
      "r 2 nmal 5 | e 55 benign_norm 279.931 val loss 0.836 val acc 0.707 best val_acc 0.707\n",
      "r 2 nmal 5 | e 60 benign_norm 281.503 val loss 0.817 val acc 0.717 best val_acc 0.717\n",
      "r 2 nmal 5 | e 65 benign_norm 283.061 val loss 0.800 val acc 0.725 best val_acc 0.725\n",
      "r 2 nmal 5 | e 70 benign_norm 284.390 val loss 0.773 val acc 0.732 best val_acc 0.735\n",
      "r 2 nmal 5 | e 75 benign_norm 286.020 val loss 0.751 val acc 0.744 best val_acc 0.744\n",
      "r 2 nmal 5 | e 80 benign_norm 287.996 val loss 0.721 val acc 0.753 best val_acc 0.753\n",
      "r 2 nmal 5 | e 85 benign_norm 289.878 val loss 0.709 val acc 0.758 best val_acc 0.758\n",
      "r 2 nmal 5 | e 90 benign_norm 292.734 val loss 0.698 val acc 0.762 best val_acc 0.762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r 2 nmal 5 | e 95 benign_norm 293.971 val loss 0.707 val acc 0.759 best val_acc 0.763\n",
      "r 2 nmal 5 | e 100 benign_norm 298.362 val loss 0.675 val acc 0.771 best val_acc 0.771\n",
      "r 2 nmal 5 | e 105 benign_norm 297.953 val loss 0.670 val acc 0.774 best val_acc 0.774\n",
      "r 2 nmal 5 | e 110 benign_norm 300.477 val loss 0.662 val acc 0.776 best val_acc 0.778\n",
      "r 2 nmal 5 | e 115 benign_norm 304.638 val loss 0.650 val acc 0.780 best val_acc 0.780\n",
      "r 2 nmal 5 | e 120 benign_norm 310.258 val loss 0.663 val acc 0.775 best val_acc 0.781\n",
      "r 2 nmal 5 | e 125 benign_norm 316.197 val loss 0.662 val acc 0.779 best val_acc 0.784\n",
      "r 2 nmal 5 | e 130 benign_norm 312.158 val loss 0.657 val acc 0.780 best val_acc 0.784\n",
      "r 2 nmal 5 | e 135 benign_norm 316.176 val loss 0.638 val acc 0.787 best val_acc 0.788\n",
      "r 2 nmal 5 | e 140 benign_norm 320.207 val loss 0.615 val acc 0.791 best val_acc 0.791\n",
      "r 2 nmal 5 | e 145 benign_norm 320.234 val loss 0.631 val acc 0.791 best val_acc 0.792\n",
      "r 2 nmal 5 | e 150 benign_norm 325.963 val loss 0.629 val acc 0.789 best val_acc 0.793\n",
      "r 2 nmal 5 | e 155 benign_norm 328.397 val loss 0.627 val acc 0.788 best val_acc 0.795\n",
      "r 2 nmal 5 | e 160 benign_norm 331.435 val loss 0.616 val acc 0.793 best val_acc 0.797\n",
      "r 2 nmal 5 | e 165 benign_norm 340.927 val loss 0.624 val acc 0.795 best val_acc 0.797\n",
      "r 2 nmal 5 | e 170 benign_norm 332.180 val loss 0.612 val acc 0.796 best val_acc 0.799\n",
      "r 2 nmal 5 | e 175 benign_norm 340.898 val loss 0.616 val acc 0.798 best val_acc 0.799\n",
      "r 2 nmal 5 | e 180 benign_norm 341.323 val loss 0.604 val acc 0.799 best val_acc 0.801\n",
      "r 2 nmal 5 | e 185 benign_norm 342.426 val loss 0.599 val acc 0.805 best val_acc 0.805\n",
      "r 2 nmal 5 | e 190 benign_norm 351.595 val loss 0.594 val acc 0.804 best val_acc 0.805\n",
      "r 2 nmal 5 | e 195 benign_norm 358.861 val loss 0.598 val acc 0.803 best val_acc 0.806\n",
      "r 2 nmal 5 | e 199 benign_norm 359.047 val loss 0.603 val acc 0.804 best val_acc 0.806\n",
      "r 2 nmal 5 | e 200 benign_norm 365.742 val loss 0.587 val acc 0.808 best val_acc 0.808\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_malicious = [5]\n",
    "n_runs = 3\n",
    "results={}\n",
    "for n_mal in n_malicious:\n",
    "    nbyz = n_mal\n",
    "    results[n_mal] = {'acc': [], 'fpr': [], 'fnr': [], 'auc': [], 'stop_e': []}\n",
    "    for run in range(n_runs):\n",
    "        torch.cuda.empty_cache()\n",
    "        nepochs=200\n",
    "        local_epochs = 2\n",
    "        batch_size = 16\n",
    "        num_workers = 100\n",
    "        local_lr = 0.1\n",
    "        global_lr = 1\n",
    "        agr = 'tr_mean'\n",
    "        best_global_acc=0\n",
    "        epoch_num = 0\n",
    "\n",
    "        fed_model = resnet20().cuda()\n",
    "        model_received = []\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        best_accs_per_round = []\n",
    "        accs_per_round = []\n",
    "        loss_per_round = []\n",
    "        home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "        # FLDetector initializations\n",
    "        weight_record = []\n",
    "        grad_record = []\n",
    "        test_grads = []\n",
    "        old_grad_list = []\n",
    "        malicious_scores = np.zeros((1, num_workers-(20-nbyz)))\n",
    "        start_detection_epoch = 10\n",
    "        window_size = 10\n",
    "        assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "        # Adaptive attack initializers\n",
    "        good_distance_range = np.zeros((1, nbyz))\n",
    "        attack_type = 'NDSS21'\n",
    "        dev_type = 'std'\n",
    "        \n",
    "        \n",
    "        while epoch_num <= nepochs:\n",
    "            torch.cuda.empty_cache()\n",
    "            round_clients = np.arange(20-nbyz, num_workers)\n",
    "            round_benign = round_clients\n",
    "            user_grads=[]\n",
    "            benign_norm = 0\n",
    "            for i in round_benign:\n",
    "                model = copy.deepcopy(fed_model)\n",
    "                optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.999**epoch_num), momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(train_loaders[i][1], model, model_received, criterion, optimizer)\n",
    "\n",
    "                params = []\n",
    "                for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                    params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat((params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "                update =  (params - model_received)\n",
    "                benign_norm += torch.norm(update)/len(round_benign)\n",
    "                user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "            weight = model_received\n",
    "\n",
    "            if (epoch_num > start_detection_epoch):\n",
    "                hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "                hvp = np.squeeze(hvp)\n",
    "            else:\n",
    "                hvp = None\n",
    "\n",
    "            good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "            if hvp is not None:\n",
    "                pred_grad = copy.deepcopy(good_old_grads)\n",
    "                distance = []\n",
    "                for i in range(len(good_old_grads)):\n",
    "                    pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "                good_distance_range = np.concatenate(\n",
    "                    (good_distance_range, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "            if attack_type != 'none' and (epoch_num < start_detection_epoch):\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "                # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "            elif epoch_num > start_detection_epoch:\n",
    "                if attack_type == 'full_trim':\n",
    "                    user_grads = full_trim(user_grads, nbyz)\n",
    "                elif attack_type == 'none':\n",
    "                    pass\n",
    "                else:\n",
    "                    if attack_type == 'NDSS21':\n",
    "                        distance_bound = np.random.choice(np.mean(good_distance_range[-1:], 0))\n",
    "                        model_re = torch.mean(good_current_grads, dim=0)\n",
    "                        if dev_type == 'unit_vec':\n",
    "                            deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                        elif dev_type == 'sign':\n",
    "                            deviation = torch.sign(model_re)\n",
    "                        elif dev_type == 'std':\n",
    "                            deviation = torch.std(good_current_grads, 0)\n",
    "                        # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_range[-1]))) / torch.norm(deviation))\n",
    "                        noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                    elif attack_type == 'mod_trim':\n",
    "                        mal_grads= full_trim(user_grads[:nbyz], nbyz)\n",
    "                        pass\n",
    "                    else:\n",
    "                        noise = torch.zeros(hvp.shape).to(device)\n",
    "                    for m in range(nbyz):\n",
    "                        user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "            agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "            if distance is not None and epoch_num > (start_detection_epoch - window_size):\n",
    "                malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "            if malicious_scores.shape[0] >= window_size+1:\n",
    "                if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                    print('!!!! Stop at iteration:', epoch_num)\n",
    "                    print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "                    acc, fpr, fnr, auc = detection(\n",
    "                        np.sum(malicious_scores[-window_size:], axis=0), nbyz, len(user_grads))\n",
    "                    results[n_mal]['acc'].append(acc)\n",
    "                    results[n_mal]['fpr'].append(fpr)\n",
    "                    results[n_mal]['fnr'].append(fnr)\n",
    "                    results[n_mal]['auc'].append(auc)\n",
    "                    results[n_mal]['stop_e'].append(epoch_num)\n",
    "                    break\n",
    "                    break\n",
    "\n",
    "            if epoch_num > (start_detection_epoch - window_size):\n",
    "                weight_record.append(weight - last_weight)\n",
    "                grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "            if (len(weight_record) > window_size):\n",
    "                del weight_record[0]\n",
    "                del grad_record[0]\n",
    "\n",
    "            last_weight = weight\n",
    "            last_grad = agg_grads\n",
    "            old_grad_list = user_grads\n",
    "            good_old_grads = good_current_grads\n",
    "            del user_grads\n",
    "\n",
    "            model_received = model_received + global_lr * agg_grads\n",
    "            fed_model = resnet20().cuda()\n",
    "            start_idx=0\n",
    "            state_dict = {}\n",
    "            previous_name = 'none'\n",
    "            for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "                start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "                start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "                params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "                state_dict[name] = params\n",
    "                previous_name = name\n",
    "            fed_model.load_state_dict(state_dict)\n",
    "            val_loss, val_acc = test(cifar10_test_loader, fed_model, criterion)\n",
    "            is_best = best_global_acc < val_acc\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "            best_accs_per_round.append(best_global_acc.cpu().item())\n",
    "            accs_per_round.append(val_acc.cpu().item())\n",
    "            loss_per_round.append(val_loss.cpu().item())\n",
    "\n",
    "            if epoch_num%5==0 or epoch_num==nepochs-1:\n",
    "                print('r %d nmal %d | e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (run, nbyz, epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "            if math.isnan(val_loss) or val_loss > 100000:\n",
    "                print('val loss %f... exit'%val_loss)\n",
    "                break\n",
    "            epoch_num+=1\n",
    "\n",
    "        final_accs_per_client=[]\n",
    "        for i in range(num_workers):\n",
    "            client_loss, client_acc = test(test_loaders[i][1], fed_model, criterion)\n",
    "            final_accs_per_client.append(client_acc.cpu().item())\n",
    "        all_results = collections.OrderedDict(\n",
    "            final_accs_per_client=np.array(final_accs_per_client),\n",
    "            accs_per_round=np.array(accs_per_round),\n",
    "            best_accs_per_round=np.array(best_accs_per_round),\n",
    "            loss_per_round=np.array(loss_per_round),\n",
    "            results=results\n",
    "        )\n",
    "        pickle.dump(all_results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq4_adaptive_cifar10_fast_trmean_NDSS21_std_m%d_r%d.pkl' % (nbyz, run)), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6d834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
