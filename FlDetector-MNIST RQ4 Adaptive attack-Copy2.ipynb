{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for MNIST with Fang/Dirichlet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821b1014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1032510/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a52c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "            \n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_train_data(trainset, num_workers=100, bias=0.5):\n",
    "    np.random.seed(0)\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        \n",
    "        if not len(each_worker_data[selected_worker]):\n",
    "            each_worker_data[selected_worker] = x[None, :]\n",
    "        else:\n",
    "            each_worker_data[selected_worker]= torch.concat((each_worker_data[selected_worker], x[None, :]))\n",
    "        \n",
    "        each_worker_label[selected_worker].append(y)\n",
    "    \n",
    "    each_worker_tr_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_tr_label = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    for i in range(num_workers):\n",
    "        w_len = len(each_worker_data[i])\n",
    "        len_tr = int(6 * w_len / 7)\n",
    "        len_te = w_len - len_tr\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        each_worker_tr_data[i] = each_worker_data[i][tr_idx]\n",
    "        each_worker_tr_label[i] = torch.Tensor(each_worker_label[i])[tr_idx]\n",
    "        \n",
    "        each_worker_te_data[i] = each_worker_data[i][te_idx]\n",
    "        each_worker_te_label[i] = torch.Tensor(each_worker_label[i])[te_idx]\n",
    "        \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    del each_worker_data, each_worker_label\n",
    "    return each_worker_tr_data, each_worker_tr_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f36f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_data_dirichlet(trainset, num_workers, alpha=1, force=False):\n",
    "    per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=alpha, force=force)\n",
    "    \n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(6*w_len/7)\n",
    "        len_te = w_len - len_tr\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        \n",
    "        for idx in tr_idx:\n",
    "            each_worker_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_data[worker_idx] = torch.stack(each_worker_data[worker_idx])\n",
    "        each_worker_label[worker_idx] = torch.Tensor(each_worker_label[worker_idx]).long()\n",
    "        \n",
    "        for idx in te_idx:\n",
    "            each_worker_te_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_te_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_te_data[worker_idx] = torch.stack(each_worker_te_data[worker_idx])\n",
    "        each_worker_te_label[worker_idx] = torch.Tensor(each_worker_te_label[worker_idx]).long()\n",
    "    \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    \n",
    "    return each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6b5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 30, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(30, 50, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(800, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17acb027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def fldetector(old_gradients, user_grads, b=0, hvp=None, agr='tr_mean'):\n",
    "    if hvp is not None:\n",
    "        hvp = torch.from_numpy(hvp).to(device)\n",
    "        pred_grad = copy.deepcopy(old_gradients)\n",
    "        distance = []\n",
    "        for i in range(len(old_gradients)):\n",
    "            pred_grad[i] += hvp\n",
    "        pred = np.zeros(100)\n",
    "        pred[:b] = 1\n",
    "        distance = torch.norm(pred_grad - user_grads, dim = 1).cpu().numpy()\n",
    "        distance = distance / np.sum(distance)\n",
    "    else:\n",
    "        distance = None\n",
    "    \n",
    "    if agr == 'average':\n",
    "        agg_grads = torch.mean(user_grads, 0)\n",
    "    elif agr == 'median':\n",
    "        agg_grads = torch.median(user_grads, 0)[0]\n",
    "    elif agr == 'tr_mean':\n",
    "        agg_grads = tr_mean(user_grads, b)\n",
    "    return agg_grads, distance\n",
    "\n",
    "def lbfgs(S_k_list, Y_k_list, v):\n",
    "    curr_S_k = torch.stack(S_k_list).T\n",
    "    curr_Y_k = torch.stack(Y_k_list).T\n",
    "    S_k_time_Y_k = np.dot(curr_S_k.T.cpu().numpy(), curr_Y_k.cpu().numpy())\n",
    "    S_k_time_S_k = np.dot(curr_S_k.T.cpu().numpy(), curr_S_k.cpu().numpy())\n",
    "    R_k = np.triu(S_k_time_Y_k)\n",
    "    L_k = S_k_time_Y_k - R_k\n",
    "    sigma_k = np.dot(Y_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()) / (np.dot(S_k_list[-1].unsqueeze(0).cpu().numpy(), S_k_list[-1].unsqueeze(0).T.cpu().numpy()))\n",
    "    D_k_diag = np.diag(S_k_time_Y_k)\n",
    "    upper_mat = np.concatenate((sigma_k * S_k_time_S_k, L_k), axis=1)\n",
    "    lower_mat = np.concatenate((L_k.T, -np.diag(D_k_diag)), axis=1)\n",
    "    mat = np.concatenate((upper_mat, lower_mat), axis=0)\n",
    "    mat_inv = np.linalg.inv(mat)\n",
    "\n",
    "    approx_prod = sigma_k * v.cpu().numpy()\n",
    "    approx_prod = approx_prod.T\n",
    "    p_mat = np.concatenate((np.dot(curr_S_k.T.cpu().numpy(), sigma_k * v.unsqueeze(0).T.cpu().numpy()), np.dot(curr_Y_k.T.cpu().numpy(), v.unsqueeze(0).T.cpu().numpy())), axis=0)\n",
    "    approx_prod -= np.dot(np.dot(np.concatenate((sigma_k * curr_S_k.cpu().numpy(), curr_Y_k.cpu().numpy()), axis=1), mat_inv), p_mat)\n",
    "\n",
    "    return approx_prod\n",
    "\n",
    "def detection(score, nobyz, nworkers):\n",
    "    estimator = KMeans(n_clusters=2)\n",
    "    estimator.fit(score.reshape(-1, 1))\n",
    "    label_pred = estimator.labels_\n",
    "    if np.mean(score[label_pred==0])<np.mean(score[label_pred==1]):\n",
    "        #0 is the label of malicious clients\n",
    "        label_pred = 1 - label_pred\n",
    "    real_label=np.ones(nworkers)\n",
    "    real_label[:nobyz]=0\n",
    "    acc=len(label_pred[label_pred==real_label])/nworkers\n",
    "    recall=1-np.sum(label_pred[:nobyz])/nobyz\n",
    "    fpr=1-np.sum(label_pred[nobyz:])/(nworkers-nobyz)\n",
    "    fnr=np.sum(label_pred[:nobyz])/nobyz\n",
    "    auc = roc_auc_score(real_label, label_pred)\n",
    "    print(\"acc %0.4f; recall %0.4f; fpr %0.4f; fnr %0.4f; auc %.4f\" % (acc, recall, fpr, fnr, auc))\n",
    "    return acc, fpr, fnr, auc\n",
    "    # print(silhouette_score(score.reshape(-1, 1), label_pred))\n",
    "\n",
    "def detection1(score, nobyz):\n",
    "    nrefs = 10\n",
    "    ks = range(1, 8)\n",
    "    gaps = np.zeros(len(ks))\n",
    "    gapDiff = np.zeros(len(ks) - 1)\n",
    "    sdk = np.zeros(len(ks))\n",
    "    min = np.min(score)\n",
    "    max = np.max(score)\n",
    "    score = (score - min)/(max-min)\n",
    "    for i, k in enumerate(ks):\n",
    "        estimator = KMeans(n_clusters=k)\n",
    "        estimator.fit(score.reshape(-1, 1))\n",
    "        label_pred = estimator.labels_\n",
    "        center = estimator.cluster_centers_\n",
    "        Wk = np.sum([np.square(score[m]-center[label_pred[m]]) for m in range(len(score))])\n",
    "        WkRef = np.zeros(nrefs)\n",
    "        for j in range(nrefs):\n",
    "            rand = np.random.uniform(0, 1, len(score))\n",
    "            estimator = KMeans(n_clusters=k)\n",
    "            estimator.fit(rand.reshape(-1, 1))\n",
    "            label_pred = estimator.labels_\n",
    "            center = estimator.cluster_centers_\n",
    "            WkRef[j] = np.sum([np.square(rand[m]-center[label_pred[m]]) for m in range(len(rand))])\n",
    "        gaps[i] = np.log(np.mean(WkRef)) - np.log(Wk)\n",
    "        sdk[i] = np.sqrt((1.0 + nrefs) / nrefs) * np.std(np.log(WkRef))\n",
    "\n",
    "        if i > 0:\n",
    "            gapDiff[i - 1] = gaps[i - 1] - gaps[i] + sdk[i]\n",
    "    #print(gapDiff)\n",
    "    for i in range(len(gapDiff)):\n",
    "        if gapDiff[i] >= 0:\n",
    "            select_k = i+1\n",
    "            break\n",
    "    if select_k == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        # print('Attack Detected!')\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8b14be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(train_data, labels, model, optimizer, batch_size=20):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    len_t = (len(train_data) // batch_size)\n",
    "    if len(train_data)%batch_size:\n",
    "        len_t += 1\n",
    "    r=np.arange(len(train_data))\n",
    "    np.random.shuffle(r)\n",
    "    train_data = train_data[r]\n",
    "    labels = labels[r]\n",
    "    for ind in range(len_t):\n",
    "        inputs = train_data[ind * batch_size:(ind + 1) * batch_size].cuda()\n",
    "        targets = labels[ind * batch_size:(ind + 1) * batch_size].cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "\n",
    "def test(test_data, labels, model, criterion, use_cuda, debug_='MEDIUM', batch_size=64):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    model.eval()\n",
    "    len_t = (len(test_data) // batch_size)\n",
    "    if len(test_data)%batch_size:\n",
    "        len_t += 1\n",
    "    with torch.no_grad():\n",
    "        for ind in range(len_t):\n",
    "            inputs = test_data[ind * batch_size:(ind + 1) * batch_size].cuda()\n",
    "            targets = labels[ind * batch_size:(ind + 1) * batch_size].cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ace04bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = True\n",
    "\n",
    "if distribution=='fang':\n",
    "    each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_train_data(all_data, num_workers=100, bias=param)\n",
    "elif distribution == 'dirichlet':\n",
    "    each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_data_dirichlet(all_data, num_workers, alpha=param, force=force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "425ded7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f651f80",
   "metadata": {},
   "source": [
    "# Histogram of number of samples / client for Fang and Dirichlet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0c9c9db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.,  5., 12., 16., 20., 15., 11., 10.,  7.,  1.]),\n",
       " array([534. , 547.6, 561.2, 574.8, 588.4, 602. , 615.6, 629.2, 642.8,\n",
       "        656.4, 670. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_lens_dirichlet = [len(d) for d in each_worker_data]\n",
    "# plt.hist(d_lens)\n",
    "plt.hist(d_lens_dirichlet, alpha=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1872a5ef",
   "metadata": {},
   "source": [
    "## Evaluate FLD using the trim attack in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6d0726b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 | val_loss 2.068 val acc 47.287 | best val_acc 47.287\n",
      "e 5 | val_loss 0.255 val acc 92.753 | best val_acc 92.753\n",
      "Attack Detected!\n",
      "Stop at iteration: 10\n",
      "acc 0.9900; recall 1.0000; fpr 0.0125; fnr 0.0000;\n",
      "0.949640709729089\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "n_malicious = [5, 10, 15, 20, 25]\n",
    "\n",
    "for n_mal in n_malicious:\n",
    "    torch.cuda.empty_cache()\n",
    "    # FLD initializations\n",
    "    weight_record = []\n",
    "    grad_record = []\n",
    "    old_grad_list = []\n",
    "    malicious_scores = np.zeros((1, num_workers))\n",
    "    start_detection_epoch = 5\n",
    "    window_size = 5\n",
    "    assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "\n",
    "    good_distance_rage = np.zeros((1, nbyz))\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    # Attack initializations\n",
    "    attack_type = 'full_trim'\n",
    "    dev_type = 'unit_vec'\n",
    "    nbyz = n_mal ################### Changing variable ###################\n",
    "\n",
    "    # FL training initializations\n",
    "    num_workers = 100\n",
    "    use_cuda = True\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    local_lr=0.01\n",
    "    local_batch_size=32\n",
    "    local_epochs=2\n",
    "    global_lr=1\n",
    "\n",
    "    # FL model initializations\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    model_received = []\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "    for e in range(50):\n",
    "        user_grads=[]\n",
    "        round_clients = np.arange(num_workers)\n",
    "        round_benign = round_clients\n",
    "        user_grads=[]\n",
    "\n",
    "        for i in round_benign:\n",
    "            model = copy.deepcopy(fed_model)\n",
    "            optimizer = optim.SGD(model.parameters(), lr = lr, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "            for epoch in range(local_epochs):\n",
    "                train_loss, train_acc = train(\n",
    "                    each_worker_data[i].reshape(-1, 1, 28, 28),\n",
    "                    torch.Tensor(each_worker_label[i]).long(), model, optimizer, local_batch_size)\n",
    "\n",
    "            params = []\n",
    "            for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                    (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "            update =  (params - model_received)\n",
    "            user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "        weight = model_received\n",
    "\n",
    "        if (e > start_detection_epoch):\n",
    "            hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "            hvp = np.squeeze(hvp)\n",
    "        else:\n",
    "            hvp = None\n",
    "\n",
    "        good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "        if hvp is not None:\n",
    "            pred_grad = copy.deepcopy(good_old_grads)\n",
    "            distance = []\n",
    "            for i in range(len(good_old_grads)):\n",
    "                pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "            good_distance_rage = np.concatenate(\n",
    "                (good_distance_rage, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "        if attack_type != 'none' and (e < start_detection_epoch):\n",
    "            user_grads = full_trim(user_grads, nbyz)\n",
    "            # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "        elif e > start_detection_epoch:\n",
    "            if attack_type == 'full_trim':\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "            elif attack_type == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                if attack_type == 'LIE':\n",
    "                    print(\"LIE\")\n",
    "                    z = 0.1\n",
    "                    noise_avg = torch.mean(user_grads[:nbyz], dim=0)\n",
    "                    noise_std = torch.std(user_grads[:nbyz], dim=0)\n",
    "                    noise = noise_avg + z * noise_std\n",
    "                elif attack_type == 'NDSS21':\n",
    "                    distance_bound = np.random.choice(np.mean(good_distance_rage[-1:], 0))\n",
    "                    model_re = torch.mean(good_current_grads, dim=0)\n",
    "                    if dev_type == 'unit_vec':\n",
    "                        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                    elif dev_type == 'sign':\n",
    "                        deviation = torch.sign(model_re)\n",
    "                    elif dev_type == 'std':\n",
    "                        deviation = torch.std(good_current_grads, 0)\n",
    "                    # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_rage[-1]))) / torch.norm(deviation))\n",
    "                    noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                elif attack_type == 'mod_trim':\n",
    "                    mal_grads= full_trim(user_grads[:nbyz], nbyz)\n",
    "                    pass\n",
    "                else:\n",
    "                    noise = torch.zeros(hvp.shape).to(device)\n",
    "                for m in range(nbyz):\n",
    "                    user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "        agg_grads, distance = trimmed_mean(old_grad_list, user_grads, nbyz, hvp)\n",
    "\n",
    "        if distance is not None and e > (start_detection_epoch - window_size):\n",
    "            malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "        if malicious_scores.shape[0] >= window_size+1:\n",
    "            if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                print('Stop at iteration:', e)\n",
    "                detection(np.sum(malicious_scores[-window_size:], axis=0), nbyz, num_workers)\n",
    "                break\n",
    "\n",
    "        if e > (start_detection_epoch - window_size):\n",
    "            weight_record.append(weight - last_weight)\n",
    "            grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "        if (len(weight_record) > window_size):\n",
    "            del weight_record[0]\n",
    "            del grad_record[0]\n",
    "\n",
    "        last_weight = weight\n",
    "        last_grad = agg_grads\n",
    "        old_grad_list = user_grads\n",
    "        good_old_grads = good_current_grads\n",
    "\n",
    "        del user_grads\n",
    "        model_received = model_received + global_lr * agg_grads\n",
    "        fed_model = cnn().to(device)\n",
    "        fed_model.apply(init_weights)\n",
    "        start_idx=0\n",
    "        state_dict = {}\n",
    "        previous_name = 'none'\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "            start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "            params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "            state_dict[name] = params\n",
    "            previous_name = name\n",
    "\n",
    "        fed_model.load_state_dict(state_dict)\n",
    "        val_loss, val_acc = test(global_test_data.reshape(-1,1,28,28), global_test_label.long(), fed_model, criterion, use_cuda, batch_size=50)\n",
    "        is_best = best_val_acc < val_acc\n",
    "        best_val_acc = max(best_val_acc, val_acc)\n",
    "        if e%5==0 or e==99:\n",
    "            print('e %d | val_loss %.3f val acc %.3f | best val_acc %.3f' % (e, val_loss, val_acc, best_val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a6b71",
   "metadata": {},
   "source": [
    "## Adaptive attack on FLD-trmean + Fang-0.5 distribution + m = {5, 10, 15, 20, 25}%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bb873af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 | val_loss 2.308 val acc 6.919 | best val_acc 6.919\n",
      "e 5 | val_loss 2.310 val acc 6.680 | best val_acc 6.919\n",
      "e 10 | val_loss 2.310 val acc 6.760 | best val_acc 6.919\n",
      "e 15 | val_loss 2.303 val acc 7.556 | best val_acc 7.556\n",
      "e 20 | val_loss 2.297 val acc 8.462 | best val_acc 8.462\n",
      "e 25 | val_loss 2.290 val acc 9.796 | best val_acc 9.796\n",
      "e 30 | val_loss 2.284 val acc 11.299 | best val_acc 11.299\n",
      "e 35 | val_loss 2.278 val acc 12.683 | best val_acc 12.683\n",
      "e 40 | val_loss 2.272 val acc 13.907 | best val_acc 13.907\n",
      "e 45 | val_loss 2.265 val acc 15.689 | best val_acc 15.689\n",
      "e 50 | val_loss 2.259 val acc 17.133 | best val_acc 17.133\n",
      "e 55 | val_loss 2.253 val acc 18.128 | best val_acc 18.128\n",
      "e 60 | val_loss 2.247 val acc 19.124 | best val_acc 19.124\n",
      "e 65 | val_loss 2.240 val acc 19.960 | best val_acc 19.960\n",
      "e 70 | val_loss 2.234 val acc 21.523 | best val_acc 21.523\n",
      "e 75 | val_loss 2.227 val acc 23.654 | best val_acc 23.654\n",
      "e 80 | val_loss 2.220 val acc 25.963 | best val_acc 25.963\n",
      "e 85 | val_loss 2.213 val acc 27.904 | best val_acc 27.904\n",
      "e 90 | val_loss 2.205 val acc 29.268 | best val_acc 29.268\n",
      "e 95 | val_loss 2.198 val acc 30.582 | best val_acc 30.582\n",
      "e 100 | val_loss 2.190 val acc 31.757 | best val_acc 31.757\n",
      "Attack Detected!\n",
      "Stop at iteration: 103\n",
      "acc 0.7000; recall 0.0000; fpr 0.1250; fnr 1.0000;\n",
      "0.8382403084517145\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SLOW BASELINE\n",
    "'''\n",
    "# delete all FLD variables\n",
    "del weight_record, grad_record, old_grad_list, num_workers, malicious_scores\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "weight_record = []\n",
    "grad_record = []\n",
    "old_grad_list = []\n",
    "\n",
    "num_workers = 100\n",
    "malicious_scores = np.zeros((1, num_workers))\n",
    "\n",
    "attack_type = 'NDSS21'\n",
    "dev_type = 'unit_vec'\n",
    "use_cuda = True\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "\n",
    "lr = 0.005\n",
    "start_detection_epoch = 10\n",
    "window_size = 10\n",
    "assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "nbyz = 20\n",
    "good_distance_rage = np.zeros((1, nbyz))\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for e in range(1000):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].reshape(-1, 1, 28, 28).cuda())\n",
    "        loss = criterion(output, each_worker_label[i].long().cuda())\n",
    "        \n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "    \n",
    "    tmp = []\n",
    "    for param in net.parameters():\n",
    "        tmp = param.data.view(-1) if not len(tmp) else torch.cat((tmp, param.data.view(-1)))\n",
    "    weight = tmp\n",
    "\n",
    "    if (e > start_detection_epoch):\n",
    "        hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "        hvp = np.squeeze(hvp)\n",
    "    else:\n",
    "        hvp = None\n",
    "\n",
    "    good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "    if hvp is not None:\n",
    "        pred_grad = copy.deepcopy(good_old_grads)\n",
    "        distance = []\n",
    "        for i in range(len(good_old_grads)):\n",
    "            pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "        good_distance_rage = np.concatenate(\n",
    "            (good_distance_rage, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "        \n",
    "    if attack_type != 'none' and (e < start_detection_epoch):\n",
    "        user_grads = full_trim(user_grads, nbyz)\n",
    "    elif e > start_detection_epoch:\n",
    "        if attack_type == 'full_trim':\n",
    "            user_grads = full_trim(user_grads, nbyz)\n",
    "        elif attack_type == 'none':\n",
    "            pass\n",
    "        else:\n",
    "            if attack_type == 'LIE':\n",
    "                print(\"LIE\")\n",
    "                z = 0.1\n",
    "                noise_avg = torch.mean(user_grads[:nbyz], dim=0)\n",
    "                noise_std = torch.std(user_grads[:nbyz], dim=0)\n",
    "                noise = noise_avg + z * noise_std\n",
    "            elif attack_type == 'NDSS21':\n",
    "                distance_bound = np.random.choice(np.mean(good_distance_rage[-1:], 0))\n",
    "                model_re = torch.mean(good_current_grads, dim=0)\n",
    "                if dev_type == 'unit_vec':\n",
    "                    deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                elif dev_type == 'sign':\n",
    "                    deviation = torch.sign(model_re)\n",
    "                elif dev_type == 'std':\n",
    "                    deviation = torch.std(good_current_grads, 0)\n",
    "                # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_rage[-1]))) / torch.norm(deviation))\n",
    "                noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "            elif attack_type == 'mod_trim':\n",
    "                mal_grads= full_trim(user_grads[:nbyz], nbyz)\n",
    "                pass\n",
    "            else:\n",
    "                noise = torch.zeros(hvp.shape).to(device)\n",
    "            for m in range(nbyz):\n",
    "                user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "    agg_grads, distance = trimmed_mean(old_grad_list, user_grads, nbyz, hvp)\n",
    "    \n",
    "    if distance is not None and e > (start_detection_epoch - window_size):\n",
    "        malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "    if malicious_scores.shape[0] >= window_size+1:\n",
    "        if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "            print('Stop at iteration:', e)\n",
    "            detection(np.sum(malicious_scores[-window_size:], axis=0), nbyz, num_workers)\n",
    "            break\n",
    "\n",
    "    if e > (start_detection_epoch - window_size):\n",
    "        weight_record.append(weight - last_weight)\n",
    "        grad_record.append(agg_grads - last_grad)\n",
    "    \n",
    "    if (len(weight_record) > window_size):\n",
    "        del weight_record[0]\n",
    "        del grad_record[0]\n",
    "    \n",
    "    last_weight = weight\n",
    "    last_grad = agg_grads\n",
    "    old_grad_list = user_grads\n",
    "    good_old_grads = good_current_grads\n",
    "\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    val_loss, val_acc = test(global_test_data.reshape(-1,1,28,28), global_test_label.long(), net, criterion, use_cuda, batch_size=50)\n",
    "    is_best = best_val_acc < val_acc\n",
    "    best_val_acc = max(best_val_acc, val_acc)\n",
    "    if e%5==0 or e==999:\n",
    "        print('e %d | val_loss %.3f val acc %.3f | best val_acc %.3f' % (e, val_loss, val_acc, best_val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3906b",
   "metadata": {},
   "source": [
    "# Slow baseline results\n",
    "\n",
    "## NDSS21 + sign\n",
    "### run 1\n",
    "Attack Detected!\n",
    "Stop at iteration: 113\n",
    "acc 0.6500; recall 0.0000; fpr 0.3158; fnr 1.0000;\n",
    "0.7177836650168169\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 125\n",
    "acc 0.6200; recall 0.0000; fpr 0.3111; fnr 1.0000;\n",
    "0.7041198588150981\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 152\n",
    "acc 0.4800; recall 0.0000; fpr 0.4353; fnr 1.0000;\n",
    "0.6699105149889638\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 26\n",
    "acc 0.2900; recall 0.0000; fpr 0.6375; fnr 1.0000;\n",
    "0.6881105605642764\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 43\n",
    "acc 0.6600; recall 1.0000; fpr 0.4533; fnr 0.0000;\n",
    "0.6907586272058862\n",
    "\n",
    "### run 2\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 97\n",
    "acc 0.5500; recall 0.0000; fpr 0.4211; fnr 1.0000;\n",
    "0.6683849381079955\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 20\n",
    "acc 0.7100; recall 1.0000; fpr 0.3222; fnr 0.0000;\n",
    "0.7064875030402401\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 21\n",
    "acc 0.6400; recall 1.0000; fpr 0.4235; fnr 0.0000;\n",
    "0.6721461157406335\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 101\n",
    "acc 0.2800; recall 0.0000; fpr 0.6500; fnr 1.0000;\n",
    "0.6459235493686176\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 26\n",
    "acc 0.8100; recall 1.0000; fpr 0.2533; fnr 0.0000;\n",
    "0.7027147928146711\n",
    "\n",
    "### run 3\n",
    "Attack Detected!\n",
    "Stop at iteration: 97\n",
    "acc 0.5500; recall 0.0000; fpr 0.4211; fnr 1.0000;\n",
    "0.6683849381079955\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 20\n",
    "acc 0.7100; recall 1.0000; fpr 0.3222; fnr 0.0000;\n",
    "0.7064875030402401\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 21\n",
    "acc 0.6400; recall 1.0000; fpr 0.4235; fnr 0.0000;\n",
    "0.6721461157406335\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 101\n",
    "acc 0.2800; recall 0.0000; fpr 0.6500; fnr 1.0000;\n",
    "0.6459235493686176\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 26\n",
    "acc 0.8100; recall 1.0000; fpr 0.2533; fnr 0.0000;\n",
    "0.7027147928146711\n",
    "\n",
    "\n",
    "### run 4*\n",
    "* (distance_bound = np.mean(good_distances))\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 109\n",
    "acc 0.5300; recall 0.0000; fpr 0.4421; fnr 1.0000;\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 105\n",
    "acc 0.4800; recall 0.0000; fpr 0.4667; fnr 1.0000;\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 128\n",
    "acc 0.5100; recall 0.0000; fpr 0.4000; fnr 1.0000;\n",
    "\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 120\n",
    "acc 0.5000; recall 0.0000; fpr 0.3750; fnr 1.0000;\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 34\n",
    "acc 0.5300; recall 0.0000; fpr 0.2933; fnr 1.0000;\n",
    "\n",
    "### run 5*\n",
    "Attack Detected!\n",
    "Stop at iteration: 129\n",
    "acc 0.5500; recall 0.0000; fpr 0.4211; fnr 1.0000;\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 125\n",
    "acc 0.5400; recall 0.0000; fpr 0.4000; fnr 1.0000;\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 132\n",
    "acc 0.5500; recall 0.0000; fpr 0.3529; fnr 1.0000;\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 52\n",
    "acc 0.3700; recall 0.0000; fpr 0.5375; fnr 1.0000;\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 107\n",
    "acc 0.1500; recall 0.0000; fpr 0.8000; fnr 1.0000;\n",
    "\n",
    "### run 6*\n",
    "Attack Detected!\n",
    "Stop at iteration: 124\n",
    "acc 0.5500; recall 0.0000; fpr 0.3889; fnr 1.0000;\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 122\n",
    "acc 0.0500; recall 0.0000; fpr 0.9412; fnr 1.0000;\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 30\n",
    "acc 0.5300; recall 0.0000; fpr 0.3375; fnr 1.0000;\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 20\n",
    "acc 0.9300; recall 1.0000; fpr 0.0933; fnr 0.0000;\n",
    "\n",
    "## NDSS21 + unit_vec\n",
    "### run 1\n",
    "Attack Detected!\n",
    "Stop at iteration: 119\n",
    "acc 0.2500; recall 0.0000; fpr 0.7368; fnr 1.0000;\n",
    "0.6906284998384221\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 47\n",
    "acc 0.7200; recall 1.0000; fpr 0.3111; fnr 0.0000;\n",
    "0.6657499192267171\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 20\n",
    "acc 0.9900; recall 1.0000; fpr 0.0118; fnr 0.0000;\n",
    "0.7902476191649904\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 45\n",
    "acc 0.4900; recall 0.0000; fpr 0.3875; fnr 1.0000;\n",
    "0.6903751972083814\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 80\n",
    "acc 0.0100; recall 0.0000; fpr 0.9867; fnr 1.0000;\n",
    "0.7476418065819977\n",
    "\n",
    "### run 2\n",
    "Attack Detected!\n",
    "Stop at iteration: 105\n",
    "acc 0.6300; recall 0.0000; fpr 0.3368; fnr 1.0000;\n",
    "0.6776102750332995\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 88\n",
    "acc 0.1100; recall 0.0000; fpr 0.8778; fnr 1.0000;\n",
    "0.7312029089370341\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 21\n",
    "acc 0.6700; recall 1.0000; fpr 0.3882; fnr 0.0000;\n",
    "0.6671735632726175\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 38\n",
    "acc 0.4900; recall 0.0000; fpr 0.3875; fnr 1.0000;\n",
    "0.6915442086050451\n",
    "\n",
    "Attack Detected!\n",
    "Stop at iteration: 50\n",
    "acc 0.4100; recall 0.0000; fpr 0.4533; fnr 1.0000;\n",
    "0.6857216699946427\n",
    "\n",
    "### run 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edb99383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 | val_loss 2.317 val acc 9.613 | best val_acc 9.613\n",
      "e 20 | val_loss 2.234 val acc 29.784 | best val_acc 29.784\n",
      "e 40 | val_loss 2.144 val acc 49.726 | best val_acc 49.726\n",
      "e 60 | val_loss 2.010 val acc 69.110 | best val_acc 69.110\n",
      "e 80 | val_loss 1.795 val acc 74.669 | best val_acc 74.669\n",
      "e 100 | val_loss 1.475 val acc 77.368 | best val_acc 77.368\n",
      "Attack Detected!\n",
      "Stop at iteration: 109\n",
      "acc 0.5300; recall 0.0000; fpr 0.4421; fnr 1.0000;\n",
      "0.6457644764619055\n",
      "e 0 | val_loss 2.306 val acc 12.780 | best val_acc 12.780\n",
      "e 20 | val_loss 2.240 val acc 26.138 | best val_acc 26.138\n",
      "e 40 | val_loss 2.135 val acc 45.333 | best val_acc 45.333\n",
      "e 60 | val_loss 1.971 val acc 65.136 | best val_acc 65.136\n",
      "e 80 | val_loss 1.726 val acc 70.495 | best val_acc 70.495\n",
      "e 100 | val_loss 1.400 val acc 74.968 | best val_acc 74.968\n",
      "Attack Detected!\n",
      "Stop at iteration: 105\n",
      "acc 0.4800; recall 0.0000; fpr 0.4667; fnr 1.0000;\n",
      "0.6327764486758649\n",
      "e 0 | val_loss 2.333 val acc 9.712 | best val_acc 9.712\n",
      "e 20 | val_loss 2.267 val acc 13.328 | best val_acc 13.328\n",
      "e 40 | val_loss 2.191 val acc 29.794 | best val_acc 29.794\n",
      "e 60 | val_loss 2.080 val acc 51.659 | best val_acc 51.659\n",
      "e 80 | val_loss 1.901 val acc 68.503 | best val_acc 68.503\n",
      "e 100 | val_loss 1.617 val acc 75.884 | best val_acc 75.884\n",
      "e 120 | val_loss 1.252 val acc 79.410 | best val_acc 79.410\n",
      "Attack Detected!\n",
      "Stop at iteration: 128\n",
      "acc 0.5100; recall 0.0000; fpr 0.4000; fnr 1.0000;\n",
      "0.6762050445101271\n",
      "e 0 | val_loss 2.309 val acc 10.639 | best val_acc 10.639\n",
      "e 20 | val_loss 2.274 val acc 12.372 | best val_acc 12.372\n",
      "e 40 | val_loss 2.174 val acc 29.246 | best val_acc 29.246\n",
      "e 60 | val_loss 2.043 val acc 57.954 | best val_acc 57.954\n",
      "e 80 | val_loss 1.837 val acc 65.903 | best val_acc 65.903\n",
      "e 100 | val_loss 1.542 val acc 71.710 | best val_acc 71.710\n",
      "Attack Detected!\n",
      "Stop at iteration: 120\n",
      "acc 0.5000; recall 0.0000; fpr 0.3750; fnr 1.0000;\n",
      "0.703277489432419\n",
      "e 0 | val_loss 2.341 val acc 9.493 | best val_acc 9.493\n",
      "e 20 | val_loss 2.297 val acc 12.232 | best val_acc 12.232\n",
      "Attack Detected!\n",
      "Stop at iteration: 34\n",
      "acc 0.5300; recall 0.0000; fpr 0.2933; fnr 1.0000;\n",
      "0.7383327018005252\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SLOW BASELINE\n",
    "'''\n",
    "n_malicious = [5, 10, 15, 20, 25]\n",
    "\n",
    "for n_mal in n_malicious:\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "#     # delete all variables\n",
    "#     del weight_record, grad_record, old_grad_list, num_workers, malicious_scores, start_detection_epoch, window_size\n",
    "#     del good_distance_rage\n",
    "#     del nepochs, num_workers, local_lr, local_epochs, global_lr, agr\n",
    "#     del attack_type, dev_type, fed_model\n",
    "\n",
    "    # FL training initializations\n",
    "    nepochs=1000\n",
    "    num_workers = 100\n",
    "    use_cuda = True\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    local_lr=1\n",
    "    global_lr=0.01\n",
    "    agr='tr_mean' # ['average', 'median', 'tr_mean']\n",
    "\n",
    "    # Attack initializations\n",
    "    attack_type = 'NDSS21'\n",
    "    dev_type = 'sign'\n",
    "    nbyz = n_mal ################### Changing variable ###################\n",
    "\n",
    "    # FLD initializations\n",
    "    weight_record = []\n",
    "    grad_record = []\n",
    "    old_grad_list = []\n",
    "    malicious_scores = np.zeros((1, num_workers))\n",
    "    start_detection_epoch = 10\n",
    "    window_size = 10\n",
    "    assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "    good_distance_rage = np.zeros((1, nbyz))\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    # FL model initializations\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    \n",
    "    for e in range(nepochs):\n",
    "        cnn_optimizer = SGD(fed_model.parameters(), lr = global_lr)\n",
    "        user_grads=[]\n",
    "        round_clients = np.arange(num_workers)\n",
    "        round_benign = round_clients\n",
    "        for i in range(num_workers):\n",
    "            net_ = copy.deepcopy(fed_model)\n",
    "            net_.zero_grad()\n",
    "            output = net_(each_worker_data[i].reshape(-1, 1, 28, 28).cuda())\n",
    "            loss = criterion(output, each_worker_label[i].long().cuda())\n",
    "            loss.backward(retain_graph = True)\n",
    "            param_grad=[]\n",
    "            for param in net_.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "            del net_\n",
    "\n",
    "        weight = []\n",
    "        for param in fed_model.parameters():\n",
    "            weight = param.data.view(-1) if not len(weight) else torch.cat((weight, param.data.view(-1)))\n",
    "\n",
    "        if (e > start_detection_epoch):\n",
    "            hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "            hvp = np.squeeze(hvp)\n",
    "        else:\n",
    "            hvp = None\n",
    "\n",
    "        good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "        if hvp is not None:\n",
    "            pred_grad = copy.deepcopy(good_old_grads)\n",
    "            distance = []\n",
    "            for i in range(len(good_old_grads)):\n",
    "                pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "            good_distance_rage = np.concatenate(\n",
    "                (good_distance_rage, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "        if attack_type != 'none' and (e < start_detection_epoch):\n",
    "            user_grads = full_trim(user_grads, nbyz)\n",
    "            # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "        elif e > start_detection_epoch:\n",
    "            if attack_type == 'full_trim':\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "            elif attack_type == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                if attack_type == 'NDSS21':\n",
    "                    model_re = torch.mean(good_current_grads, dim=0)\n",
    "                    if dev_type == 'unit_vec':\n",
    "                        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                    elif dev_type == 'sign':\n",
    "                        deviation = torch.sign(model_re)\n",
    "                    elif dev_type == 'std':\n",
    "                        deviation = torch.std(good_current_grads, 0)\n",
    "                    # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_rage[-1]))) / torch.norm(deviation))\n",
    "                    \n",
    "                else:\n",
    "                    noise = torch.zeros(hvp.shape).to(device)\n",
    "                for m in range(nbyz):\n",
    "                    distance_bound = np.mean(np.mean(good_distance_rage[-1:], 0))\n",
    "                    noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                    user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "        agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "        if distance is not None and e > (start_detection_epoch - window_size):\n",
    "            malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "        if malicious_scores.shape[0] >= window_size+1:\n",
    "            if e == window_size + 1: print('shape of malicious scores: ', malicious_scores.shape)\n",
    "            if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                print('Stop at iteration:', e)\n",
    "                detection(np.sum(malicious_scores[-window_size:], axis=0), nbyz, num_workers)\n",
    "                break\n",
    "\n",
    "        if e > (start_detection_epoch - window_size):\n",
    "            weight_record.append(weight - last_weight)\n",
    "            grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "        if (len(weight_record) > window_size):\n",
    "            del weight_record[0]\n",
    "            del grad_record[0]\n",
    "\n",
    "        last_weight = weight\n",
    "        last_grad = agg_grads\n",
    "        old_grad_list = user_grads\n",
    "        good_old_grads = good_current_grads\n",
    "\n",
    "        del user_grads\n",
    "        start_idx=0\n",
    "        cnn_optimizer.zero_grad()\n",
    "        model_grads=[]\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "        cnn_optimizer.step(model_grads)\n",
    "        val_loss, val_acc = test(global_test_data.reshape(-1,1,28,28), global_test_label.long(), fed_model, criterion, use_cuda, batch_size=50)\n",
    "        is_best = best_val_acc < val_acc\n",
    "        best_val_acc = max(best_val_acc, val_acc)\n",
    "        if e%20==0 or e==nepochs-1:\n",
    "            print('e %d | val_loss %.3f val acc %.3f | best val_acc %.3f' % (e, val_loss, val_acc, best_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea868c2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 | val_loss 2.320 val acc 10.001 | best val_acc 10.001\n",
      "e 20 | val_loss 2.252 val acc 10.539 | best val_acc 10.539\n",
      "e 40 | val_loss 2.183 val acc 31.995 | best val_acc 31.995\n",
      "e 60 | val_loss 2.085 val acc 60.922 | best val_acc 60.922\n",
      "e 80 | val_loss 1.932 val acc 72.348 | best val_acc 72.348\n",
      "e 100 | val_loss 1.692 val acc 76.372 | best val_acc 76.372\n",
      "e 120 | val_loss 1.364 val acc 79.161 | best val_acc 79.161\n",
      "Attack Detected!\n",
      "Stop at iteration: 129\n",
      "acc 0.5500; recall 0.0000; fpr 0.4211; fnr 1.0000;\n",
      "0.6694340981362383\n",
      "e 0 | val_loss 2.315 val acc 5.907 | best val_acc 5.907\n",
      "e 20 | val_loss 2.248 val acc 17.681 | best val_acc 17.681\n",
      "e 40 | val_loss 2.163 val acc 46.060 | best val_acc 46.060\n",
      "e 60 | val_loss 2.054 val acc 63.323 | best val_acc 63.323\n",
      "e 80 | val_loss 1.893 val acc 70.674 | best val_acc 70.674\n",
      "e 100 | val_loss 1.655 val acc 74.410 | best val_acc 74.410\n",
      "e 120 | val_loss 1.346 val acc 77.757 | best val_acc 77.757\n",
      "Attack Detected!\n",
      "Stop at iteration: 125\n",
      "acc 0.5400; recall 0.0000; fpr 0.4000; fnr 1.0000;\n",
      "0.7065963348533192\n",
      "e 0 | val_loss 2.309 val acc 8.826 | best val_acc 8.826\n",
      "e 20 | val_loss 2.266 val acc 18.548 | best val_acc 18.548\n",
      "e 40 | val_loss 2.195 val acc 31.079 | best val_acc 31.079\n",
      "e 60 | val_loss 2.100 val acc 59.418 | best val_acc 59.418\n",
      "e 80 | val_loss 1.950 val acc 68.602 | best val_acc 68.602\n",
      "e 100 | val_loss 1.722 val acc 72.428 | best val_acc 72.428\n",
      "e 120 | val_loss 1.413 val acc 75.037 | best val_acc 75.037\n",
      "Attack Detected!\n",
      "Stop at iteration: 132\n",
      "acc 0.5500; recall 0.0000; fpr 0.3529; fnr 1.0000;\n",
      "0.6760074519434771\n",
      "e 0 | val_loss 2.323 val acc 10.897 | best val_acc 10.897\n",
      "e 20 | val_loss 2.268 val acc 10.230 | best val_acc 10.907\n",
      "e 40 | val_loss 2.148 val acc 44.805 | best val_acc 44.805\n",
      "Attack Detected!\n",
      "Stop at iteration: 52\n",
      "acc 0.3700; recall 0.0000; fpr 0.5375; fnr 1.0000;\n",
      "0.6889612468374652\n",
      "e 0 | val_loss 2.313 val acc 3.616 | best val_acc 3.616\n",
      "e 20 | val_loss 2.302 val acc 3.765 | best val_acc 3.765\n",
      "e 40 | val_loss 2.231 val acc 20.590 | best val_acc 20.590\n",
      "e 60 | val_loss 2.126 val acc 53.292 | best val_acc 53.292\n",
      "e 80 | val_loss 1.960 val acc 58.074 | best val_acc 58.074\n",
      "e 100 | val_loss 1.697 val acc 64.439 | best val_acc 64.439\n",
      "Attack Detected!\n",
      "Stop at iteration: 107\n",
      "acc 0.1500; recall 0.0000; fpr 0.8000; fnr 1.0000;\n",
      "0.6583212622182559\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SLOW BASELINE\n",
    "'''\n",
    "n_malicious = [5, 10, 15, 20, 25]\n",
    "\n",
    "for n_mal in n_malicious:\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "#     # delete all variables\n",
    "#     del weight_record, grad_record, old_grad_list, num_workers, malicious_scores, start_detection_epoch, window_size\n",
    "#     del good_distance_rage\n",
    "#     del nepochs, num_workers, local_lr, local_epochs, global_lr, agr\n",
    "#     del attack_type, dev_type, fed_model\n",
    "\n",
    "    # FL training initializations\n",
    "    nepochs=1000\n",
    "    num_workers = 100\n",
    "    use_cuda = True\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    local_lr=1\n",
    "    global_lr=0.01\n",
    "    agr='tr_mean' # ['average', 'median', 'tr_mean']\n",
    "\n",
    "    # Attack initializations\n",
    "    attack_type = 'NDSS21'\n",
    "    dev_type = 'sign'\n",
    "    nbyz = n_mal ################### Changing variable ###################\n",
    "\n",
    "    # FLD initializations\n",
    "    weight_record = []\n",
    "    grad_record = []\n",
    "    old_grad_list = []\n",
    "    malicious_scores = np.zeros((1, num_workers))\n",
    "    start_detection_epoch = 10\n",
    "    window_size = 10\n",
    "    assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "    good_distance_rage = np.zeros((1, nbyz))\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    # FL model initializations\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    \n",
    "    for e in range(nepochs):\n",
    "        cnn_optimizer = SGD(fed_model.parameters(), lr = global_lr)\n",
    "        user_grads=[]\n",
    "        round_clients = np.arange(num_workers)\n",
    "        round_benign = round_clients\n",
    "        for i in range(num_workers):\n",
    "            net_ = copy.deepcopy(fed_model)\n",
    "            net_.zero_grad()\n",
    "            output = net_(each_worker_data[i].reshape(-1, 1, 28, 28).cuda())\n",
    "            loss = criterion(output, each_worker_label[i].long().cuda())\n",
    "            loss.backward(retain_graph = True)\n",
    "            param_grad=[]\n",
    "            for param in net_.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "            del net_\n",
    "\n",
    "        weight = []\n",
    "        for param in fed_model.parameters():\n",
    "            weight = param.data.view(-1) if not len(weight) else torch.cat((weight, param.data.view(-1)))\n",
    "\n",
    "        if (e > start_detection_epoch):\n",
    "            hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "            hvp = np.squeeze(hvp)\n",
    "        else:\n",
    "            hvp = None\n",
    "\n",
    "        good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "        if hvp is not None:\n",
    "            pred_grad = copy.deepcopy(good_old_grads)\n",
    "            distance = []\n",
    "            for i in range(len(good_old_grads)):\n",
    "                pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "            good_distance_rage = np.concatenate(\n",
    "                (good_distance_rage, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "        if attack_type != 'none' and (e < start_detection_epoch):\n",
    "            user_grads = full_trim(user_grads, nbyz)\n",
    "            # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "        elif e > start_detection_epoch:\n",
    "            if attack_type == 'full_trim':\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "            elif attack_type == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                if attack_type == 'NDSS21':\n",
    "                    model_re = torch.mean(good_current_grads, dim=0)\n",
    "                    if dev_type == 'unit_vec':\n",
    "                        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                    elif dev_type == 'sign':\n",
    "                        deviation = torch.sign(model_re)\n",
    "                    elif dev_type == 'std':\n",
    "                        deviation = torch.std(good_current_grads, 0)\n",
    "                    # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_rage[-1]))) / torch.norm(deviation))\n",
    "                    \n",
    "                else:\n",
    "                    noise = torch.zeros(hvp.shape).to(device)\n",
    "                for m in range(nbyz):\n",
    "                    distance_bound = np.mean(np.mean(good_distance_rage[-1:], 0))\n",
    "                    noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                    user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "        agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "        if distance is not None and e > (start_detection_epoch - window_size):\n",
    "            malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "        if malicious_scores.shape[0] >= window_size+1:\n",
    "            if e == window_size + 1: print('shape of malicious scores: ', malicious_scores.shape)\n",
    "            if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                print('Stop at iteration:', e)\n",
    "                detection(np.sum(malicious_scores[-window_size:], axis=0), nbyz, num_workers)\n",
    "                break\n",
    "\n",
    "        if e > (start_detection_epoch - window_size):\n",
    "            weight_record.append(weight - last_weight)\n",
    "            grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "        if (len(weight_record) > window_size):\n",
    "            del weight_record[0]\n",
    "            del grad_record[0]\n",
    "\n",
    "        last_weight = weight\n",
    "        last_grad = agg_grads\n",
    "        old_grad_list = user_grads\n",
    "        good_old_grads = good_current_grads\n",
    "\n",
    "        del user_grads\n",
    "        start_idx=0\n",
    "        cnn_optimizer.zero_grad()\n",
    "        model_grads=[]\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "        cnn_optimizer.step(model_grads)\n",
    "        val_loss, val_acc = test(global_test_data.reshape(-1,1,28,28), global_test_label.long(), fed_model, criterion, use_cuda, batch_size=50)\n",
    "        is_best = best_val_acc < val_acc\n",
    "        best_val_acc = max(best_val_acc, val_acc)\n",
    "        if e%20==0 or e==nepochs-1:\n",
    "            print('e %d | val_loss %.3f val acc %.3f | best val_acc %.3f' % (e, val_loss, val_acc, best_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc6097f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop at iteration: 113\n",
      "run 0 n_mal 5 | val_loss 1.253 val acc 81.034 | best val_acc 81.034\n",
      "acc 0.5294; recall 0.0000; fpr 0.4375; fnr 1.0000; auc 0.2812\n",
      "run 0 e 113 | val_loss 1.253 val acc 81.034 | best val_acc 81.034\n",
      "Stop at iteration: 95\n",
      "run 1 n_mal 5 | val_loss 1.275 val acc 78.424 | best val_acc 78.424\n",
      "acc 0.4824; recall 0.0000; fpr 0.4875; fnr 1.0000; auc 0.2562\n",
      "run 1 e 95 | val_loss 1.275 val acc 78.424 | best val_acc 78.424\n",
      "Stop at iteration: 128\n",
      "run 2 n_mal 5 | val_loss 1.168 val acc 81.970 | best val_acc 81.970\n",
      "acc 0.5529; recall 0.0000; fpr 0.4125; fnr 1.0000; auc 0.2938\n",
      "run 2 e 128 | val_loss 1.168 val acc 81.970 | best val_acc 81.970\n",
      "Stop at iteration: 89\n",
      "run 3 n_mal 5 | val_loss 1.595 val acc 75.804 | best val_acc 75.804\n",
      "acc 0.4824; recall 0.0000; fpr 0.4875; fnr 1.0000; auc 0.2562\n",
      "run 3 e 89 | val_loss 1.595 val acc 75.804 | best val_acc 75.804\n",
      "Stop at iteration: 91\n",
      "run 4 n_mal 5 | val_loss 1.277 val acc 78.813 | best val_acc 78.813\n",
      "acc 0.3882; recall 0.0000; fpr 0.5875; fnr 1.0000; auc 0.2062\n",
      "run 4 e 91 | val_loss 1.277 val acc 78.813 | best val_acc 78.813\n",
      "Stop at iteration: 20\n",
      "run 0 n_mal 10 | val_loss 2.269 val acc 25.779 | best val_acc 25.779\n",
      "acc 0.4444; recall 0.0000; fpr 0.5000; fnr 1.0000; auc 0.2500\n",
      "run 0 e 20 | val_loss 2.269 val acc 25.779 | best val_acc 25.779\n",
      "Stop at iteration: 132\n",
      "run 1 n_mal 10 | val_loss 1.404 val acc 79.699 | best val_acc 79.699\n",
      "acc 0.5778; recall 0.0000; fpr 0.3500; fnr 1.0000; auc 0.3250\n",
      "run 1 e 132 | val_loss 1.404 val acc 79.699 | best val_acc 79.699\n",
      "Stop at iteration: 128\n",
      "run 2 n_mal 10 | val_loss 1.383 val acc 77.119 | best val_acc 77.119\n",
      "acc 0.5000; recall 0.0000; fpr 0.4375; fnr 1.0000; auc 0.2812\n",
      "run 2 e 128 | val_loss 1.383 val acc 77.119 | best val_acc 77.119\n",
      "Stop at iteration: 135\n",
      "run 3 n_mal 10 | val_loss 1.114 val acc 80.167 | best val_acc 80.167\n",
      "acc 0.5778; recall 0.0000; fpr 0.3500; fnr 1.0000; auc 0.3250\n",
      "run 3 e 135 | val_loss 1.114 val acc 80.167 | best val_acc 80.167\n",
      "Stop at iteration: 21\n",
      "run 4 n_mal 10 | val_loss 2.250 val acc 18.607 | best val_acc 18.607\n",
      "acc 0.5778; recall 0.0000; fpr 0.3500; fnr 1.0000; auc 0.3250\n",
      "run 4 e 21 | val_loss 2.250 val acc 18.607 | best val_acc 18.607\n",
      "Stop at iteration: 128\n",
      "run 0 n_mal 15 | val_loss 1.155 val acc 79.809 | best val_acc 79.809\n",
      "acc 0.5263; recall 0.0000; fpr 0.3750; fnr 1.0000; auc 0.3125\n",
      "run 0 e 128 | val_loss 1.155 val acc 79.809 | best val_acc 79.809\n",
      "Stop at iteration: 37\n",
      "run 1 n_mal 15 | val_loss 2.187 val acc 23.060 | best val_acc 23.060\n",
      "acc 0.5789; recall 1.0000; fpr 0.5000; fnr 0.0000; auc 0.7500\n",
      "run 1 e 37 | val_loss 2.187 val acc 23.060 | best val_acc 23.060\n",
      "Stop at iteration: 76\n",
      "run 2 n_mal 15 | val_loss 1.949 val acc 67.278 | best val_acc 67.278\n",
      "acc 0.4526; recall 0.0000; fpr 0.4625; fnr 1.0000; auc 0.2687\n",
      "run 2 e 76 | val_loss 1.949 val acc 67.278 | best val_acc 67.278\n",
      "Stop at iteration: 154\n",
      "run 3 n_mal 15 | val_loss 1.067 val acc 81.572 | best val_acc 81.572\n",
      "acc 0.4947; recall 0.0000; fpr 0.4125; fnr 1.0000; auc 0.2938\n",
      "run 3 e 154 | val_loss 1.067 val acc 81.572 | best val_acc 81.572\n",
      "Stop at iteration: 75\n",
      "run 4 n_mal 15 | val_loss 2.071 val acc 66.829 | best val_acc 66.829\n",
      "acc 0.2947; recall 0.0000; fpr 0.6500; fnr 1.0000; auc 0.1750\n",
      "run 4 e 75 | val_loss 2.071 val acc 66.829 | best val_acc 66.829\n",
      "Stop at iteration: 111\n",
      "run 0 n_mal 20 | val_loss 1.233 val acc 77.189 | best val_acc 77.189\n",
      "acc 0.3600; recall 0.0000; fpr 0.5500; fnr 1.0000; auc 0.2250\n",
      "run 0 e 111 | val_loss 1.233 val acc 77.189 | best val_acc 77.189\n",
      "Stop at iteration: 88\n",
      "run 1 n_mal 20 | val_loss 2.001 val acc 57.496 | best val_acc 57.496\n",
      "acc 0.3500; recall 0.0000; fpr 0.5625; fnr 1.0000; auc 0.2188\n",
      "run 1 e 88 | val_loss 2.001 val acc 57.496 | best val_acc 57.496\n",
      "Stop at iteration: 106\n",
      "run 2 n_mal 20 | val_loss 2.058 val acc 56.629 | best val_acc 56.669\n",
      "acc 0.3800; recall 0.0000; fpr 0.5250; fnr 1.0000; auc 0.2375\n",
      "run 2 e 106 | val_loss 2.058 val acc 56.629 | best val_acc 56.669\n",
      "Stop at iteration: 66\n",
      "run 3 n_mal 20 | val_loss 2.076 val acc 59.010 | best val_acc 59.010\n",
      "acc 0.4300; recall 0.0000; fpr 0.4625; fnr 1.0000; auc 0.2687\n",
      "run 3 e 66 | val_loss 2.076 val acc 59.010 | best val_acc 59.010\n",
      "Stop at iteration: 28\n",
      "run 4 n_mal 20 | val_loss 2.207 val acc 25.989 | best val_acc 25.989\n",
      "acc 0.6800; recall 0.0000; fpr 0.1500; fnr 1.0000; auc 0.4250\n",
      "run 4 e 28 | val_loss 2.207 val acc 25.989 | best val_acc 25.989\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SLOW BASELINE\n",
    "'''\n",
    "n_malicious = [5, 10, 15, 20]\n",
    "n_runs=5\n",
    "results = {}\n",
    "for n_mal in n_malicious:\n",
    "    results[n_mal] = {'acc': [], 'fpr': [], 'fnr': [], 'auc': [], 'stop_e': []}\n",
    "    for run in range(n_runs):\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            # delete all variables\n",
    "            del weight_record, grad_record, old_grad_list, num_workers, malicious_scores, start_detection_epoch, window_size\n",
    "            del good_distance_rage\n",
    "            del nepochs, num_workers, local_lr, local_epochs, global_lr, agr\n",
    "            del attack_type, dev_type, fed_model\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # FL training initializations\n",
    "        nepochs=1000\n",
    "        num_workers = 100\n",
    "        use_cuda = True\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        local_lr=1\n",
    "        global_lr=0.01\n",
    "        agr='tr_mean' # ['average', 'median', 'tr_mean']\n",
    "\n",
    "        # Attack initializations\n",
    "        attack_type = 'NDSS21'\n",
    "        dev_type = 'sign'\n",
    "        nbyz = n_mal\n",
    "\n",
    "        # FLD initializations\n",
    "        weight_record = []\n",
    "        grad_record = []\n",
    "        old_grad_list = []\n",
    "        malicious_scores = np.zeros((1, num_workers-(20-nbyz)))\n",
    "        start_detection_epoch = 10\n",
    "        window_size = 10\n",
    "        assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "        good_distance_rage = np.zeros((1, nbyz))\n",
    "        best_val_acc = 0.0\n",
    "\n",
    "        # FL model initializations\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "        fed_model = cnn().to(device)\n",
    "        fed_model.apply(init_weights)\n",
    "\n",
    "        for e in range(nepochs):\n",
    "            cnn_optimizer = SGD(fed_model.parameters(), lr = global_lr)\n",
    "            user_grads=[]\n",
    "\n",
    "            for i in range((20-nbyz), num_workers):\n",
    "                net_ = copy.deepcopy(fed_model)\n",
    "                net_.zero_grad()\n",
    "                output = net_(each_worker_data[i].reshape(-1, 1, 28, 28).cuda())\n",
    "                loss = criterion(output, each_worker_label[i].long().cuda())\n",
    "                loss.backward(retain_graph = True)\n",
    "                param_grad=[]\n",
    "                for param in net_.parameters():\n",
    "                    param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "                user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "                del net_\n",
    "\n",
    "            weight = []\n",
    "            for param in fed_model.parameters():\n",
    "                weight = param.data.view(-1) if not len(weight) else torch.cat((weight, param.data.view(-1)))\n",
    "\n",
    "            if (e > start_detection_epoch):\n",
    "                hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "                hvp = np.squeeze(hvp)\n",
    "            else:\n",
    "                hvp = None\n",
    "\n",
    "            good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "            if hvp is not None:\n",
    "                pred_grad = copy.deepcopy(good_old_grads)\n",
    "                distance = []\n",
    "                for i in range(len(good_old_grads)):\n",
    "                    pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "                good_distance_rage = np.concatenate(\n",
    "                    (good_distance_rage, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "            if attack_type != 'none' and (e < start_detection_epoch):\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "                # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "            elif e > start_detection_epoch:\n",
    "                if attack_type == 'full_trim':\n",
    "                    user_grads = full_trim(user_grads, nbyz)\n",
    "                elif attack_type == 'none':\n",
    "                    pass\n",
    "                else:\n",
    "                    if attack_type == 'NDSS21':\n",
    "                        model_re = torch.mean(good_current_grads, dim=0)\n",
    "                        if dev_type == 'unit_vec':\n",
    "                            deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                        elif dev_type == 'sign':\n",
    "                            deviation = torch.sign(model_re)\n",
    "                        elif dev_type == 'std':\n",
    "                            deviation = torch.std(good_current_grads, 0)\n",
    "                        # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_rage[-1]))) / torch.norm(deviation))\n",
    "\n",
    "                    else:\n",
    "                        noise = torch.zeros(hvp.shape).to(device)\n",
    "                    for m in range(nbyz):\n",
    "                        distance_bound = np.mean(np.mean(good_distance_rage[-1:], 0))\n",
    "                        noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                        user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "            agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "            if distance is not None and e > (start_detection_epoch - window_size):\n",
    "                malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "            if malicious_scores.shape[0] >= window_size+1:\n",
    "                if e == window_size + 1: print('shape of malicious scores: ', malicious_scores.shape)\n",
    "                if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                    print('Stop at iteration:', e)\n",
    "                    print('run %d n_mal %d | val_loss %.3f val acc %.3f | best val_acc %.3f' % (run, n_mal, val_loss, val_acc, best_val_acc))\n",
    "                    acc, fpr, fnr, auc = detection(\n",
    "                        np.sum(malicious_scores[-window_size:], axis=0), nbyz, len(user_grads))\n",
    "                    results[n_mal]['acc'].append(acc)\n",
    "                    results[n_mal]['fpr'].append(fpr)\n",
    "                    results[n_mal]['fnr'].append(fnr)\n",
    "                    results[n_mal]['auc'].append(auc)\n",
    "                    break\n",
    "\n",
    "            if e > (start_detection_epoch - window_size):\n",
    "                weight_record.append(weight - last_weight)\n",
    "                grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "            if (len(weight_record) > window_size):\n",
    "                del weight_record[0]\n",
    "                del grad_record[0]\n",
    "\n",
    "            last_weight = weight\n",
    "            last_grad = agg_grads\n",
    "            old_grad_list = user_grads\n",
    "            good_old_grads = good_current_grads\n",
    "\n",
    "            del user_grads\n",
    "            start_idx=0\n",
    "            cnn_optimizer.zero_grad()\n",
    "            model_grads=[]\n",
    "            for i, param in enumerate(fed_model.parameters()):\n",
    "                param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "                start_idx=start_idx+len(param.data.view(-1))\n",
    "                param_=param_.cuda()\n",
    "                model_grads.append(param_)\n",
    "            cnn_optimizer.step(model_grads)\n",
    "            val_loss, val_acc = test(global_test_data.reshape(-1,1,28,28), global_test_label.long(), fed_model, criterion, use_cuda, batch_size=50)\n",
    "            is_best = best_val_acc < val_acc\n",
    "            best_val_acc = max(best_val_acc, val_acc)\n",
    "        print('run %d e %d | val_loss %.3f val acc %.3f | best val_acc %.3f' % (run, e, val_loss, val_acc, best_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6739602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nmal 5 | fpr 0.482500 fnr 1.000000 auc 0.258750\n",
      "nmal 10 | fpr 0.397500 fnr 1.000000 auc 0.301250\n",
      "nmal 15 | fpr 0.480000 fnr 0.800000 auc 0.360000\n",
      "nmal 20 | fpr 0.450000 fnr 1.000000 auc 0.275000\n"
     ]
    }
   ],
   "source": [
    "for nmal in [5, 10, 15, 20]:\n",
    "    print('nmal %d | fpr %f fnr %f auc %f' % (nmal,\n",
    "                                              np.array(results[nmal]['fpr']).mean(),\n",
    "                                              np.array(results[nmal]['fnr']).mean(),\n",
    "                                              np.array(results[nmal]['auc']).mean(),\n",
    "                                             ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d70d30",
   "metadata": {},
   "source": [
    "# Fast baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b831f805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 | val_loss 1.642 val acc 69.270 | best val_acc 69.270\n",
      "e 5 | val_loss 0.178 val acc 94.920 | best val_acc 94.920\n",
      "e 10 | val_loss 0.114 val acc 96.504 | best val_acc 96.504\n",
      "Stop at iteration: 11\n",
      "acc 0.9300; recall 0.0000; fpr 0.0211; fnr 1.0000; auc 0.4895\n",
      "e 0 | val_loss 1.702 val acc 67.108 | best val_acc 67.108\n",
      "e 5 | val_loss 0.183 val acc 94.531 | best val_acc 94.531\n",
      "Stop at iteration: 10\n",
      "acc 0.8900; recall 0.0000; fpr 0.0111; fnr 1.0000; auc 0.4944\n",
      "e 0 | val_loss 1.822 val acc 67.168 | best val_acc 67.168\n",
      "e 5 | val_loss 0.202 val acc 94.093 | best val_acc 94.093\n",
      "e 10 | val_loss 0.107 val acc 96.763 | best val_acc 96.763\n",
      "e 15 | val_loss 0.081 val acc 97.500 | best val_acc 97.500\n",
      "e 20 | val_loss 0.069 val acc 97.928 | best val_acc 97.928\n",
      "e 25 | val_loss 0.061 val acc 98.147 | best val_acc 98.147\n",
      "e 30 | val_loss 0.056 val acc 98.157 | best val_acc 98.197\n",
      "e 35 | val_loss 0.053 val acc 98.356 | best val_acc 98.356\n",
      "e 40 | val_loss 0.051 val acc 98.466 | best val_acc 98.466\n",
      "e 45 | val_loss 0.050 val acc 98.506 | best val_acc 98.506\n",
      "e 0 | val_loss 1.925 val acc 62.695 | best val_acc 62.695\n",
      "e 5 | val_loss 0.202 val acc 93.993 | best val_acc 93.993\n",
      "e 10 | val_loss 0.111 val acc 96.643 | best val_acc 96.643\n",
      "Stop at iteration: 12\n",
      "acc 0.7800; recall 0.0000; fpr 0.0250; fnr 1.0000; auc 0.4875\n",
      "e 0 | val_loss 2.049 val acc 43.022 | best val_acc 43.022\n",
      "e 5 | val_loss 0.215 val acc 93.436 | best val_acc 93.436\n",
      "e 10 | val_loss 0.109 val acc 96.593 | best val_acc 96.593\n",
      "e 15 | val_loss 0.079 val acc 97.510 | best val_acc 97.510\n",
      "e 20 | val_loss 0.067 val acc 97.988 | best val_acc 97.988\n",
      "e 25 | val_loss 0.060 val acc 98.237 | best val_acc 98.237\n",
      "e 30 | val_loss 0.058 val acc 98.436 | best val_acc 98.436\n",
      "e 35 | val_loss 0.059 val acc 98.536 | best val_acc 98.536\n",
      "e 40 | val_loss 0.061 val acc 98.516 | best val_acc 98.536\n",
      "e 45 | val_loss 0.064 val acc 98.595 | best val_acc 98.595\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "n_malicious = [5, 10, 15, 20, 25]\n",
    "\n",
    "for n_mal in n_malicious:\n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        # delete all variables\n",
    "        del weight_record, grad_record, old_grad_list, num_workers, malicious_scores, start_detection_epoch, window_size\n",
    "        del good_distance_rage\n",
    "        del nepochs, num_workers, local_lr, local_epochs, global_lr, agr\n",
    "        del attack_type, dev_type, fed_model\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # FL training initializations\n",
    "    num_workers = 100\n",
    "    use_cuda = True\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    local_lr=0.01\n",
    "    local_batch_size=32\n",
    "    local_epochs=2\n",
    "    global_lr=1\n",
    "    agr='tr_mean' # ['average', 'median', 'tr_mean']\n",
    "\n",
    "    # Attack initializations\n",
    "    attack_type = 'NDSS21'\n",
    "    dev_type = 'unit_vec'\n",
    "    nbyz = n_mal ################### Changing variable ###################\n",
    "\n",
    "    # FLD initializations\n",
    "    weight_record = []\n",
    "    grad_record = []\n",
    "    old_grad_list = []\n",
    "    malicious_scores = np.zeros((1, num_workers))\n",
    "    start_detection_epoch = 5\n",
    "    window_size = 5\n",
    "    assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "    good_distance_rage = np.zeros((1, nbyz))\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "\n",
    "    # FL model initializations\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    model_received = []\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "    for e in range(50):\n",
    "        user_grads=[]\n",
    "        round_clients = np.arange(num_workers)\n",
    "        round_benign = round_clients\n",
    "        user_grads=[]\n",
    "\n",
    "        for i in round_benign:\n",
    "            model = copy.deepcopy(fed_model)\n",
    "            optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "            for epoch in range(local_epochs):\n",
    "                train_loss, train_acc = train(\n",
    "                    each_worker_data[i].reshape(-1, 1, 28, 28),\n",
    "                    torch.Tensor(each_worker_label[i]).long(), model, optimizer, local_batch_size)\n",
    "\n",
    "            params = []\n",
    "            for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                    (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "            update =  (params - model_received)\n",
    "            user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "        weight = model_received\n",
    "\n",
    "        if (e > start_detection_epoch):\n",
    "            hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "            hvp = np.squeeze(hvp)\n",
    "        else:\n",
    "            hvp = None\n",
    "\n",
    "        good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "        if hvp is not None:\n",
    "            pred_grad = copy.deepcopy(good_old_grads)\n",
    "            distance = []\n",
    "            for i in range(len(good_old_grads)):\n",
    "                pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "            good_distance_rage = np.concatenate(\n",
    "                (good_distance_rage, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "        if attack_type != 'none' and (e < start_detection_epoch):\n",
    "            user_grads = full_trim(user_grads, nbyz)\n",
    "            # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "        elif e > start_detection_epoch:\n",
    "            if attack_type == 'full_trim':\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "            elif attack_type == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                if attack_type == 'LIE':\n",
    "                    z = 0.1\n",
    "                    noise_avg = torch.mean(user_grads[:nbyz], dim=0)\n",
    "                    noise_std = torch.std(user_grads[:nbyz], dim=0)\n",
    "                    noise = noise_avg + z * noise_std\n",
    "                elif attack_type == 'NDSS21':\n",
    "                    distance_bound = np.random.choice(np.mean(good_distance_rage[-1:], 0))\n",
    "                    model_re = torch.mean(good_current_grads, dim=0)\n",
    "                    if dev_type == 'unit_vec':\n",
    "                        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                    elif dev_type == 'sign':\n",
    "                        deviation = torch.sign(model_re)\n",
    "                    elif dev_type == 'std':\n",
    "                        deviation = torch.std(good_current_grads, 0)\n",
    "                    # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_rage[-1]))) / torch.norm(deviation))\n",
    "                    noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                else:\n",
    "                    noise = torch.zeros(hvp.shape).to(device)\n",
    "                for m in range(nbyz):\n",
    "                    user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "        agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "        if distance is not None and e > (start_detection_epoch - window_size):\n",
    "            malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "        if malicious_scores.shape[0] >= window_size+1:\n",
    "            if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                print('Stop at iteration:', e)\n",
    "                detection(np.sum(malicious_scores[-window_size:], axis=0), nbyz, num_workers)\n",
    "                break\n",
    "\n",
    "        if e > (start_detection_epoch - window_size):\n",
    "            weight_record.append(weight - last_weight)\n",
    "            grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "        if (len(weight_record) > window_size):\n",
    "            del weight_record[0]\n",
    "            del grad_record[0]\n",
    "\n",
    "        last_weight = weight\n",
    "        last_grad = agg_grads\n",
    "        old_grad_list = user_grads\n",
    "        good_old_grads = good_current_grads\n",
    "\n",
    "        del user_grads\n",
    "        model_received = model_received + global_lr * agg_grads\n",
    "        fed_model = cnn().to(device)\n",
    "        fed_model.apply(init_weights)\n",
    "        start_idx=0\n",
    "        state_dict = {}\n",
    "        previous_name = 'none'\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "            start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "            params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "            state_dict[name] = params\n",
    "            previous_name = name\n",
    "\n",
    "        fed_model.load_state_dict(state_dict)\n",
    "        val_loss, val_acc = test(global_test_data.reshape(-1,1,28,28), global_test_label.long(), fed_model, criterion, use_cuda, batch_size=50)\n",
    "        is_best = best_val_acc < val_acc\n",
    "        best_val_acc = max(best_val_acc, val_acc)\n",
    "        if e%5==0 or e==99:\n",
    "            print('e %d | val_loss %.3f val acc %.3f | best val_acc %.3f' % (e, val_loss, val_acc, best_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56edca38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 | val_loss 1.353 val acc 79.211 | best val_acc 79.211\n",
      "e 5 | val_loss 0.171 val acc 95.029 | best val_acc 95.029\n",
      "e 10 | val_loss 0.114 val acc 96.524 | best val_acc 96.524\n",
      "Stop at iteration: 13\n",
      "acc 0.9400; recall 0.0000; fpr 0.0105; fnr 1.0000; auc 0.4947\n",
      "e 0 | val_loss 1.636 val acc 66.471 | best val_acc 66.471\n",
      "e 5 | val_loss 0.181 val acc 94.611 | best val_acc 94.611\n",
      "e 10 | val_loss 0.111 val acc 96.603 | best val_acc 96.603\n",
      "Stop at iteration: 11\n",
      "acc 0.8700; recall 0.0000; fpr 0.0333; fnr 1.0000; auc 0.4833\n",
      "e 0 | val_loss 1.708 val acc 72.179 | best val_acc 72.179\n",
      "e 5 | val_loss 0.199 val acc 94.083 | best val_acc 94.083\n",
      "e 10 | val_loss 0.110 val acc 96.514 | best val_acc 96.514\n",
      "Stop at iteration: 15\n",
      "acc 0.8400; recall 0.0000; fpr 0.0118; fnr 1.0000; auc 0.4941\n",
      "e 0 | val_loss 2.012 val acc 58.412 | best val_acc 58.412\n",
      "e 5 | val_loss 0.204 val acc 94.213 | best val_acc 94.213\n",
      "e 10 | val_loss 0.128 val acc 96.275 | best val_acc 96.275\n",
      "e 15 | val_loss 0.095 val acc 97.181 | best val_acc 97.181\n",
      "e 20 | val_loss 0.077 val acc 97.589 | best val_acc 97.589\n",
      "e 25 | val_loss 0.066 val acc 97.898 | best val_acc 97.898\n",
      "Stop at iteration: 28\n",
      "acc 0.7600; recall 0.0000; fpr 0.0500; fnr 1.0000; auc 0.4750\n",
      "e 0 | val_loss 2.115 val acc 44.716 | best val_acc 44.716\n",
      "e 5 | val_loss 0.204 val acc 94.143 | best val_acc 94.143\n",
      "e 10 | val_loss 0.116 val acc 96.494 | best val_acc 96.494\n",
      "e 15 | val_loss 0.081 val acc 97.440 | best val_acc 97.440\n",
      "e 20 | val_loss 0.067 val acc 97.878 | best val_acc 97.878\n",
      "e 25 | val_loss 0.060 val acc 98.177 | best val_acc 98.177\n",
      "e 30 | val_loss 0.055 val acc 98.287 | best val_acc 98.287\n",
      "e 35 | val_loss 0.050 val acc 98.476 | best val_acc 98.476\n",
      "e 40 | val_loss 0.050 val acc 98.516 | best val_acc 98.516\n",
      "e 45 | val_loss 0.049 val acc 98.605 | best val_acc 98.605\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "n_malicious = [5, 10, 15, 20, 25]\n",
    "\n",
    "for n_mal in n_malicious:\n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        # delete all variables\n",
    "        del weight_record, grad_record, old_grad_list, num_workers, malicious_scores, start_detection_epoch, window_size\n",
    "        del good_distance_rage\n",
    "        del nepochs, num_workers, local_lr, local_epochs, global_lr, agr\n",
    "        del attack_type, dev_type, fed_model\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # FL training initializations\n",
    "    num_workers = 100\n",
    "    use_cuda = True\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    local_lr=0.01\n",
    "    local_batch_size=32\n",
    "    local_epochs=2\n",
    "    global_lr=1\n",
    "    agr='tr_mean' # ['average', 'median', 'tr_mean']\n",
    "\n",
    "    # Attack initializations\n",
    "    attack_type = 'NDSS21'\n",
    "    dev_type = 'sign'\n",
    "    nbyz = n_mal ################### Changing variable ###################\n",
    "\n",
    "    # FLD initializations\n",
    "    weight_record = []\n",
    "    grad_record = []\n",
    "    old_grad_list = []\n",
    "    malicious_scores = np.zeros((1, num_workers))\n",
    "    start_detection_epoch = 5\n",
    "    window_size = 5\n",
    "    assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "    good_distance_rage = np.zeros((1, nbyz))\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "\n",
    "    # FL model initializations\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    model_received = []\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "    for e in range(50):\n",
    "        user_grads=[]\n",
    "        round_clients = np.arange(num_workers)\n",
    "        round_benign = round_clients\n",
    "        user_grads=[]\n",
    "\n",
    "        for i in round_benign:\n",
    "            model = copy.deepcopy(fed_model)\n",
    "            optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "            for epoch in range(local_epochs):\n",
    "                train_loss, train_acc = train(\n",
    "                    each_worker_data[i].reshape(-1, 1, 28, 28),\n",
    "                    torch.Tensor(each_worker_label[i]).long(), model, optimizer, local_batch_size)\n",
    "\n",
    "            params = []\n",
    "            for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                    (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "            update =  (params - model_received)\n",
    "            user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "        weight = model_received\n",
    "\n",
    "        if (e > start_detection_epoch):\n",
    "            hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "            hvp = np.squeeze(hvp)\n",
    "        else:\n",
    "            hvp = None\n",
    "\n",
    "        good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "        if hvp is not None:\n",
    "            pred_grad = copy.deepcopy(good_old_grads)\n",
    "            distance = []\n",
    "            for i in range(len(good_old_grads)):\n",
    "                pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "            good_distance_rage = np.concatenate(\n",
    "                (good_distance_rage, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "        if attack_type != 'none' and (e < start_detection_epoch):\n",
    "            user_grads = full_trim(user_grads, nbyz)\n",
    "            # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "        elif e > start_detection_epoch:\n",
    "            if attack_type == 'full_trim':\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "            elif attack_type == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                if attack_type == 'LIE':\n",
    "                    z = 0.1\n",
    "                    noise_avg = torch.mean(user_grads[:nbyz], dim=0)\n",
    "                    noise_std = torch.std(user_grads[:nbyz], dim=0)\n",
    "                    noise = noise_avg + z * noise_std\n",
    "                elif attack_type == 'NDSS21':\n",
    "                    distance_bound = np.random.choice(np.mean(good_distance_rage[-1:], 0))\n",
    "                    model_re = torch.mean(good_current_grads, dim=0)\n",
    "                    if dev_type == 'unit_vec':\n",
    "                        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                    elif dev_type == 'sign':\n",
    "                        deviation = torch.sign(model_re)\n",
    "                    elif dev_type == 'std':\n",
    "                        deviation = torch.std(good_current_grads, 0)\n",
    "                    # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_rage[-1]))) / torch.norm(deviation))\n",
    "                    noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                else:\n",
    "                    noise = torch.zeros(hvp.shape).to(device)\n",
    "                for m in range(nbyz):\n",
    "                    user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "        agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "        if distance is not None and e > (start_detection_epoch - window_size):\n",
    "            malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "        if malicious_scores.shape[0] >= window_size+1:\n",
    "            if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                print('Stop at iteration:', e)\n",
    "                detection(np.sum(malicious_scores[-window_size:], axis=0), nbyz, num_workers)\n",
    "                break\n",
    "\n",
    "        if e > (start_detection_epoch - window_size):\n",
    "            weight_record.append(weight - last_weight)\n",
    "            grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "        if (len(weight_record) > window_size):\n",
    "            del weight_record[0]\n",
    "            del grad_record[0]\n",
    "\n",
    "        last_weight = weight\n",
    "        last_grad = agg_grads\n",
    "        old_grad_list = user_grads\n",
    "        good_old_grads = good_current_grads\n",
    "\n",
    "        del user_grads\n",
    "        model_received = model_received + global_lr * agg_grads\n",
    "        fed_model = cnn().to(device)\n",
    "        fed_model.apply(init_weights)\n",
    "        start_idx=0\n",
    "        state_dict = {}\n",
    "        previous_name = 'none'\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "            start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "            params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "            state_dict[name] = params\n",
    "            previous_name = name\n",
    "\n",
    "        fed_model.load_state_dict(state_dict)\n",
    "        val_loss, val_acc = test(global_test_data.reshape(-1,1,28,28), global_test_label.long(), fed_model, criterion, use_cuda, batch_size=50)\n",
    "        is_best = best_val_acc < val_acc\n",
    "        best_val_acc = max(best_val_acc, val_acc)\n",
    "        if e%5==0 or e==99:\n",
    "            print('e %d | val_loss %.3f val acc %.3f | best val_acc %.3f' % (e, val_loss, val_acc, best_val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050a1d3",
   "metadata": {},
   "source": [
    "# Label flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a779827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 | val_loss 1.739 val acc 63.861 | best val_acc 63.861\n",
      "e 5 | val_loss 0.226 val acc 94.521 | best val_acc 94.521\n",
      "Stop at iteration: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu108/4303768/ipykernel_1201586/2337786522.py:55: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  recall=1-np.sum(label_pred[:nobyz])/nobyz\n",
      "/scratch/gypsum-gpu108/4303768/ipykernel_1201586/2337786522.py:57: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  fnr=np.sum(label_pred[:nobyz])/nobyz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m detection1(np\u001b[38;5;241m.\u001b[39msum(malicious_scores[\u001b[38;5;241m-\u001b[39mwindow_size:], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), nbyz):\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStop at iteration:\u001b[39m\u001b[38;5;124m'\u001b[39m, e)\n\u001b[0;32m--> 138\u001b[0m         \u001b[43mdetection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmalicious_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbyz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;241m>\u001b[39m (start_detection_epoch \u001b[38;5;241m-\u001b[39m window_size):\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mdetection\u001b[0;34m(score, nobyz, nworkers)\u001b[0m\n\u001b[1;32m     56\u001b[0m fpr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(label_pred[nobyz:])\u001b[38;5;241m/\u001b[39m(nworkers\u001b[38;5;241m-\u001b[39mnobyz)\n\u001b[1;32m     57\u001b[0m fnr\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(label_pred[:nobyz])\u001b[38;5;241m/\u001b[39mnobyz\n\u001b[0;32m---> 58\u001b[0m auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc \u001b[39m\u001b[38;5;132;01m%0.4f\u001b[39;00m\u001b[38;5;124m; recall \u001b[39m\u001b[38;5;132;01m%0.4f\u001b[39;00m\u001b[38;5;124m; fpr \u001b[39m\u001b[38;5;132;01m%0.4f\u001b[39;00m\u001b[38;5;124m; fnr \u001b[39m\u001b[38;5;132;01m%0.4f\u001b[39;00m\u001b[38;5;124m; auc \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (acc, recall, fpr, fnr, auc))\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m acc, fpr, fnr, auc\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:571\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    569\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n\u001b[1;32m    570\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m label_binarize(y_true, classes\u001b[38;5;241m=\u001b[39mlabels)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    580\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[1;32m    581\u001b[0m         y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    585\u001b[0m     )\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/sklearn/metrics/_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m     78\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true)\n",
      "File \u001b[0;32m/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:339\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    344\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "'''\n",
    "FAST BASELINE\n",
    "'''\n",
    "n_malicious = [0]\n",
    "\n",
    "for n_mal in n_malicious:\n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        # delete all variables\n",
    "        del weight_record, grad_record, old_grad_list, num_workers, malicious_scores, start_detection_epoch, window_size\n",
    "        del good_distance_rage\n",
    "        del nepochs, num_workers, local_lr, local_epochs, global_lr, agr\n",
    "        del attack_type, dev_type, fed_model\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # FL training initializations\n",
    "    num_workers = 100\n",
    "    use_cuda = True\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    local_lr=0.01\n",
    "    local_batch_size=32\n",
    "    local_epochs=2\n",
    "    global_lr=1\n",
    "    agr='tr_mean' # ['average', 'median', 'tr_mean']\n",
    "\n",
    "    # Attack initializations\n",
    "    attack_type = 'none'\n",
    "    dev_type = 'sign'\n",
    "    nbyz = n_mal ################### Changing variable ###################\n",
    "\n",
    "    # FLD initializations\n",
    "    weight_record = []\n",
    "    grad_record = []\n",
    "    old_grad_list = []\n",
    "    malicious_scores = np.zeros((1, num_workers))\n",
    "    start_detection_epoch = 5\n",
    "    window_size = 5\n",
    "    assert (start_detection_epoch - window_size >= 0), 'start_detection_epoch %d should be more than window_size %d' % (start_detection_epoch, window_size)\n",
    "    good_distance_rage = np.zeros((1, nbyz))\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "\n",
    "    # FL model initializations\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    model_received = []\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "    for e in range(50):\n",
    "        user_grads=[]\n",
    "        round_clients = np.arange(num_workers)\n",
    "        round_benign = round_clients\n",
    "        user_grads=[]\n",
    "\n",
    "        for i in round_benign:\n",
    "            model = copy.deepcopy(fed_model)\n",
    "            optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9, weight_decay=1e-4)\n",
    "            if i < 10:\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(\n",
    "                        each_worker_data[i].reshape(-1, 1, 28, 28),\n",
    "                        torch.Tensor((1+each_worker_label[i])%10).long(), model, optimizer, local_batch_size)\n",
    "            else:\n",
    "                for epoch in range(local_epochs):\n",
    "                    train_loss, train_acc = train(\n",
    "                        each_worker_data[i].reshape(-1, 1, 28, 28),\n",
    "                        torch.Tensor(each_worker_label[i]).long(), model, optimizer, local_batch_size)\n",
    "\n",
    "            params = []\n",
    "            for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "                params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                    (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "            update =  (params - model_received)\n",
    "            user_grads = update[None,:] if len(user_grads) == 0 else torch.cat((user_grads, update[None,:]), 0)\n",
    "\n",
    "        weight = model_received\n",
    "\n",
    "        if (e > start_detection_epoch):\n",
    "            hvp = lbfgs(weight_record, grad_record, weight - last_weight)\n",
    "            hvp = np.squeeze(hvp)\n",
    "        else:\n",
    "            hvp = None\n",
    "\n",
    "        good_current_grads = copy.deepcopy(user_grads[:nbyz])\n",
    "        if hvp is not None:\n",
    "            pred_grad = copy.deepcopy(good_old_grads)\n",
    "            distance = []\n",
    "            for i in range(len(good_old_grads)):\n",
    "                pred_grad[i] += torch.from_numpy(hvp).to(device)\n",
    "            good_distance_rage = np.concatenate(\n",
    "                (good_distance_rage, torch.norm(pred_grad - good_current_grads, dim = 1).cpu().numpy()[None,:]), 0)\n",
    "\n",
    "        if attack_type != 'none' and (e < start_detection_epoch):\n",
    "            user_grads = full_trim(user_grads, nbyz)\n",
    "            # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "        elif e > start_detection_epoch:\n",
    "            if attack_type == 'full_trim':\n",
    "                user_grads = full_trim(user_grads, nbyz)\n",
    "            elif attack_type == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                if attack_type == 'LIE':\n",
    "                    z = 0.1\n",
    "                    noise_avg = torch.mean(user_grads[:nbyz], dim=0)\n",
    "                    noise_std = torch.std(user_grads[:nbyz], dim=0)\n",
    "                    noise = noise_avg + z * noise_std\n",
    "                elif attack_type == 'NDSS21':\n",
    "                    distance_bound = np.random.choice(np.mean(good_distance_rage[-1:], 0))\n",
    "                    model_re = torch.mean(good_current_grads, dim=0)\n",
    "                    if dev_type == 'unit_vec':\n",
    "                        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "                    elif dev_type == 'sign':\n",
    "                        deviation = torch.sign(model_re)\n",
    "                    elif dev_type == 'std':\n",
    "                        deviation = torch.std(good_current_grads, 0)\n",
    "                    # noise = deviation * ((distance_bound + np.random.uniform(0, np.std(good_distance_rage[-1]))) / torch.norm(deviation))\n",
    "                    noise = deviation * ((distance_bound)) / torch.norm(deviation)\n",
    "                else:\n",
    "                    noise = torch.zeros(hvp.shape).to(device)\n",
    "                for m in range(nbyz):\n",
    "                    user_grads[m] = old_grad_list[m] + torch.from_numpy(hvp).to(device) + noise\n",
    "\n",
    "        agg_grads, distance = fldetector(old_grad_list, user_grads, nbyz, hvp, agr=agr)\n",
    "\n",
    "        if distance is not None and e > (start_detection_epoch - window_size):\n",
    "            malicious_scores = np.concatenate((malicious_scores, distance[None, :]), 0)\n",
    "\n",
    "        if malicious_scores.shape[0] >= window_size+1:\n",
    "            if detection1(np.sum(malicious_scores[-window_size:], axis=0), nbyz):\n",
    "                print('Stop at iteration:', e)\n",
    "                detection(np.sum(malicious_scores[-window_size:], axis=0), nbyz, num_workers)\n",
    "                break\n",
    "\n",
    "        if e > (start_detection_epoch - window_size):\n",
    "            weight_record.append(weight - last_weight)\n",
    "            grad_record.append(agg_grads - last_grad)\n",
    "\n",
    "        if (len(weight_record) > window_size):\n",
    "            del weight_record[0]\n",
    "            del grad_record[0]\n",
    "\n",
    "        last_weight = weight\n",
    "        last_grad = agg_grads\n",
    "        old_grad_list = user_grads\n",
    "        good_old_grads = good_current_grads\n",
    "\n",
    "        del user_grads\n",
    "        model_received = model_received + global_lr * agg_grads\n",
    "        fed_model = cnn().to(device)\n",
    "        fed_model.apply(init_weights)\n",
    "        start_idx=0\n",
    "        state_dict = {}\n",
    "        previous_name = 'none'\n",
    "        for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "            start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "            start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "            params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "            state_dict[name] = params\n",
    "            previous_name = name\n",
    "\n",
    "        fed_model.load_state_dict(state_dict)\n",
    "        val_loss, val_acc = test(global_test_data.reshape(-1,1,28,28), global_test_label.long(), fed_model, criterion, use_cuda, batch_size=50)\n",
    "        is_best = best_val_acc < val_acc\n",
    "        best_val_acc = max(best_val_acc, val_acc)\n",
    "        if e%5==0 or e==99:\n",
    "            print('e %d | val_loss %.3f val acc %.3f | best val_acc %.3f' % (e, val_loss, val_acc, best_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1cbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
