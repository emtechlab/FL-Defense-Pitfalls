{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821b1014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu108/4303768/ipykernel_1203878/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a52c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "            \n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_train_data(trainset, num_workers=100, bias=0.5):\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        \n",
    "        if not len(each_worker_data[selected_worker]):\n",
    "            each_worker_data[selected_worker] = x[None, :]\n",
    "        else:\n",
    "            each_worker_data[selected_worker]= torch.concat((each_worker_data[selected_worker], x[None, :]))\n",
    "        \n",
    "        each_worker_label[selected_worker].append(y)\n",
    "    \n",
    "    each_worker_tr_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_tr_label = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    for i in range(num_workers):\n",
    "        w_len = len(each_worker_data[i])\n",
    "        len_tr = int(6 * w_len / 7)\n",
    "        len_te = w_len - len_tr\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        each_worker_tr_data[i] = each_worker_data[i][tr_idx]\n",
    "        each_worker_tr_label[i] = torch.Tensor(each_worker_label[i])[tr_idx]\n",
    "        \n",
    "        each_worker_te_data[i] = each_worker_data[i][te_idx]\n",
    "        each_worker_te_label[i] = torch.Tensor(each_worker_label[i])[te_idx]\n",
    "        \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    del each_worker_data, each_worker_label\n",
    "    return each_worker_tr_data, each_worker_tr_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f36f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_data_dirichlet(trainset, num_workers, alpha=1, force=False):\n",
    "    per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=alpha, force=force)\n",
    "    \n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(6*w_len/7)\n",
    "        len_te = w_len - len_tr\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        \n",
    "        for idx in tr_idx:\n",
    "            each_worker_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_data[worker_idx] = torch.stack(each_worker_data[worker_idx])\n",
    "        each_worker_label[worker_idx] = torch.Tensor(each_worker_label[worker_idx]).long()\n",
    "        \n",
    "        for idx in te_idx:\n",
    "            each_worker_te_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_te_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_te_data[worker_idx] = torch.stack(each_worker_te_data[worker_idx])\n",
    "        each_worker_te_label[worker_idx] = torch.Tensor(each_worker_te_label[worker_idx]).long()\n",
    "    \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    \n",
    "    return each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trim attack\n",
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ab16268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDSS attack\n",
    "def our_attack_median(all_updates, n_attackers, dev_type='unit_vec'):\n",
    "    model_re = (all_updates)\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "    lamda = torch.Tensor([10.0]).cuda() #compute_lambda_our(all_updates, model_re, n_attackers)\n",
    "    threshold_diff = 1e-5\n",
    "    prev_loss = -1\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    iters = 0 \n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        agg_grads = torch.median(mal_updates, 0)[0]\n",
    "        \n",
    "        loss = torch.norm(agg_grads - model_re)\n",
    "        \n",
    "        if prev_loss < loss: #attack successful\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:#attack failed\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "        prev_loss = loss\n",
    "        \n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "    mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "    return mal_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca6b5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 30, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(30, 50, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(800, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a6b71",
   "metadata": {},
   "source": [
    "# Match baseline with fang distribution and 80 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ace04bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = True\n",
    "\n",
    "if distribution=='fang':\n",
    "    each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_train_data(all_data, num_workers=100, bias=param)\n",
    "elif distribution == 'dirichlet':\n",
    "    each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_data_dirichlet(all_data, num_workers, alpha=param, force=force)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b9e98",
   "metadata": {},
   "source": [
    "# Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67a4ed08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.10195141377937077 0.10195141377937077\n",
      "100 0.7241138988450816 0.7241138988450816\n",
      "200 0.8746515332536838 0.8746515332536838\n",
      "300 0.9005376344086021 0.9005376344086021\n",
      "400 0.9122859418558343 0.9122859418558343\n",
      "500 0.9217443249701314 0.9217443249701314\n",
      "600 0.9287136598964556 0.9287136598964556\n",
      "700 0.9359816806053365 0.9359816806053365\n",
      "800 0.9430505774591796 0.9430505774591796\n",
      "900 0.946236559139785 0.9463361210673039\n",
      "999 0.9495221027479092 0.9495221027479092\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "num_workers = 100\n",
    "nbyz = 20\n",
    "lr = 0.01\n",
    "nepochs=1000\n",
    "best_acc = 0\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "for e in range(nepochs):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(nbyz, num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "    agg_grads = torch.mean(user_grads, 0)\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        acc = correct/total\n",
    "        best_acc = max(best_acc, acc)\n",
    "    best_accs_per_round.append(best_acc)\n",
    "    accs_per_round.append(acc)\n",
    "    if e%100==0 or e==nepochs-1:\n",
    "        print(e, acc, best_acc)\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        outputs = net(each_worker_te_data[i].cuda())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += len(each_worker_te_label[i])\n",
    "        correct += (predicted == each_worker_te_label[i].long().cuda()).sum().item()\n",
    "        acc = correct/total\n",
    "    final_accs_per_client.append(acc)\n",
    "\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    ")\n",
    "\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_mnist_slow_mean.pkl'), 'wb'), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfff660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.13839107925129432 0.13839107925129432\n",
      "100 0.26473516527279967 0.26473516527279967\n",
      "200 0.428215850258861 0.428215850258861\n",
      "300 0.6329151732377539 0.6329151732377539\n",
      "400 0.7400438072481084 0.8118279569892473\n",
      "500 0.8461768219832736 0.8634010354440462\n",
      "600 0.7329749103942652 0.8865989645559538\n",
      "700 0.7830545599362804 0.8951612903225806\n",
      "800 0.8276583034647551 0.9036240541616886\n",
      "900 0.9074074074074074 0.9175627240143369\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "nbyz = 20\n",
    "lr = 0.01\n",
    "nepochs=1000\n",
    "best_acc=0\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "for e in range(nepochs):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "    user_grads = full_trim(user_grads, nbyz)\n",
    "    agg_grads = tr_mean(user_grads, nbyz)\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        acc = correct/total\n",
    "        best_acc = max(best_acc, acc)\n",
    "    best_accs_per_round.append(best_acc)\n",
    "    accs_per_round.append(acc)\n",
    "    if e%100==0 or e==nepochs-1:\n",
    "        print(e, acc, best_acc)\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        outputs = net(each_worker_te_data[i].cuda())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += len(each_worker_te_label[i])\n",
    "        correct += (predicted == each_worker_te_label[i].long().cuda()).sum().item()\n",
    "        acc = correct/total\n",
    "    final_accs_per_client.append(acc)\n",
    "\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    ")\n",
    "\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_mnist_slow_trmean_trim20.pkl'), 'wb'), )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993be4b",
   "metadata": {},
   "source": [
    "# Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9860241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.11927518916766229 0.11927518916766229\n",
      "100 0.7697132616487455 0.7697132616487455\n",
      "200 0.7053962564715253 0.7697132616487455\n",
      "300 0.7737953006770211 0.8020708880923935\n",
      "400 0.8378136200716846 0.8378136200716846\n",
      "500 0.824173636001593 0.8472720031859817\n",
      "600 0.8395061728395061 0.856431700517722\n",
      "700 0.8619076065312624 0.8767423337315811\n",
      "800 0.8738550378335325 0.8890880127439267\n",
      "900 0.8368180007964954 0.9168657905217045\n",
      "999 0.9414575866188769 0.9445440063719633\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "num_workers = 100\n",
    "nbyz = 20\n",
    "lr = 0.01\n",
    "nepochs=1000\n",
    "best_acc = 0\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "for e in range(nepochs):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(nbyz, num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "    agg_grads = torch.median(user_grads, 0)[0]\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        acc = correct/total\n",
    "        best_acc = max(best_acc, acc)\n",
    "    best_accs_per_round.append(best_acc)\n",
    "    accs_per_round.append(acc)\n",
    "    if e%100==0 or e==nepochs-1:\n",
    "        print(e, acc, best_acc)\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        outputs = net(each_worker_te_data[i].cuda())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += len(each_worker_te_label[i])\n",
    "        correct += (predicted == each_worker_te_label[i].long().cuda()).sum().item()\n",
    "        acc = correct/total\n",
    "    final_accs_per_client.append(acc)\n",
    "\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    ")\n",
    "\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_mnist_slow_median.pkl'), 'wb'), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90fa8487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.1163878932696137 0.24870569494225409\n",
      "200 0.008960573476702509 0.24870569494225409\n",
      "300 0.17293906810035842 0.24870569494225409\n",
      "400 0.11031461569095978 0.24870569494225409\n",
      "500 0.1424731182795699 0.24870569494225409\n",
      "600 0.2670250896057348 0.2670250896057348\n",
      "700 0.4345878136200717 0.4369772998805257\n",
      "800 0.4911389884508164 0.49193548387096775\n",
      "900 0.6145957785742732 0.7382516925527678\n",
      "999 0.7387495021903624 0.8428912783751493\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "nbyz = 20\n",
    "lr = 0.01\n",
    "nepochs=1000\n",
    "best_acc=0\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "for e in range(nepochs):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "    # user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "    user_grads = full_trim(user_grads, nbyz)\n",
    "    agg_grads = torch.median(user_grads, 0)[0]\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        acc = correct/total\n",
    "        best_acc = max(best_acc, acc)\n",
    "    best_accs_per_round.append(best_acc)\n",
    "    accs_per_round.append(acc)\n",
    "    if e%100==0 or e==nepochs-1:\n",
    "        print(e, acc, best_acc)\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        outputs = net(each_worker_te_data[i].cuda())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += len(each_worker_te_label[i])\n",
    "        correct += (predicted == each_worker_te_label[i].long().cuda()).sum().item()\n",
    "        acc = correct/total\n",
    "    final_accs_per_client.append(acc)\n",
    "\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    ")\n",
    "\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_mnist_slow_median_trim20.pkl'), 'wb'), )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb5ef3",
   "metadata": {},
   "source": [
    "# Good FedAvg baseline MNIST + Fang distribution + 80 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24164616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(train_data, labels, model, optimizer, batch_size=20):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    len_t = (len(train_data) // batch_size)\n",
    "    if len(train_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    r=np.arange(len(train_data))\n",
    "    np.random.shuffle(r)\n",
    "    \n",
    "    train_data = train_data[r]\n",
    "    labels = labels[r]\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "\n",
    "        inputs = train_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "        targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "\n",
    "def test(test_data, labels, model, criterion, use_cuda, debug_='MEDIUM', batch_size=64):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    len_t = (len(test_data) // batch_size)\n",
    "    if len(test_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ind in range(len_t):\n",
    "            # measure data loading time\n",
    "            inputs = test_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "            targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf95e8",
   "metadata": {},
   "source": [
    "# Mean without any attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2189af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 val loss 0.902 val acc 85.872 best val_acc 85.872\n",
      "e 10 val loss 0.079 val acc 97.601 best val_acc 97.601\n",
      "e 20 val loss 0.059 val acc 98.208 best val_acc 98.208\n",
      "e 30 val loss 0.052 val acc 98.457 best val_acc 98.467\n",
      "e 40 val loss 0.048 val acc 98.616 best val_acc 98.616\n",
      "e 49 val loss 0.045 val acc 98.686 best val_acc 98.686\n",
      "e 50 val loss 0.045 val acc 98.686 best val_acc 98.686\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "use_cuda=True\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "resume=False\n",
    "round_nclients = num_workers\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "fed_model = cnn().to(device)\n",
    "fed_model.apply(init_weights)\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    \n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(\n",
    "                each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "        update =  (params - model_received)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "    agg_update = torch.mean(user_updates, 0)\n",
    "    del user_updates\n",
    "    #update the global model\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    #load the state dictionary back into the global model\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, val_loss, val_acc, best_global_acc))\n",
    "    epoch_num+=1\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    client_loss, client_acc = test(each_worker_te_data[i], each_worker_te_label[i].long(),\n",
    "                                   fed_model, criterion, use_cuda)\n",
    "    final_accs_per_client.append(client_acc)\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    "    loss_per_round=np.array(loss_per_round)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_mnist_fast_mean.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272e7b3",
   "metadata": {},
   "source": [
    "# Mean under full trim attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f056cec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 val loss 1.760 val acc 78.465 best val_acc 78.465\n",
      "e 10 val loss 0.134 val acc 95.998 best val_acc 95.998\n",
      "e 20 val loss 0.122 val acc 96.426 best val_acc 96.426\n",
      "e 30 val loss 0.118 val acc 96.724 best val_acc 96.724\n",
      "e 40 val loss 0.116 val acc 96.794 best val_acc 96.854\n",
      "e 49 val loss 0.115 val acc 96.864 best val_acc 96.864\n",
      "e 50 val loss 0.116 val acc 96.824 best val_acc 96.864\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "use_cuda=True\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "resume=False\n",
    "round_nclients = num_workers\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "fed_model = cnn().to(device)\n",
    "fed_model.apply(init_weights)\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    \n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(\n",
    "                each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "        update =  (params - model_received)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "    \n",
    "    user_updates = full_trim(user_updates, nbyz)\n",
    "    agg_update = tr_mean(user_updates, nbyz)\n",
    "    del user_updates\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, val_loss, val_acc, best_global_acc))\n",
    "    epoch_num+=1\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    client_loss, client_acc = test(each_worker_te_data[i], each_worker_te_label[i].long(),\n",
    "                                   fed_model, criterion, use_cuda)\n",
    "    final_accs_per_client.append(client_acc)\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    "    loss_per_round=np.array(loss_per_round)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_mnist_fast_mean_trmean_trim20.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ebae2",
   "metadata": {},
   "source": [
    "# Median without any attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d43fc9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 val loss 1.133 val acc 86.858 best val_acc 86.858\n",
      "e 10 val loss 0.082 val acc 97.541 best val_acc 97.541\n",
      "e 20 val loss 0.062 val acc 98.108 best val_acc 98.108\n",
      "e 30 val loss 0.054 val acc 98.407 best val_acc 98.407\n",
      "e 40 val loss 0.050 val acc 98.477 best val_acc 98.507\n",
      "e 49 val loss 0.047 val acc 98.576 best val_acc 98.576\n",
      "e 50 val loss 0.047 val acc 98.596 best val_acc 98.596\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "use_cuda=True\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "resume=False\n",
    "round_nclients = num_workers\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "fed_model = cnn().to(device)\n",
    "fed_model.apply(init_weights)\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    \n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(\n",
    "                each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "        update =  (params - model_received)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "    \n",
    "    agg_update = torch.median(user_updates, 0)[0]\n",
    "    del user_updates\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, val_loss, val_acc, best_global_acc))\n",
    "    epoch_num+=1\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    client_loss, client_acc = test(each_worker_te_data[i], each_worker_te_label[i].long(),\n",
    "                                   fed_model, criterion, use_cuda)\n",
    "    final_accs_per_client.append(client_acc)\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    "    loss_per_round=np.array(loss_per_round)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_mnist_fast_median.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ab5c8",
   "metadata": {},
   "source": [
    "# Median under full trim attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb05db53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 val loss 1.735 val acc 76.563 best val_acc 76.563\n",
      "e 10 val loss 0.126 val acc 96.207 best val_acc 96.207\n",
      "e 20 val loss 0.109 val acc 96.774 best val_acc 96.774\n",
      "e 30 val loss 0.103 val acc 97.023 best val_acc 97.023\n",
      "e 40 val loss 0.101 val acc 97.123 best val_acc 97.143\n",
      "e 49 val loss 0.099 val acc 97.202 best val_acc 97.232\n",
      "e 50 val loss 0.099 val acc 97.182 best val_acc 97.232\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "use_cuda=True\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "resume=False\n",
    "round_nclients = num_workers\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "fed_model = cnn().to(device)\n",
    "fed_model.apply(init_weights)\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    \n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = local_lr, momentum=0.9)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(\n",
    "                each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "        update =  (params - model_received)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "    \n",
    "    user_updates = full_trim(user_updates, nbyz)\n",
    "    agg_update = torch.median(user_updates, 0)[0]\n",
    "    del user_updates\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, val_loss, val_acc, best_global_acc))\n",
    "    epoch_num+=1\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    client_loss, client_acc = test(each_worker_te_data[i], each_worker_te_label[i].long(),\n",
    "                                   fed_model, criterion, use_cuda)\n",
    "    final_accs_per_client.append(client_acc)\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    "    loss_per_round=np.array(loss_per_round)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_mnist_fast_median_trim20.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f36249",
   "metadata": {},
   "source": [
    "# ====== Below are previous explorations. Ignore them! ======"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94925375",
   "metadata": {},
   "source": [
    "# Faster baselines with server momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e592082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "nbyz = 20\n",
    "nepochs=100\n",
    "best_acc=0\n",
    "lr = 0.1\n",
    "cnn_optimizer = SGD(net.parameters(), lr = lr, momentum=0.9, weight_decay=1e-5)\n",
    "for e in range(nepochs):\n",
    "    user_grads = []\n",
    "    for i in range(nbyz, num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "    agg_grads = torch.mean(user_grads, 0)\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    if e%100==0 or e==nepochs-1:\n",
    "        total, correct = 0,0\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            acc = correct/total\n",
    "            best_acc = max(best_acc, acc)\n",
    "        print(e, acc, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41294142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.18996415770609318 0.18996415770609318\n",
      "20 0.09259259259259259 0.18996415770609318\n",
      "40 0.23725607327757867 0.23725607327757867\n",
      "60 0.09219434488251693 0.23725607327757867\n",
      "80 0.1942453205894066 0.23725607327757867\n",
      "100 0.5595380326563122 0.5595380326563122\n",
      "120 0.09747112704101951 0.5595380326563122\n",
      "140 0.09747112704101951 0.5595380326563122\n",
      "160 0.09747112704101951 0.5595380326563122\n",
      "180 0.09747112704101951 0.5595380326563122\n",
      "199 0.09747112704101951 0.5595380326563122\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "nbyz = 20\n",
    "nepochs=200\n",
    "best_acc=0\n",
    "lr = 0.05\n",
    "cnn_optimizer = SGD(net.parameters(), lr = lr, momentum=0.9, weight_decay=1e-5)\n",
    "for e in range(nepochs):\n",
    "    user_grads = []\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "    user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "    agg_grads = tr_mean(user_grads, nbyz)\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    if e%20==0 or e==nepochs-1:\n",
    "        total, correct = 0,0\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            acc = correct/total\n",
    "            best_acc = max(best_acc, acc)\n",
    "        print(e, acc, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "nbyz = 20\n",
    "nepochs=1000\n",
    "best_acc=0\n",
    "lr = 0.01\n",
    "cnn_optimizer = SGD(net.parameters(), lr = lr, momentum=0.9, weight_decay=1e-5)\n",
    "for e in range(nepochs):\n",
    "    user_grads = []\n",
    "    for i in range(nbyz, num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "    agg_grads = torch.median(user_grads, 0)[0]\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    if e%100==0 or e==nepochs-1:\n",
    "        total, correct = 0,0\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            acc = correct/total\n",
    "            best_acc = max(best_acc, acc)\n",
    "        print(e, acc, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "nbyz = 20\n",
    "nepochs=100\n",
    "best_acc=0\n",
    "lr = 0.1\n",
    "cnn_optimizer = SGD(net.parameters(), lr = lr, momentum=0.9, weight_decay=1e-5)\n",
    "for e in range(nepochs):\n",
    "    user_grads = []\n",
    "    for i in range(nbyz, num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "    user_grads[:nbyz] = full_trim(user_grads[:nbyz], nbyz)\n",
    "    agg_grads = torch.median(user_grads, 0)[0]\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    if e%20==0 or e==nepochs-1:\n",
    "        total, correct = 0,0\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            acc = correct/total\n",
    "            best_acc = max(best_acc, acc)\n",
    "        print(e, acc, best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c07e8",
   "metadata": {},
   "source": [
    "# Mount trim attack bad baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04bf297a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.08076080462059351\n",
      "50 0.29874526986656047\n",
      "100 0.38548097988448515\n",
      "150 0.4051981676956781\n",
      "200 0.4898426608245369\n",
      "250 0.6350328619796853\n",
      "300 0.8146783509261103\n",
      "350 0.7818163712407887\n",
      "400 0.807010555666202\n",
      "450 0.6992630950009958\n",
      "500 0.7984465245966939\n",
      "550 0.8383788090021908\n",
      "600 0.8644692292372037\n",
      "650 0.9151563433578969\n",
      "700 0.8799044015136427\n",
      "750 0.7283409679346744\n",
      "800 0.9242182832105158\n",
      "850 0.9305915156343358\n",
      "900 0.9334793865763792\n",
      "950 0.9288986257717586\n",
      "999 0.933379804819757\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "nbyz = 28\n",
    "lr = 0.02\n",
    "for e in range(1000):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "    user_grads = full_trim(user_grads, nbyz)\n",
    "    agg_grads = tr_mean(user_grads, nbyz)\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    if e%50==0 or e==999:\n",
    "        total, correct = 0,0\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(e,correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "393dfa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.10250796178343949\n",
      "50 0.2483081210191083\n",
      "100 0.3976910828025478\n",
      "150 0.5472730891719745\n",
      "200 0.7301950636942676\n",
      "250 0.8241441082802548\n",
      "300 0.7927945859872612\n",
      "350 0.794187898089172\n",
      "400 0.8379777070063694\n",
      "450 0.8304140127388535\n",
      "500 0.8810708598726115\n",
      "550 0.8898288216560509\n",
      "600 0.9166003184713376\n",
      "650 0.9093351910828026\n",
      "700 0.9161027070063694\n",
      "750 0.9248606687898089\n",
      "800 0.8802746815286624\n",
      "850 0.9094347133757962\n",
      "900 0.9301353503184714\n",
      "950 0.9267515923566879\n",
      "999 0.929140127388535\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "nbyz = 20\n",
    "lr = 0.02\n",
    "for e in range(1000):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "    user_grads = full_trim(user_grads, nbyz)\n",
    "    agg_grads = tr_mean(user_grads, nbyz)\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    if e%50==0 or e==999:\n",
    "        total, correct = 0,0\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(e,correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8f0e89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1506767515923567\n",
      "50 0.11275875796178345\n",
      "100 0.3004578025477707\n",
      "150 0.34156050955414013\n",
      "200 0.40017914012738853\n",
      "250 0.43580812101910826\n",
      "300 0.4590963375796178\n",
      "350 0.4823845541401274\n",
      "400 0.6436106687898089\n",
      "450 0.7318869426751592\n",
      "500 0.6098726114649682\n",
      "550 0.7991640127388535\n",
      "600 0.7964769108280255\n",
      "650 0.7870222929936306\n",
      "700 0.7532842356687898\n",
      "750 0.800656847133758\n",
      "800 0.8605692675159236\n",
      "850 0.7570660828025477\n",
      "900 0.897890127388535\n",
      "950 0.9058519108280255\n",
      "999 0.8165804140127388\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "nbyz = 20\n",
    "lr = 0.01\n",
    "for e in range(1000):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "    user_grads = full_trim(user_grads, nbyz)\n",
    "    agg_grads = tr_mean(user_grads, nbyz)\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    if e%50==0 or e==999:\n",
    "        total, correct = 0,0\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(e,correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3b596fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.09659430392352121\n",
      "50 0.0898227444732125\n",
      "100 0.02101175064728142\n",
      "150 0.12557259510057758\n",
      "200 0.14280023899621588\n",
      "250 0.2940649273053177\n",
      "300 0.40818562039434375\n",
      "350 0.5251941844254132\n",
      "400 0.5418243377813184\n",
      "450 0.6308504282015535\n",
      "500 0.7849034056960765\n",
      "550 0.8366859191396137\n",
      "600 0.8732324238199561\n",
      "650 0.8091017725552678\n",
      "700 0.8170683130850428\n",
      "750 0.8386775542720574\n",
      "800 0.8091017725552678\n",
      "850 0.7159928301135232\n",
      "900 0.9263095000995818\n",
      "950 0.9284007169886477\n",
      "999 0.9061939852619\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "nbyz = 28\n",
    "lr = 0.02\n",
    "for e in range(1000):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "    user_grads = full_trim(user_grads, nbyz)\n",
    "    agg_grads = torch.median(user_grads, 0)[0]\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    if e%50==0 or e==999:\n",
    "        total, correct = 0,0\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(e,correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef2e87",
   "metadata": {},
   "source": [
    "# attack good baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "725d36b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 benign_norm 1.284 val loss 2.157 val acc 50.827 best val_acc 50.827\n",
      "e 5 benign_norm 0.698 val loss 0.288 val acc 91.546 best val_acc 91.546\n",
      "e 10 benign_norm 0.639 val loss 0.227 val acc 93.129 best val_acc 93.129\n",
      "e 15 benign_norm 0.632 val loss 0.214 val acc 93.467 best val_acc 93.467\n",
      "e 20 benign_norm 0.636 val loss 0.208 val acc 93.736 best val_acc 93.736\n",
      "e 25 benign_norm 0.645 val loss 0.205 val acc 93.916 best val_acc 93.916\n",
      "e 30 benign_norm 0.655 val loss 0.207 val acc 93.926 best val_acc 93.926\n",
      "e 35 benign_norm 0.660 val loss 0.204 val acc 94.155 best val_acc 94.155\n",
      "e 40 benign_norm 0.672 val loss 0.203 val acc 94.264 best val_acc 94.264\n",
      "e 45 benign_norm 0.681 val loss 0.203 val acc 94.344 best val_acc 94.344\n",
      "e 49 benign_norm 0.681 val loss 0.192 val acc 94.812 best val_acc 94.812\n",
      "e 50 benign_norm 0.689 val loss 0.193 val acc 94.692 best val_acc 94.812\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "nepochs=2000\n",
    "local_epochs = 2\n",
    "batch_size = 8\n",
    "num_workers = 100\n",
    "local_lr = 0.05\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 28\n",
    "byz_type = 'full_trim'\n",
    "aggregation = 'trim'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "resume=False\n",
    "round_nclients = num_workers\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "fed_model = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    \n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.99**epoch_num))\n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(\n",
    "                each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "    \n",
    "    user_updates = full_trim(user_updates, nbyz)\n",
    "    agg_update = tr_mean(user_updates, nbyz)\n",
    "    # agg_update = torch.mean(user_updates, 0)\n",
    "\n",
    "    del user_updates\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = cnn().to(device)\n",
    "    net.apply(init_weights)\n",
    "    \n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "\n",
    "    if epoch_num%5==0 or epoch_num==nepochs-1:\n",
    "        val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "        is_best = best_global_acc < val_acc\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "    epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f52d20ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 benign_norm 1.295 val loss 2.157 val acc 60.795 best val_acc 60.795\n",
      "e 5 benign_norm 0.706 val loss 0.305 val acc 91.316 best val_acc 91.316\n",
      "e 10 benign_norm 0.631 val loss 0.234 val acc 92.920 best val_acc 92.920\n",
      "e 15 benign_norm 0.626 val loss 0.217 val acc 93.457 best val_acc 93.457\n",
      "e 20 benign_norm 0.630 val loss 0.209 val acc 93.527 best val_acc 93.527\n",
      "e 25 benign_norm 0.640 val loss 0.200 val acc 93.726 best val_acc 93.726\n",
      "e 30 benign_norm 0.645 val loss 0.199 val acc 93.935 best val_acc 93.935\n",
      "e 35 benign_norm 0.659 val loss 0.197 val acc 94.045 best val_acc 94.045\n",
      "e 40 benign_norm 0.679 val loss 0.195 val acc 94.294 best val_acc 94.294\n",
      "e 45 benign_norm 0.682 val loss 0.199 val acc 94.384 best val_acc 94.384\n",
      "e 49 benign_norm 0.687 val loss 0.193 val acc 94.523 best val_acc 94.523\n",
      "e 50 benign_norm 0.696 val loss 0.190 val acc 94.553 best val_acc 94.553\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "nepochs=2000\n",
    "local_epochs = 2\n",
    "batch_size = 8\n",
    "num_workers = 100\n",
    "local_lr = 0.05\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 28\n",
    "byz_type = 'full_trim'\n",
    "aggregation = 'trim'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "resume=False\n",
    "round_nclients = num_workers\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "fed_model = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    \n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr = local_lr*(0.99**epoch_num))\n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(\n",
    "                each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "    \n",
    "    user_updates = full_trim(user_updates, nbyz)\n",
    "    agg_update = torch.median(user_updates, 0)[0]\n",
    "\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = cnn().to(device)\n",
    "    net.apply(init_weights)\n",
    "    \n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "\n",
    "    if epoch_num%5==0 or epoch_num==nepochs-1:\n",
    "        val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "        is_best = best_global_acc < val_acc\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "        print('e %d benign_norm %.3f val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, benign_norm, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "    epoch_num+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
