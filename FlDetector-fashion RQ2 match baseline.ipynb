{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c792f",
   "metadata": {},
   "source": [
    "# FLDetector for Fashion MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821b1014",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gypsum-gpu044/6605877/ipykernel_2681350/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cbef18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /work/vshejwalkar_umass_edu/anaconda/envs/myenv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0,'./utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from SGD import *\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a52c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26708069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet_train_data(trainset, no_participants, alpha=0.9, force=False):\n",
    "        \"\"\"\n",
    "            Input: Number of participants and alpha (param for distribution)\n",
    "            Output: A list of indices denoting data in CIFAR training set.\n",
    "            Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "            Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "            dirichlet distribution to sample number of images in each class.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants)) or force:\n",
    "            print('generating participant indices for alpha %.1f'%alpha)\n",
    "            np.random.seed(0)\n",
    "            cifar_classes = {}\n",
    "            for ind, x in enumerate(trainset):\n",
    "                _, label = x\n",
    "                if label in cifar_classes:\n",
    "                    cifar_classes[label].append(ind)\n",
    "                else:\n",
    "                    cifar_classes[label] = [ind]\n",
    "\n",
    "            per_participant_list = defaultdict(list)\n",
    "            no_classes = len(cifar_classes.keys())\n",
    "            for n in range(no_classes):\n",
    "                random.shuffle(cifar_classes[n])\n",
    "                sampled_probabilities = len(cifar_classes[n]) * np.random.dirichlet(\n",
    "                    np.array(no_participants * [alpha]))\n",
    "                for user in range(no_participants):\n",
    "                    no_imgs = int(round(sampled_probabilities[user]))\n",
    "                    sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "                    per_participant_list[user].extend(sampled_list)\n",
    "                    cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "            with open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'wb') as f:\n",
    "                pickle.dump(per_participant_list, f)\n",
    "        else:\n",
    "            per_participant_list = pickle.load(open('./dirichlet_a_%.1f_nusers_%d.pkl'%(alpha, no_participants), 'rb'))\n",
    "            \n",
    "        return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c258374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_train_data(trainset, num_workers=100, bias=0.5):\n",
    "    np.random.seed(42)\n",
    "    bias_weight = bias\n",
    "    other_group_size = (1 - bias_weight) / 9.\n",
    "    worker_per_group = num_workers / 10\n",
    "\n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    for i, (x, y) in enumerate(trainset):\n",
    "        # assign a data point to a group\n",
    "        upper_bound = (y) * (1 - bias_weight) / 9. + bias_weight\n",
    "        lower_bound = (y) * (1 - bias_weight) / 9.\n",
    "        rd = np.random.random_sample()\n",
    "\n",
    "        if rd > upper_bound:\n",
    "            worker_group = int(np.floor((rd - upper_bound) / other_group_size) + y + 1)\n",
    "        elif rd < lower_bound:\n",
    "            worker_group = int(np.floor(rd / other_group_size))\n",
    "        else:\n",
    "            worker_group = y\n",
    "\n",
    "        rd = np.random.random_sample()\n",
    "        selected_worker = int(worker_group * worker_per_group + int(np.floor(rd * worker_per_group)))\n",
    "        \n",
    "        if not len(each_worker_data[selected_worker]):\n",
    "            each_worker_data[selected_worker] = x[None, :]\n",
    "        else:\n",
    "            each_worker_data[selected_worker]= torch.concat((each_worker_data[selected_worker], x[None, :]))\n",
    "        \n",
    "        each_worker_label[selected_worker].append(y)\n",
    "    \n",
    "    each_worker_tr_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_tr_label = [[] for _ in range(num_workers)]\n",
    "    each_worker_val_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_val_label = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "\n",
    "    for i in range(num_workers):\n",
    "        w_len = len(each_worker_data[i])\n",
    "        len_tr = int(5 * w_len / 7)\n",
    "        len_val = int(1 * w_len / 7)\n",
    "        len_te = w_len - (len_tr + len_val)\n",
    "        # tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        # te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        w = np.arange(w_len)\n",
    "        np.random.shuffle(w)\n",
    "        tr_idx, val_idx, te_idx = w[:len_tr], w[len_tr : (len_tr + len_val)], w[-len_te:]\n",
    "        each_worker_tr_data[i] = each_worker_data[i][tr_idx]\n",
    "        each_worker_tr_label[i] = torch.Tensor(each_worker_label[i])[tr_idx]\n",
    "        each_worker_val_data[i] = each_worker_data[i][val_idx]\n",
    "        each_worker_val_label[i] = torch.Tensor(each_worker_label[i])[val_idx]\n",
    "        each_worker_te_data[i] = each_worker_data[i][te_idx]\n",
    "        each_worker_te_label[i] = torch.Tensor(each_worker_label[i])[te_idx]\n",
    "        \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    del each_worker_data, each_worker_label\n",
    "    return each_worker_tr_data, each_worker_tr_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0f36f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_data_dirichlet(trainset, num_workers, alpha=1, force=False):\n",
    "    per_participant_list = sample_dirichlet_train_data(trainset, num_workers, alpha=alpha, force=force)\n",
    "    \n",
    "    each_worker_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    each_worker_te_data = [[] for _ in range(num_workers)]\n",
    "    each_worker_te_label = [[] for _ in range(num_workers)]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    for worker_idx in range(len(per_participant_list)):\n",
    "        w_indices = np.array(per_participant_list[worker_idx])\n",
    "        w_len = len(w_indices)\n",
    "        len_tr = int(6*w_len/7)\n",
    "        len_te = w_len - len_tr\n",
    "        tr_idx = np.random.choice(w_len, len_tr, replace=False)\n",
    "        te_idx = np.delete(np.arange(w_len), tr_idx)\n",
    "        \n",
    "        for idx in tr_idx:\n",
    "            each_worker_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_data[worker_idx] = torch.stack(each_worker_data[worker_idx])\n",
    "        each_worker_label[worker_idx] = torch.Tensor(each_worker_label[worker_idx]).long()\n",
    "        \n",
    "        for idx in te_idx:\n",
    "            each_worker_te_data[worker_idx].append(trainset[idx][0])\n",
    "            each_worker_te_label[worker_idx].append(trainset[idx][1])\n",
    "        each_worker_te_data[worker_idx] = torch.stack(each_worker_te_data[worker_idx])\n",
    "        each_worker_te_label[worker_idx] = torch.Tensor(each_worker_te_label[worker_idx]).long()\n",
    "    \n",
    "    global_test_data = torch.concat(each_worker_te_data)\n",
    "    global_test_label = torch.concat(each_worker_te_label)\n",
    "    \n",
    "    return each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_trim(v, f):\n",
    "    '''\n",
    "    Full-knowledge Trim attack. w.l.o.g., we assume the first f worker devices are compromised.\n",
    "    v: the list of squeezed gradients\n",
    "    f: the number of compromised worker devices\n",
    "    '''\n",
    "    vi_shape = v[0].unsqueeze(0).T.shape\n",
    "    v_tran = v.T\n",
    "    \n",
    "    maximum_dim = torch.max(v_tran, dim=1)\n",
    "    maximum_dim = maximum_dim[0].reshape(vi_shape)\n",
    "    minimum_dim = torch.min(v_tran, dim=1)\n",
    "    minimum_dim = minimum_dim[0].reshape(vi_shape)\n",
    "    direction = torch.sign(torch.sum(v_tran, dim=-1, keepdims=True))\n",
    "    directed_dim = (direction > 0) * minimum_dim + (direction < 0) * maximum_dim\n",
    "\n",
    "    for i in range(f):\n",
    "        random_12 = 2\n",
    "        tmp = directed_dim * ((direction * directed_dim > 0) / random_12 + (direction * directed_dim < 0) * random_12)\n",
    "        tmp = tmp.squeeze()\n",
    "        v[i] = tmp\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319ce946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_attack_median(all_updates, n_attackers, dev_type='unit_vec'):\n",
    "    model_re = (all_updates)\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "    lamda = torch.Tensor([10.0]).cuda() #compute_lambda_our(all_updates, model_re, n_attackers)\n",
    "    threshold_diff = 1e-5\n",
    "    prev_loss = -1\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    iters = 0 \n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        agg_grads = torch.median(mal_updates, 0)[0]\n",
    "        \n",
    "        loss = torch.norm(agg_grads - model_re)\n",
    "        \n",
    "        if prev_loss < loss:\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "        prev_loss = loss\n",
    "        \n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "    mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "    return mal_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee2f5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca6b5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 30, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(30, 50, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(1250, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "#         print(x.shape)\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae94ee06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266060"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cnn()\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ace04bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = torch.utils.data.ConcatDataset((trainset, testset))\n",
    "num_workers = 100\n",
    "distribution='fang'\n",
    "param = .5\n",
    "force = True\n",
    "if distribution=='fang':\n",
    "    each_worker_data, each_worker_label, each_worker_val_data, each_worker_val_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_train_data(all_data, num_workers=100, bias=param)\n",
    "elif distribution == 'dirichlet':\n",
    "    each_worker_data, each_worker_label, each_worker_te_data, each_worker_te_label, global_test_data, global_test_label = get_client_data_dirichlet(all_data, num_workers, alpha=param, force=force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a30decba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10048"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(global_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a6b71",
   "metadata": {},
   "source": [
    "# Slow baselines + fang distribution + 80 benign clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fadc176c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.12201170518797738 0.12201170518797738\n",
      "20 0.48467413947029064 0.49727209602222\n",
      "40 0.6145223688126178 0.6856462652514631\n",
      "60 0.6839599246106537 0.6918956452732864\n",
      "80 0.7349469298680686 0.7349469298680686\n",
      "100 0.7505207816684852 0.7528023013589922\n",
      "120 0.7559765896240452 0.7635155242535463\n",
      "140 0.7336573752603909 0.7737327646066858\n",
      "160 0.767086598551731 0.7797837516119432\n",
      "180 0.7768078563634561 0.785437952584069\n",
      "200 0.7687729391925404 0.7949608173792283\n",
      "220 0.7600436464636445 0.8017061799424661\n",
      "240 0.7942664418212478 0.8073603809145918\n",
      "260 0.7718480309493105 0.8145025295109612\n",
      "280 0.7984326951691301 0.8145025295109612\n",
      "300 0.812518599345303 0.8216446781073307\n",
      "320 0.8024997520087293 0.8237278047812717\n",
      "340 0.8204543200079357 0.8268028965380418\n",
      "360 0.820156730483087 0.8287868267037001\n",
      "380 0.8140065469695467 0.830969149885924\n",
      "400 0.821247892074199 0.8337466521178455\n",
      "420 0.8166848526931852 0.8359289753000695\n",
      "440 0.8221406606487451 0.8393016565816883\n",
      "460 0.8300763813113778 0.839599246106537\n",
      "480 0.8410871937307807 0.8437654994544193\n",
      "499 0.8308699533776411 0.8445590715206824\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "num_workers = 100\n",
    "nbyz = 20\n",
    "lr = 0.1\n",
    "batch_size = 32\n",
    "nepochs=500\n",
    "best_acc = 0\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "for e in range(nepochs):\n",
    "    #cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(nbyz, num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        idx = np.random.choice(len(each_worker_data[i]), batch_size)\n",
    "        output = net_(each_worker_data[i][idx].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i][idx]).long().cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "    agg_grads = torch.mean(user_grads, 0)\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    \n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        acc = correct/total\n",
    "        best_acc = max(best_acc, acc)\n",
    "    best_accs_per_round.append(best_acc)\n",
    "    accs_per_round.append(acc)\n",
    "    if e%20==0 or e==nepochs-1:\n",
    "        print(e, acc, best_acc)\n",
    "\n",
    "# final_accs_per_client=[]\n",
    "# for i in range(num_workers):\n",
    "#     total, correct = 0,0\n",
    "#     with torch.no_grad():\n",
    "#         outputs = net(each_worker_te_data[i].cuda())\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += len(each_worker_te_label[i])\n",
    "#         correct += (predicted == each_worker_te_label[i].long().cuda()).sum().item()\n",
    "#         acc = correct/total\n",
    "#     final_accs_per_client.append(acc)\n",
    "\n",
    "# results = collections.OrderedDict(\n",
    "#     final_accs_per_client=np.array(final_accs_per_client),\n",
    "#     accs_per_round=np.array(accs_per_round),\n",
    "#     best_accs_per_round=np.array(best_accs_per_round),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b9e98",
   "metadata": {},
   "source": [
    "## Less slow Mean for FedRecover experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "727fbc41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.17454943741909787 0.17454943741909787\n",
      "20 0.46151548342128845 0.46151548342128845\n",
      "40 0.5153838494473763 0.5153838494473763\n",
      "60 0.5330080653191277 0.5589963158418799\n",
      "80 0.6352683461117196 0.6352683461117196\n",
      "100 0.6417405157821369 0.6419396594643035\n",
      "120 0.6835606890371403 0.6897341431843075\n",
      "140 0.667131335258389 0.6897341431843075\n",
      "160 0.6735039330877228 0.6997908991337249\n",
      "180 0.6897341431843075 0.7013840485910584\n",
      "200 0.7128348103156428 0.7128348103156428\n",
      "220 0.7156228218659763 0.7156228218659763\n",
      "240 0.7043712038235587 0.7258787214975605\n",
      "260 0.7194065518271433 0.7367320521756447\n",
      "280 0.7203026983968933 0.7392213482027282\n",
      "300 0.7362341929702281 0.7392213482027282\n",
      "320 0.7461913770785622 0.7461913770785622\n",
      "340 0.7515682564970626 0.7534601214776461\n",
      "360 0.7495768196753958 0.7542566962063129\n",
      "380 0.7454943741909787 0.7542566962063129\n",
      "399 0.7600318629891467 0.7600318629891467\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "num_workers = 100\n",
    "nbyz = 20\n",
    "lr = 0.02\n",
    "nepochs=400\n",
    "best_acc = 0\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "for e in range(nepochs):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr*(.9995**e))\n",
    "    user_grads = []\n",
    "    for i in range(nbyz, num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "    agg_grads = torch.mean(user_grads, 0)\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        acc = correct/total\n",
    "        best_acc = max(best_acc, acc)\n",
    "    best_accs_per_round.append(best_acc)\n",
    "    accs_per_round.append(acc)\n",
    "    if e%20==0 or e==nepochs-1:\n",
    "        print(e, acc, best_acc)\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        outputs = net(each_worker_te_data[i].cuda())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += len(each_worker_te_label[i])\n",
    "        correct += (predicted == each_worker_te_label[i].long().cuda()).sum().item()\n",
    "        acc = correct/total\n",
    "    final_accs_per_client.append(acc)\n",
    "\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c188a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of data: train 49960 validation 9959 test 10081\n"
     ]
    }
   ],
   "source": [
    "tr_len = val_len = te_len = 0\n",
    "for i in range(len(each_worker_data)):\n",
    "    tr_len += len(each_worker_data[i])\n",
    "    val_len += len(each_worker_val_data[i])\n",
    "    te_len += len(each_worker_te_data[i])\n",
    "print(f'Lengths of data: train {tr_len} validation {val_len} test {te_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d683c0",
   "metadata": {},
   "source": [
    "# Mean without any attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c655e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.09969249082432298 0.09969249082432298\n",
      "20 0.37238369209403827 0.37238369209403827\n",
      "40 0.5212776510266839 0.5212776510266839\n",
      "60 0.5851602023608768 0.5851602023608768\n",
      "80 0.625136395198889 0.625136395198889\n",
      "100 0.6549945441920444 0.6549945441920444\n",
      "120 0.6736434877492312 0.6736434877492312\n",
      "140 0.6859438547763119 0.6859438547763119\n",
      "160 0.653209007042952 0.689018946533082\n",
      "180 0.658764011506795 0.689018946533082\n",
      "200 0.66302946136296 0.689018946533082\n",
      "220 0.6715603610752902 0.689018946533082\n",
      "240 0.6784049201468109 0.689018946533082\n",
      "260 0.6819759944449956 0.689018946533082\n",
      "280 0.6867374268425751 0.6906060906656085\n",
      "300 0.6907052871738915 0.6947723440134908\n",
      "320 0.689118143041365 0.6992361868862216\n",
      "340 0.6925900208312668 0.7089574446979466\n",
      "360 0.7006249380021823 0.710643785338756\n",
      "380 0.7113381608967364 0.7141156631286578\n",
      "400 0.7196706675925008 0.7196706675925008\n",
      "420 0.7209602222001785 0.7211586152167444\n",
      "440 0.7199682571173495 0.7211586152167444\n",
      "460 0.7025096716595576 0.7253248685646265\n",
      "480 0.7235393314155342 0.7362364844757464\n",
      "500 0.7274079952385676 0.7390139867076679\n",
      "520 0.7303838904870549 0.7425850610058526\n",
      "540 0.7339549647852396 0.7436762225969646\n",
      "560 0.7362364844757464 0.7453625632377741\n",
      "580 0.7376252355917071 0.7466521178454518\n",
      "600 0.742188274972721 0.7480408689614125\n",
      "620 0.7498264061105049 0.7509175677016169\n",
      "640 0.7395099692490824 0.7520087292927289\n",
      "660 0.7368316635254439 0.7520087292927289\n",
      "680 0.7417914889395893 0.7520087292927289\n",
      "700 0.7436762225969646 0.7520087292927289\n",
      "720 0.7464537248288861 0.7520087292927289\n",
      "740 0.7495288165856562 0.7520087292927289\n",
      "760 0.7519095327844459 0.7520087292927289\n",
      "780 0.7568693581985914 0.7575637337565718\n",
      "800 0.7636147207618292 0.7661938299771848\n",
      "820 0.7647058823529411 0.768177760142843\n",
      "840 0.7665906160103164 0.769268921733955\n",
      "860 0.768177760142843 0.7710544588830474\n",
      "880 0.7694673147505208 0.7728399960321397\n",
      "900 0.7702608868167841 0.7745263366729491\n",
      "920 0.7717488344410277 0.7766094633468902\n",
      "940 0.7709552623747644 0.7766094633468902\n",
      "960 0.7601428429719274 0.7766094633468902\n",
      "980 0.7679793671262771 0.7766094633468902\n",
      "999 0.7676817776014284 0.7766094633468902\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "num_workers = 100\n",
    "nbyz = 20\n",
    "lr = 0.01\n",
    "nepochs=1000\n",
    "best_acc = 0\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "cnn_optimizer = optim.SGD(net.parameters(), lr = lr)\n",
    "\n",
    "for e in range(nepochs):\n",
    "    user_grads = []\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "    agg_grads = torch.mean(user_grads, 0)\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    # model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param.grad=param_.cuda()\n",
    "        # model_grads.append(param_)\n",
    "    cnn_optimizer.step()\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        acc = correct/total\n",
    "        best_acc = max(best_acc, acc)\n",
    "    best_accs_per_round.append(best_acc)\n",
    "    accs_per_round.append(acc)\n",
    "    if e%20==0 or e==nepochs-1:\n",
    "        print(e, acc, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67a4ed08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.09937269740117495 0.09937269740117495\n",
      "100 0.5121975505327093 0.5121975505327093\n",
      "200 0.636861495569053 0.6486109728168874\n",
      "300 0.6866474161107239 0.6866474161107239\n",
      "400 0.716120681071393 0.716120681071393\n",
      "500 0.7206014139201434 0.7275714427959773\n",
      "600 0.7442995120979787 0.7442995120979787\n",
      "700 0.7543562680473962 0.7543562680473962\n",
      "800 0.7470875236483122 0.7549536990938962\n",
      "900 0.7655083142487304 0.7662053171363139\n",
      "999 0.7624215871751469 0.7693916160509808\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "num_workers = 100\n",
    "nbyz = 20\n",
    "lr = 0.01\n",
    "nepochs=1000\n",
    "best_acc = 0\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "for e in range(nepochs):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(nbyz, num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "    agg_grads = torch.mean(user_grads, 0)\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        acc = correct/total\n",
    "        best_acc = max(best_acc, acc)\n",
    "    best_accs_per_round.append(best_acc)\n",
    "    accs_per_round.append(acc)\n",
    "    if e%100==0 or e==nepochs-1:\n",
    "        print(e, acc, best_acc)\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        outputs = net(each_worker_te_data[i].cuda())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += len(each_worker_te_label[i])\n",
    "        correct += (predicted == each_worker_te_label[i].long().cuda()).sum().item()\n",
    "        acc = correct/total\n",
    "    final_accs_per_client.append(acc)\n",
    "\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_fashion_slow_mean.pkl'), 'wb'), )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb3c8d",
   "metadata": {},
   "source": [
    "# Mean under full trim attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cf403bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.09767997610275814 0.09767997610275814\n",
      "100 0.23867370307676988 0.23867370307676988\n",
      "200 0.3300806531912775 0.3300806531912775\n",
      "300 0.43234093398386936 0.43234093398386936\n",
      "400 0.5213581599123768 0.5213581599123768\n",
      "500 0.5155829931295429 0.5443592552026287\n",
      "600 0.5495369909389625 0.5729363735935478\n",
      "700 0.5351986458229613 0.6112715324106343\n",
      "800 0.568654784426964 0.6192372796973016\n",
      "900 0.6142586876431345 0.6192372796973016\n",
      "999 0.5324106342726277 0.6404460818480534\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "num_workers = 100\n",
    "nbyz = 20\n",
    "lr = 0.01\n",
    "nepochs=1000\n",
    "best_acc = 0\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "\n",
    "for e in range(nepochs):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "    user_grads = full_trim(user_grads, nbyz)\n",
    "    agg_grads = tr_mean(user_grads, 0)\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        acc = correct/total\n",
    "        best_acc = max(best_acc, acc)\n",
    "    best_accs_per_round.append(best_acc)\n",
    "    accs_per_round.append(acc)\n",
    "    if e%100==0 or e==nepochs-1:\n",
    "        print(e, acc, best_acc)\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        outputs = net(each_worker_te_data[i].cuda())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += len(each_worker_te_label[i])\n",
    "        correct += (predicted == each_worker_te_label[i].long().cuda()).sum().item()\n",
    "        acc = correct/total\n",
    "    final_accs_per_client.append(acc)\n",
    "\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_fashion_slow_trmean_trim20.pkl'), 'wb'), )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993be4b",
   "metadata": {},
   "source": [
    "# Median without any attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6b9375b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.09907398187792492 0.09907398187792492\n",
      "100 0.5924524544458827 0.5924524544458827\n",
      "200 0.49766006173454147 0.6430349497162202\n",
      "300 0.6203325699492184 0.6430349497162202\n",
      "400 0.5586976003186299 0.6684257691924723\n",
      "500 0.6465199641541373 0.6716120681071392\n",
      "600 0.6584685850841382 0.6871452753161406\n",
      "700 0.6669321915762223 0.704172060141392\n",
      "800 0.6710146370606392 0.704172060141392\n",
      "900 0.683759832719307 0.704172060141392\n",
      "999 0.6952105944438912 0.704172060141392\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "num_workers = 100\n",
    "nbyz = 20\n",
    "lr = 0.01\n",
    "nepochs=1000\n",
    "best_acc = 0\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "for e in range(nepochs):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(nbyz, num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "    \n",
    "    agg_grads = torch.median(user_grads, 0)[0]\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        acc = correct/total\n",
    "        best_acc = max(best_acc, acc)\n",
    "    best_accs_per_round.append(best_acc)\n",
    "    accs_per_round.append(acc)\n",
    "    if e%100==0 or e==nepochs-1:\n",
    "        print(e, acc, best_acc)\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        outputs = net(each_worker_te_data[i].cuda())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += len(each_worker_te_label[i])\n",
    "        correct += (predicted == each_worker_te_label[i].long().cuda()).sum().item()\n",
    "        acc = correct/total\n",
    "    final_accs_per_client.append(acc)\n",
    "\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_fashion_slow_median.pkl'), 'wb'), )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5519022f",
   "metadata": {},
   "source": [
    "# Median under full trim attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b937819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0887185104052574 0.0887185104052574\n",
      "100 0.18729463307776562 0.21776361644926814\n",
      "200 0.19675395798068307 0.4230807527631186\n",
      "300 0.3940057751667828 0.4230807527631186\n",
      "400 0.34989544956686247 0.5362939360748781\n",
      "500 0.37429055063228117 0.5362939360748781\n",
      "600 0.41929702280195164 0.5446579707258787\n",
      "700 0.4897938862889575 0.5446579707258787\n",
      "800 0.42178631882903517 0.6279996017126357\n",
      "900 0.5562083042915463 0.6279996017126357\n",
      "999 0.5091108234591257 0.6349696305884696\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net = cnn().to(device)\n",
    "net.apply(init_weights)\n",
    "num_workers = 100\n",
    "nbyz = 20\n",
    "lr = 0.01\n",
    "nepochs=1000\n",
    "best_acc = 0\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "\n",
    "for e in range(nepochs):\n",
    "    cnn_optimizer = SGD(net.parameters(), lr = lr)\n",
    "    user_grads = []\n",
    "    for i in range(nbyz, num_workers):\n",
    "        net_ = copy.deepcopy(net)\n",
    "        net_.zero_grad()\n",
    "        output = net_(each_worker_data[i].cuda())\n",
    "        loss = criterion(output, torch.Tensor(each_worker_label[i]).long().cuda())\n",
    "        loss.backward(retain_graph = True)\n",
    "        param_grad=[]\n",
    "        for param in net_.parameters():\n",
    "            param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "        user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "        del net_\n",
    "\n",
    "    user_grads = full_trim(user_grads, nbyz)\n",
    "    agg_grads = torch.median(user_grads, 0)[0]\n",
    "    del user_grads\n",
    "    start_idx=0\n",
    "    cnn_optimizer.zero_grad()\n",
    "    model_grads=[]\n",
    "    for i, param in enumerate(net.parameters()):\n",
    "        param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx=start_idx+len(param.data.view(-1))\n",
    "        param_=param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "    cnn_optimizer.step(model_grads)\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = global_test_data.to(device), global_test_label.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        acc = correct/total\n",
    "        best_acc = max(best_acc, acc)\n",
    "    best_accs_per_round.append(best_acc)\n",
    "    accs_per_round.append(acc)\n",
    "    if e%100==0 or e==nepochs-1:\n",
    "        print(e, acc, best_acc)\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    total, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        outputs = net(each_worker_te_data[i].cuda())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += len(each_worker_te_label[i])\n",
    "        correct += (predicted == each_worker_te_label[i].long().cuda()).sum().item()\n",
    "        acc = correct/total\n",
    "    final_accs_per_client.append(acc)\n",
    "\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_fashion_slow_median_trim20.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb5ef3",
   "metadata": {},
   "source": [
    "# Good FedAvg baseline Fashion MNIST + Fang distribution + 80 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24164616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "def train(train_data, labels, model, optimizer, batch_size=20):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    len_t = (len(train_data) // batch_size)\n",
    "    if len(train_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    r=np.arange(len(train_data))\n",
    "    np.random.shuffle(r)\n",
    "    \n",
    "    train_data = train_data[r]\n",
    "    labels = labels[r]\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "\n",
    "        inputs = train_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "        targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "\n",
    "def test(test_data, labels, model, criterion, use_cuda, debug_='MEDIUM', batch_size=64):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    len_t = (len(test_data) // batch_size)\n",
    "    if len(test_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ind in range(len_t):\n",
    "            # measure data loading time\n",
    "            inputs = test_data[ind * batch_size:(ind + 1) * batch_size]\n",
    "            targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092fa012",
   "metadata": {},
   "source": [
    "# Mean without any attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2189af91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 val loss 1.402 val acc 69.700 best val_acc 69.700\n",
      "e 10 val loss 0.442 val acc 83.979 best val_acc 83.979\n",
      "e 20 val loss 0.362 val acc 87.016 best val_acc 87.016\n",
      "e 30 val loss 0.327 val acc 88.270 best val_acc 88.290\n",
      "e 40 val loss 0.306 val acc 89.246 best val_acc 89.266\n",
      "e 49 val loss 0.295 val acc 89.585 best val_acc 89.585\n",
      "e 50 val loss 0.293 val acc 89.565 best val_acc 89.585\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "fed_model = cnn().to(device)\n",
    "fed_model.apply(init_weights)\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    \n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr, momentum=0.9, weight_decay=1e-4)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(\n",
    "                each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    agg_update = torch.mean(user_updates, 0)\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    \n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, val_loss, val_acc, best_global_acc))\n",
    "    epoch_num+=1\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    client_loss, client_acc = test(each_worker_te_data[i], each_worker_te_label[i].long(),\n",
    "                                   fed_model, criterion, use_cuda)\n",
    "    final_accs_per_client.append(client_acc)\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    "    loss_per_round=np.array(loss_per_round)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_fashion_fast_mean.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84297aac",
   "metadata": {},
   "source": [
    "# Mean under full trim attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f0d4ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 val loss 1.904 val acc 57.204 best val_acc 57.204\n",
      "e 10 val loss 0.557 val acc 78.941 best val_acc 78.941\n",
      "e 20 val loss 0.507 val acc 81.470 best val_acc 81.470\n",
      "e 30 val loss 0.498 val acc 82.137 best val_acc 82.376\n",
      "e 40 val loss 0.489 val acc 82.585 best val_acc 82.655\n",
      "e 49 val loss 0.480 val acc 83.142 best val_acc 83.142\n",
      "e 50 val loss 0.481 val acc 83.142 best val_acc 83.142\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "fed_model = cnn().to(device)\n",
    "fed_model.apply(init_weights)\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    \n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr, momentum=0.9, weight_decay=1e-4)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(\n",
    "                each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    user_updates = full_trim(user_updates, nbyz)\n",
    "    agg_update = tr_mean(user_updates, nbyz)\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    \n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, val_loss, val_acc, best_global_acc))\n",
    "    epoch_num+=1\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    client_loss, client_acc = test(each_worker_te_data[i], each_worker_te_label[i].long(),\n",
    "                                   fed_model, criterion, use_cuda)\n",
    "    final_accs_per_client.append(client_acc)\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    "    loss_per_round=np.array(loss_per_round)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_fashion_fast_trmean_trim20.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ebae2",
   "metadata": {},
   "source": [
    "# Median without any attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43fc9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 val loss 1.726 val acc 59.653 best val_acc 59.653\n",
      "e 10 val loss 0.451 val acc 83.352 best val_acc 83.352\n",
      "e 20 val loss 0.369 val acc 86.677 best val_acc 86.677\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "fed_model = cnn().to(device)\n",
    "fed_model.apply(init_weights)\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(nbyz, num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    \n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr, momentum=0.9, weight_decay=1e-4)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(\n",
    "                each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    agg_update = torch.median(user_updates, 0)[0]\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    \n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, val_loss, val_acc, best_global_acc))\n",
    "    epoch_num+=1\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    client_loss, client_acc = test(each_worker_te_data[i], each_worker_te_label[i].long(),\n",
    "                                   fed_model, criterion, use_cuda)\n",
    "    final_accs_per_client.append(client_acc)\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    "    loss_per_round=np.array(loss_per_round)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_fashion_fast_median.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa171cd5",
   "metadata": {},
   "source": [
    "# Median under full trim attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29671566",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "local_epochs = 2\n",
    "batch_size = 16\n",
    "num_workers = 100\n",
    "local_lr = 0.01\n",
    "global_lr = 1\n",
    "nepochs = 50\n",
    "nbyz = 20\n",
    "\n",
    "best_global_acc=0\n",
    "epoch_num = 0\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "fed_model = cnn().to(device)\n",
    "fed_model.apply(init_weights)\n",
    "model_received = []\n",
    "for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "    model_received = param.view(-1).data.type(torch.cuda.FloatTensor) if len(model_received) == 0 else torch.cat((model_received, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "best_accs_per_round = []\n",
    "accs_per_round = []\n",
    "loss_per_round = []\n",
    "home_dir = '/home/vshejwalkar_umass_edu/fedrecover/'\n",
    "\n",
    "while epoch_num <= nepochs:\n",
    "    torch.cuda.empty_cache()\n",
    "    round_clients = np.arange(num_workers)\n",
    "    round_benign = round_clients\n",
    "    user_updates=[]\n",
    "    benign_norm = 0\n",
    "    \n",
    "    for i in round_benign:\n",
    "        model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr, momentum=0.9, weight_decay=1e-4)\n",
    "        for epoch in range(local_epochs):\n",
    "            train_loss, train_acc = train(\n",
    "                each_worker_data[i], torch.Tensor(each_worker_label[i]).long(), model, optimizer, batch_size)\n",
    "\n",
    "        params = []\n",
    "        for i, (name, param) in enumerate(model.state_dict().items()):\n",
    "            params = param.view(-1).data.type(torch.cuda.FloatTensor) if len(params) == 0 else torch.cat(\n",
    "                (params, param.view(-1).data.type(torch.cuda.FloatTensor)))\n",
    "\n",
    "        update =  (params - model_received)\n",
    "        benign_norm += torch.norm(update)/len(round_benign)\n",
    "        user_updates = update[None,:] if len(user_updates) == 0 else torch.cat((user_updates, update[None,:]), 0)\n",
    "\n",
    "    user_updates = full_trim(user_updates, nbyz)\n",
    "    agg_update = torch.median(user_updates, 0)[0]\n",
    "    del user_updates\n",
    "\n",
    "    model_received = model_received + global_lr * agg_update\n",
    "    fed_model = cnn().to(device)\n",
    "    fed_model.apply(init_weights)\n",
    "    \n",
    "    start_idx=0\n",
    "    state_dict = {}\n",
    "    previous_name = 'none'\n",
    "    for i, (name, param) in enumerate(fed_model.state_dict().items()):\n",
    "        start_idx = 0 if i == 0 else start_idx + len(fed_model.state_dict()[previous_name].data.view(-1))\n",
    "        start_end = start_idx + len(fed_model.state_dict()[name].data.view(-1))\n",
    "        params = model_received[start_idx:start_end].reshape(fed_model.state_dict()[name].data.shape)\n",
    "        state_dict[name] = params\n",
    "        previous_name = name\n",
    "    fed_model.load_state_dict(state_dict)\n",
    "    val_loss, val_acc = test(global_test_data, global_test_label.long(), fed_model, criterion, use_cuda)\n",
    "    is_best = best_global_acc < val_acc\n",
    "    best_global_acc = max(best_global_acc, val_acc)\n",
    "    best_accs_per_round.append(best_global_acc)\n",
    "    accs_per_round.append(val_acc)\n",
    "    loss_per_round.append(val_loss)\n",
    "    if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "        print('e %d val loss %.3f val acc %.3f best val_acc %.3f'% (epoch_num, val_loss, val_acc, best_global_acc))\n",
    "    epoch_num+=1\n",
    "\n",
    "final_accs_per_client=[]\n",
    "for i in range(num_workers):\n",
    "    client_loss, client_acc = test(each_worker_te_data[i], each_worker_te_label[i].long(),\n",
    "                                   fed_model, criterion, use_cuda)\n",
    "    final_accs_per_client.append(client_acc)\n",
    "results = collections.OrderedDict(\n",
    "    final_accs_per_client=np.array(final_accs_per_client),\n",
    "    accs_per_round=np.array(accs_per_round),\n",
    "    best_accs_per_round=np.array(best_accs_per_round),\n",
    "    loss_per_round=np.array(loss_per_round)\n",
    ")\n",
    "pickle.dump(results, open(os.path.join(home_dir, 'FLDetector_plots_data/rq1_baseline_fashion_fast_median_trim20.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f118ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
